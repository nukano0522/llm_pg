{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForMultipleChoice\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Hub上のllm-book/JGLUEのリポジトリから\n",
    "# JCommonsenseQAのデータを読み込む\n",
    "train_dataset = load_dataset(\n",
    "    \"llm-book/JGLUE\", name=\"JCommonsenseQA\", split=\"train\"\n",
    ")\n",
    "valid_dataset = load_dataset(\n",
    "    \"llm-book/JGLUE\", name=\"JCommonsenseQA\", split=\"validation\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'choice0': '世界',\n",
      " 'choice1': '写真集',\n",
      " 'choice2': '絵本',\n",
      " 'choice3': '論文',\n",
      " 'choice4': '図鑑',\n",
      " 'label': 2,\n",
      " 'q_id': 0,\n",
      " 'question': '主に子ども向けのもので、イラストのついた物語が書かれているものはどれ？'}\n",
      "{'choice0': Value(dtype='string', id=None),\n",
      " 'choice1': Value(dtype='string', id=None),\n",
      " 'choice2': Value(dtype='string', id=None),\n",
      " 'choice3': Value(dtype='string', id=None),\n",
      " 'choice4': Value(dtype='string', id=None),\n",
      " 'label': ClassLabel(names=['choice0',\n",
      "                            'choice1',\n",
      "                            'choice2',\n",
      "                            'choice3',\n",
      "                            'choice4'],\n",
      "                     id=None),\n",
      " 'q_id': Value(dtype='int64', id=None),\n",
      " 'question': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "# pprintで見やすく表示する\n",
    "pprint(train_dataset[idx])\n",
    "pprint(train_dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5}\n"
     ]
    }
   ],
   "source": [
    "# 各設問の選択肢の数をチェック\n",
    "all_num_choices = []\n",
    "for q in train_dataset:\n",
    "    # 選択肢の数を\"choice\"から始まるキーの数として算出\n",
    "    num_choices = sum(\n",
    "        key.startswith(\"choice\") for key in q.keys()\n",
    "    )\n",
    "    all_num_choices.append(num_choices)\n",
    "\n",
    "# 選択肢の数のユニークな値を確認\n",
    "print(set(all_num_choices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model_name = \"cl-tohoku/bert-base-japanese-v3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8939/8939 [00:00<00:00, 11378.66it/s]\n",
      "100%|██████████| 1119/1119 [00:00<00:00, 11792.97it/s]\n"
     ]
    }
   ],
   "source": [
    "class Dataset1(Dataset):\n",
    "    def __init__(self, ds):\n",
    "        self.features = [\n",
    "            {\n",
    "                'question': row['question'],\n",
    "                'choice0': row['choice0'],\n",
    "                'choice1': row['choice1'],\n",
    "                'choice2': row['choice2'],\n",
    "                'choice3': row['choice3'],\n",
    "                'choice4': row['choice4'],\n",
    "                'label': row['label']\n",
    "            } for row in tqdm(ds)\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index]\n",
    "\n",
    "train_dataset1 = Dataset1(train_dataset)\n",
    "valid_dataset1 = Dataset1(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Dataset1(IterableDataset):\n",
    "#     def __init__(self, ds):\n",
    "#         self.features = [\n",
    "#             {\n",
    "#                 'question': row['question'],\n",
    "#                 'choice0': row['choice0'],\n",
    "#                 'choice1': row['choice1'],\n",
    "#                 'choice2': row['choice2'],\n",
    "#                 'choice3': row['choice3'],\n",
    "#                 'choice4': row['choice4'],\n",
    "#                 'label': row['label']\n",
    "#             } for row in tqdm(ds)\n",
    "#         ]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.features)\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         return iter(self.features)\n",
    "\n",
    "# train_dataset1 = Dataset1(train_dataset)\n",
    "# valid_dataset1 = Dataset1(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': '主に子ども向けのもので、イラストのついた物語が書かれているものはどれ？',\n",
       "  'choice0': '世界',\n",
       "  'choice1': '写真集',\n",
       "  'choice2': '絵本',\n",
       "  'choice3': '論文',\n",
       "  'choice4': '図鑑',\n",
       "  'label': 2},\n",
       " {'question': '未成年者を監護・教育し，彼らを監督し，彼らの財産上の利益を守る法律上の義務をもつ人は？',\n",
       "  'choice0': '浮浪者',\n",
       "  'choice1': '保護者',\n",
       "  'choice2': 'お坊さん',\n",
       "  'choice3': '宗教者',\n",
       "  'choice4': '預言者',\n",
       "  'label': 1}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset1[0:2]\n",
    "# train_dataset[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8939\n",
      "{'question': '主に子ども向けのもので、イラストのついた物語が書かれているものはどれ？', 'choice0': '世界', 'choice1': '写真集', 'choice2': '絵本', 'choice3': '論文', 'choice4': '図鑑', 'label': 2}\n"
     ]
    }
   ],
   "source": [
    "# datasetのサイズを確認\n",
    "print(len(train_dataset1))\n",
    "# datasetの中身を確認\n",
    "tmp = next(iter(train_dataset1))\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollator1():\n",
    "    def __init__(self, tokenizer, max_length=64):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        num_choices = 5\n",
    "        examples = {\n",
    "            'question': list(map(lambda x: x['question'], examples)),\n",
    "            # 'choice_list': list(map(lambda x: [x['choice0'], x['choice1'], x['choice2'], x['choice3'], x['choice4']], examples)),\n",
    "            'choice_list': list(map(lambda x: [x['choice0'], x['choice1'], x['choice2'], x['choice3'], x['choice4']], examples)),\n",
    "            'repeated_question_list': list(map(lambda x: [x['question'], x['question'], x['question'], x['question'], x['question']], examples)),\n",
    "            'label': list(map(lambda x: x['label'], examples)),\n",
    "        }\n",
    "\n",
    "        # 選択肢と設問のリストを1次元に平坦化\n",
    "        repeated_question_list_flat = [item for sublist in examples['repeated_question_list'] for item in sublist]\n",
    "        choice_list_flat = [item for sublist in examples['choice_list'] for item in sublist]\n",
    "        \n",
    "        # (バッチサイズ * 選択肢数, 最大系列長)の形式でエンコード\n",
    "        encodings = self.tokenizer(repeated_question_list_flat,\n",
    "                                   choice_list_flat,\n",
    "                                   padding=True, \n",
    "                                   truncation=True,\n",
    "                                   max_length=self.max_length,\n",
    "                                   return_tensors='pt')\n",
    "\n",
    "        # （バッチサイズ, 選択肢数, 最大系列長）に変換\n",
    "        batch_size = len(examples[\"question\"])\n",
    "        batch = {\n",
    "            k: v.view(batch_size, num_choices, -1)\n",
    "            for k, v in encodings.items()\n",
    "        }\n",
    "        batch[\"labels\"] = torch.tensor(examples['label'])\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_model_name)\n",
    "data_collator = DataCollator1(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([4, 5, 40])\n",
      "token_type_ids torch.Size([4, 5, 40])\n",
      "attention_mask torch.Size([4, 5, 40])\n",
      "labels torch.Size([4])\n",
      "tensor([[    2, 13182, 16044, 12994,   464, 12518,   457,   384, 14930,   464,\n",
      "         12584,   449, 13360,   430, 14220,   494,   456, 12483, 12518,   465,\n",
      "         19382,    46,     3, 12575,     3,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    2, 13182, 16044, 12994,   464, 12518,   457,   384, 14930,   464,\n",
      "         12584,   449, 13360,   430, 14220,   494,   456, 12483, 12518,   465,\n",
      "         19382,    46,     3, 13409,  6460,     3,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    2, 13182, 16044, 12994,   464, 12518,   457,   384, 14930,   464,\n",
      "         12584,   449, 13360,   430, 14220,   494,   456, 12483, 12518,   465,\n",
      "         19382,    46,     3, 20647,     3,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    2, 13182, 16044, 12994,   464, 12518,   457,   384, 14930,   464,\n",
      "         12584,   449, 13360,   430, 14220,   494,   456, 12483, 12518,   465,\n",
      "         19382,    46,     3, 15252,     3,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    2, 13182, 16044, 12994,   464, 12518,   457,   384, 14930,   464,\n",
      "         12584,   449, 13360,   430, 14220,   494,   456, 12483, 12518,   465,\n",
      "         19382,    46,     3, 22929,     3,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "tensor([2, 1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# データローダの挙動を確認\n",
    "loader = DataLoader(train_dataset1, collate_fn=data_collator, batch_size=4)\n",
    "batch = next(iter(loader))\n",
    "\n",
    "# batchの各keyのsizeを確認\n",
    "for k, v in batch.items():\n",
    "    print(k, v.size())\n",
    "\n",
    "# batchの中身を確認\n",
    "pprint(batch[\"input_ids\"][0])\n",
    "print(batch[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 主に 子ども 向け の もの で 、 イラスト の つい た 物語 が 書か れ て いる もの は どれ? [SEP] 絵本 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "tensor([2, 1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# IDから元の文字列を復元\n",
    "print(tokenizer.decode(batch[\"input_ids\"][0][2]))\n",
    "\n",
    "# ラベルの確認\n",
    "print(batch[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['choice0', 'choice1', 'choice2', 'choice3', 'choice4'], id=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.features[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForMultipleChoice\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultipleChoiceModelOutput(loss=tensor(1.8538, grad_fn=<NllLossBackward0>), logits=tensor([[-0.5226, -0.4835, -0.5693, -0.5206, -0.5575],\n",
       "        [-0.7181, -0.9051,  0.1348, -0.6280, -0.1062],\n",
       "        [ 0.4246,  0.3564,  0.4028, -0.0059,  0.2815],\n",
       "        [ 0.4608,  0.4725,  0.4362,  0.0568,  0.2600]],\n",
       "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers_model_name = \"cl-tohoku/bert-base-japanese-v3\"\n",
    "\n",
    "\n",
    "model = AutoModelForMultipleChoice.from_pretrained(\n",
    "    transformers_model_name,\n",
    "    num_labels=train_dataset.features[\"label\"].num_classes,\n",
    ")\n",
    "\n",
    "print(type(model).__name__)\n",
    "\n",
    "# モデルの出力を確認\n",
    "outputs = model(**batch)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output_jcommonsenseqa\",  # 結果の保存フォルダ\n",
    "    per_device_train_batch_size=8,  # 訓練時のバッチサイズ\n",
    "    per_device_eval_batch_size=8,  # 評価時のバッチサイズ\n",
    "    learning_rate=2e-5,  # 学習率\n",
    "    lr_scheduler_type=\"linear\",  # 学習率スケジューラの種類\n",
    "    warmup_ratio=0.1,  # 学習率のウォームアップの長さを指定\n",
    "    num_train_epochs=3,  # エポック数\n",
    "    save_strategy=\"epoch\",  # チェックポイントの保存タイミング\n",
    "    logging_strategy=\"epoch\",  # ロギングのタイミング\n",
    "    evaluation_strategy=\"epoch\",  # 検証セットによる評価のタイミング\n",
    "    load_best_model_at_end=True,  # 訓練後に開発セットで最良のモデルをロード\n",
    "    metric_for_best_model=\"accuracy\",  # 最良のモデルを決定する評価指標\n",
    "    fp16=True,  # 自動混合精度演算の有効化\n",
    "    remove_unused_columns=False, # 入力データに含まれない列を削除するかどうか(https://qiita.com/m__k/items/2c4e476d7ac81a3a44af)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_accuracy(\n",
    "    eval_pred: tuple[np.ndarray, np.ndarray]\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"予測ラベルと正解ラベルから正解率を計算\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    # predictionsは各ラベルについてのスコア\n",
    "    # 最もスコアの高いインデックスを予測ラベルとする\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shuhei/.pyenv/versions/3.10.3/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c302929df864ed3b69ec7d1a7bc05a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7662, 'learning_rate': 1.4824387011265739e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad62d2059d0743f8a2071c08f2030bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4517534673213959, 'eval_accuracy': 0.8319928507596068, 'eval_runtime': 3.9359, 'eval_samples_per_second': 284.307, 'eval_steps_per_second': 35.57, 'epoch': 1.0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      2\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m      3\u001b[0m     train_dataset\u001b[39m=\u001b[39mtrain_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_accuracy,\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1536\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1537\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1538\u001b[0m )\n\u001b[0;32m-> 1539\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1540\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1541\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1542\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1543\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1544\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/transformers/trainer.py:1821\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1818\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1819\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss_step\n\u001b[0;32m-> 1821\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_flos \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfloating_point_ops(inputs))\n\u001b[1;32m   1823\u001b[0m is_last_step_and_steps_less_than_grad_acc \u001b[39m=\u001b[39m (\n\u001b[1;32m   1824\u001b[0m     steps_in_epoch \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39mand\u001b[39;00m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m steps_in_epoch\n\u001b[1;32m   1825\u001b[0m )\n\u001b[1;32m   1827\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1828\u001b[0m     total_batched_samples \u001b[39m%\u001b[39m args\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m   1829\u001b[0m     \u001b[39mor\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1833\u001b[0m     \u001b[39m# the `or` condition of `is_last_step_and_steps_less_than_grad_acc` is not covered\u001b[39;00m\n\u001b[1;32m   1834\u001b[0m     \u001b[39m# in accelerate. So, explicitly enable sync gradients to True in that case.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/transformers/trainer.py:3379\u001b[0m, in \u001b[0;36mTrainer.floating_point_ops\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3366\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3367\u001b[0m \u001b[39mFor models that inherit from [`PreTrainedModel`], uses that method to compute the number of floating point\u001b[39;00m\n\u001b[1;32m   3368\u001b[0m \u001b[39moperations for every backward + forward pass. If using another model, either implement such a method in the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3376\u001b[0m \u001b[39m    `int`: The number of floating-point operations.\u001b[39;00m\n\u001b[1;32m   3377\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3378\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39m\"\u001b[39m\u001b[39mfloating_point_ops\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 3379\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mfloating_point_ops(inputs)\n\u001b[1;32m   3380\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3381\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/transformers/modeling_utils.py:1021\u001b[0m, in \u001b[0;36mModuleUtilsMixin.floating_point_ops\u001b[0;34m(self, input_dict, exclude_embeddings)\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfloating_point_ops\u001b[39m(\n\u001b[1;32m    998\u001b[0m     \u001b[39mself\u001b[39m, input_dict: Dict[\u001b[39mstr\u001b[39m, Union[torch\u001b[39m.\u001b[39mTensor, Any]], exclude_embeddings: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    999\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m   1000\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m \u001b[39m    Get number of (optionally, non-embeddings) floating-point operations for the forward and backward passes of a\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m \u001b[39m    batch with this transformer model. Default approximation neglects the quadratic dependency on the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[39m        `int`: The number of floating-point operations.\u001b[39;00m\n\u001b[1;32m   1019\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m6\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimate_tokens(input_dict) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_parameters(exclude_embeddings\u001b[39m=\u001b[39;49mexclude_embeddings)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/transformers/modeling_utils.py:969\u001b[0m, in \u001b[0;36mModuleUtilsMixin.num_parameters\u001b[0;34m(self, only_trainable, exclude_embeddings)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[39mif\u001b[39;00m exclude_embeddings:\n\u001b[1;32m    966\u001b[0m     embedding_param_names \u001b[39m=\u001b[39m [\n\u001b[1;32m    967\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m.weight\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m name, module_type \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnamed_modules() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(module_type, nn\u001b[39m.\u001b[39mEmbedding)\n\u001b[1;32m    968\u001b[0m     ]\n\u001b[0;32m--> 969\u001b[0m     non_embedding_parameters \u001b[39m=\u001b[39m [\n\u001b[1;32m    970\u001b[0m         parameter \u001b[39mfor\u001b[39;00m name, parameter \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnamed_parameters() \u001b[39mif\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m embedding_param_names\n\u001b[1;32m    971\u001b[0m     ]\n\u001b[1;32m    972\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msum\u001b[39m(p\u001b[39m.\u001b[39mnumel() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m non_embedding_parameters \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mrequires_grad \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m only_trainable)\n\u001b[1;32m    973\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/transformers/modeling_utils.py:969\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[39mif\u001b[39;00m exclude_embeddings:\n\u001b[1;32m    966\u001b[0m     embedding_param_names \u001b[39m=\u001b[39m [\n\u001b[1;32m    967\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m.weight\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m name, module_type \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnamed_modules() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(module_type, nn\u001b[39m.\u001b[39mEmbedding)\n\u001b[1;32m    968\u001b[0m     ]\n\u001b[0;32m--> 969\u001b[0m     non_embedding_parameters \u001b[39m=\u001b[39m [\n\u001b[1;32m    970\u001b[0m         parameter \u001b[39mfor\u001b[39;00m name, parameter \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnamed_parameters() \u001b[39mif\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m embedding_param_names\n\u001b[1;32m    971\u001b[0m     ]\n\u001b[1;32m    972\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msum\u001b[39m(p\u001b[39m.\u001b[39mnumel() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m non_embedding_parameters \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mrequires_grad \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m only_trainable)\n\u001b[1;32m    973\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/torch/nn/modules/module.py:2115\u001b[0m, in \u001b[0;36mModule.named_parameters\u001b[0;34m(self, prefix, recurse, remove_duplicate)\u001b[0m\n\u001b[1;32m   2090\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Returns an iterator over module parameters, yielding both the\u001b[39;00m\n\u001b[1;32m   2091\u001b[0m \u001b[39mname of the parameter as well as the parameter itself.\u001b[39;00m\n\u001b[1;32m   2092\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2110\u001b[0m \n\u001b[1;32m   2111\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_named_members(\n\u001b[1;32m   2113\u001b[0m     \u001b[39mlambda\u001b[39;00m module: module\u001b[39m.\u001b[39m_parameters\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   2114\u001b[0m     prefix\u001b[39m=\u001b[39mprefix, recurse\u001b[39m=\u001b[39mrecurse, remove_duplicate\u001b[39m=\u001b[39mremove_duplicate)\n\u001b[0;32m-> 2115\u001b[0m \u001b[39myield from\u001b[39;00m gen\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/torch/nn/modules/module.py:2050\u001b[0m, in \u001b[0;36mModule._named_members\u001b[0;34m(self, get_members_fn, prefix, recurse, remove_duplicate)\u001b[0m\n\u001b[1;32m   2048\u001b[0m modules \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnamed_modules(prefix\u001b[39m=\u001b[39mprefix, remove_duplicate\u001b[39m=\u001b[39mremove_duplicate) \u001b[39mif\u001b[39;00m recurse \u001b[39melse\u001b[39;00m [(prefix, \u001b[39mself\u001b[39m)]\n\u001b[1;32m   2049\u001b[0m \u001b[39mfor\u001b[39;00m module_prefix, module \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m-> 2050\u001b[0m     members \u001b[39m=\u001b[39m get_members_fn(module)\n\u001b[1;32m   2051\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m members:\n\u001b[1;32m   2052\u001b[0m         \u001b[39mif\u001b[39;00m v \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m v \u001b[39min\u001b[39;00m memo:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/torch/nn/modules/module.py:2113\u001b[0m, in \u001b[0;36mModule.named_parameters.<locals>.<lambda>\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m   2084\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnamed_parameters\u001b[39m(\n\u001b[1;32m   2085\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m   2086\u001b[0m         prefix: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   2087\u001b[0m         recurse: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   2088\u001b[0m         remove_duplicate: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   2089\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[Tuple[\u001b[39mstr\u001b[39m, Parameter]]:\n\u001b[1;32m   2090\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Returns an iterator over module parameters, yielding both the\u001b[39;00m\n\u001b[1;32m   2091\u001b[0m \u001b[39m    name of the parameter as well as the parameter itself.\u001b[39;00m\n\u001b[1;32m   2092\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2110\u001b[0m \n\u001b[1;32m   2111\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m     gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_named_members(\n\u001b[0;32m-> 2113\u001b[0m         \u001b[39mlambda\u001b[39;00m module: module\u001b[39m.\u001b[39;49m_parameters\u001b[39m.\u001b[39;49mitems(),\n\u001b[1;32m   2114\u001b[0m         prefix\u001b[39m=\u001b[39mprefix, recurse\u001b[39m=\u001b[39mrecurse, remove_duplicate\u001b[39m=\u001b[39mremove_duplicate)\n\u001b[1;32m   2115\u001b[0m     \u001b[39myield from\u001b[39;00m gen\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_accuracy,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7421a72a7e604c069040dc5c15f37905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3.0,\n",
      " 'eval_accuracy': 0.837354781054513,\n",
      " 'eval_loss': 0.5849103331565857,\n",
      " 'eval_runtime': 4.0165,\n",
      " 'eval_samples_per_second': 278.599,\n",
      " 'eval_steps_per_second': 34.856}\n"
     ]
    }
   ],
   "source": [
    "# 検証セットでモデルを評価\n",
    "eval_metrics = trainer.evaluate(valid_dataset)\n",
    "pprint(eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
