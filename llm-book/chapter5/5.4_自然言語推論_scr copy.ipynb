{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from datasets import load_dataset\n",
    "from transformers import BatchEncoding, AutoTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset('llm-book/JGLUE', name='JNLI', split='train')\n",
    "valid_dataset = load_dataset('llm-book/JGLUE', name='JNLI', split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence_pair_id': '0',\n",
       " 'yjcaptions_id': '100124-104404-104405',\n",
       " 'sentence1': '二人の男性がジャンボジェット機を見ています。',\n",
       " 'sentence2': '2人の男性が、白い飛行機を眺めています。',\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassLabel(names=['entailment', 'contradiction', 'neutral'], id=None)\n",
      "{'sentence_pair_id': '1', 'yjcaptions_id': '100124-104405-104404', 'sentence1': '2人の男性が、白い飛行機を眺めています。', 'sentence2': '二人の男性がジャンボジェット機を見ています。', 'label': 2}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.features[\"label\"])\n",
    "print(train_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence_pair_id', 'yjcaptions_id', 'sentence1', 'sentence2', 'label'],\n",
       "    num_rows: 20073\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model_name = \"cl-tohoku/bert-base-japanese-v3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for row in tqdm(valid_dataset):\n",
    "#     print(row[\"sentence1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 20073/20073 [00:01<00:00, 16826.86it/s]\n",
      "\n",
      "\u001b[A\n",
      "100%|██████████| 2434/2434 [00:00<00:00, 14818.15it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import IterableDataset, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Dataset1(IterableDataset):\n",
    "    def __init__(self, ds):\n",
    "        self.features = [\n",
    "            {\n",
    "                'sentence_pair_id': row['sentence_pair_id'],\n",
    "                'yjcaptions_id': row['yjcaptions_id'],\n",
    "                'sentence1': row['sentence1'],\n",
    "                'sentence2': row['sentence2'],\n",
    "                'label': row['label']\n",
    "            } for row in tqdm(ds)\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.features)\n",
    "\n",
    "train_dataset1 = Dataset1(train_dataset)\n",
    "valid_dataset1 = Dataset1(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20073\n",
      "{'sentence_pair_id': '0', 'yjcaptions_id': '100124-104404-104405', 'sentence1': '二人の男性がジャンボジェット機を見ています。', 'sentence2': '2人の男性が、白い飛行機を眺めています。', 'label': 2}\n",
      "{'sentence_pair_id': '0', 'yjcaptions_id': '100124-104404-104405', 'sentence1': '二人の男性がジャンボジェット機を見ています。', 'sentence2': '2人の男性が、白い飛行機を眺めています。', 'label': 2}\n"
     ]
    }
   ],
   "source": [
    "# datasetのサイズを確認\n",
    "print(len(train_dataset1))\n",
    "# datasetの中身を確認\n",
    "tmp = next(iter(train_dataset1))\n",
    "print(tmp)\n",
    "\n",
    "tmp = next(iter(train_dataset))\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence_pair_id': '0',\n",
       "  'yjcaptions_id': '100124-104404-104405',\n",
       "  'sentence1': '二人の男性がジャンボジェット機を見ています。',\n",
       "  'sentence2': '2人の男性が、白い飛行機を眺めています。',\n",
       "  'label': 2},\n",
       " {'sentence_pair_id': '1',\n",
       "  'yjcaptions_id': '100124-104405-104404',\n",
       "  'sentence1': '2人の男性が、白い飛行機を眺めています。',\n",
       "  'sentence2': '二人の男性がジャンボジェット機を見ています。',\n",
       "  'label': 2},\n",
       " {'sentence_pair_id': '2',\n",
       "  'yjcaptions_id': '100142-104431-104432',\n",
       "  'sentence1': '男性が子供を抱き上げて立っています。',\n",
       "  'sentence2': '坊主頭の男性が子供を抱いて立っています。',\n",
       "  'label': 2},\n",
       " {'sentence_pair_id': '3',\n",
       "  'yjcaptions_id': '100142-104432-104431',\n",
       "  'sentence1': '坊主頭の男性が子供を抱いて立っています。',\n",
       "  'sentence2': '男性が子供を抱き上げて立っています。',\n",
       "  'label': 0}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset1)[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class DataCollator1():\n",
    "    def __init__(self, tokenizer, max_length=256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        # sentence1とsentence2を連結し、encodingsを返す\n",
    "        examples = {\n",
    "            'sentence1': list(map(lambda x: x['sentence1'], examples)),\n",
    "            'sentence2': list(map(lambda x: x['sentence2'], examples)),\n",
    "            'label': list(map(lambda x: x['label'], examples)),\n",
    "        }\n",
    "        encodings = self.tokenizer(\n",
    "                                   examples['sentence1'],\n",
    "                                   examples['sentence2'],\n",
    "                                   padding=True, \n",
    "                                   truncation=True,\n",
    "                                   max_length=self.max_length,\n",
    "                                   return_tensors='pt')\n",
    "\n",
    "        encodings['labels'] = torch.tensor(examples['label'])\n",
    "        return encodings\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_model_name)\n",
    "data_collator = DataCollator1(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([8, 36])\n",
      "token_type_ids torch.Size([8, 36])\n",
      "attention_mask torch.Size([8, 36])\n",
      "labels torch.Size([8])\n",
      "tensor([    2, 27714,  6589,   464, 13341,   430, 13275,   500, 16563,   456,\n",
      "        16996,   456,   422, 12995,   385,     3, 13341,   430, 13275,   500,\n",
      "        18967, 12867,   456, 16996,   456,   422, 12995,   385,     3,     0,\n",
      "            0,     0,     0,     0,     0,     0])\n",
      "tensor([2, 2, 2, 0, 2, 2, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "loader = DataLoader(train_dataset1, collate_fn=data_collator, batch_size=8)\n",
    "batch = next(iter(loader))\n",
    "# batch = next(iter(loader))\n",
    "\n",
    "# batchの各keyのsizeを確認\n",
    "for k, v in batch.items():\n",
    "    print(k, v.size())\n",
    "\n",
    "# batchの中身を確認\n",
    "pprint(batch[\"input_ids\"][3])\n",
    "print(batch[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['entailment', 'contradiction', 'neutral'], id=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.features[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v3 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.8577, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2862, -0.1456,  0.5988],\n",
       "        [-0.2874, -0.1582,  0.6100],\n",
       "        [-0.2977, -0.1579,  0.5402],\n",
       "        [-0.2814, -0.1529,  0.5309],\n",
       "        [-0.2276, -0.1309,  0.5875],\n",
       "        [-0.2257, -0.1272,  0.5945],\n",
       "        [-0.2463, -0.1110,  0.5373],\n",
       "        [-0.2219, -0.0948,  0.5658]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "class_label = train_dataset.features[\"label\"]\n",
    "label2id = {label: id for id, label in enumerate(class_label.names)}\n",
    "id2label = {id: label for id, label in enumerate(class_label.names)}\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    transformer_model_name,\n",
    "    num_labels=class_label.num_classes,\n",
    "    label2id=label2id,  # ラベル名からIDへの対応を指定\n",
    "    id2label=id2label,  # IDからラベル名への対応を指定\n",
    ")\n",
    "print(type(model).__name__)\n",
    "\n",
    "# モデルの出力を確認\n",
    "outputs = model(**batch)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output_jnli\",  # 結果の保存フォルダ\n",
    "    per_device_train_batch_size=8,  # 訓練時のバッチサイズ\n",
    "    per_device_eval_batch_size=8,  # 評価時のバッチサイズ\n",
    "    learning_rate=2e-5,  # 学習率\n",
    "    lr_scheduler_type=\"linear\",  # 学習率スケジューラの種類\n",
    "    warmup_ratio=0.1,  # 学習率のウォームアップの長さを指定\n",
    "    num_train_epochs=3,  # エポック数\n",
    "    label_names=['labels'],  # ラベル名を指定\n",
    "    save_strategy=\"epoch\",  # チェックポイントの保存タイミング\n",
    "    logging_strategy=\"epoch\",  # ロギングのタイミング\n",
    "    evaluation_strategy=\"epoch\",  # 検証セットによる評価のタイミング\n",
    "    load_best_model_at_end=True,  # 訓練後に開発セットで最良のモデルをロード\n",
    "    metric_for_best_model=\"accuracy\",  # 最良のモデルを決定する評価指標\n",
    "    fp16=True,  # 自動混合精度演算の有効化\n",
    "    remove_unused_columns=False, # 入力データに含まれない列を削除するかどうか(https://qiita.com/m__k/items/2c4e476d7ac81a3a44af)\n",
    ")\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./output/model',\n",
    "#     evaluation_strategy='epoch',    # 検証セットによる評価のタイミング\n",
    "#     logging_strategy='epoch',   # ロギングのタイミング\n",
    "#     save_strategy='epoch',  # チェックポイントの保存タイミング\n",
    "#     # save_total_limit=1,\n",
    "#     label_names=['labels'], # ラベル名を指定\n",
    "#     lr_scheduler_type='linear',   # 学習率スケジューラの種類\n",
    "#     metric_for_best_model='accuracy',   # 最良のモデルを決定する評価指標\n",
    "#     load_best_model_at_end=True,    # 訓練後に開発セットで最良のモデルをロード\n",
    "#     per_device_train_batch_size=16, # 訓練時のバッチサイズ\n",
    "#     per_device_eval_batch_size=16,  # 評価時のバッチサイズ\n",
    "#     num_train_epochs=5,\n",
    "#     # remove_unused_columns=False,\n",
    "#     # report_to='none'\n",
    "#     fp16=True,  # 自動混合精度演算の有効化\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_accuracy(\n",
    "    eval_pred: tuple[np.ndarray, np.ndarray]\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"予測ラベルと正解ラベルから正解率を計算\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    # predictionsは各ラベルについてのスコア\n",
    "    # 最もスコアの高いインデックスを予測ラベルとする\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c9fe3d7fddb4561ae2b79d7abfc7679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4928, 'learning_rate': 1.4826619448133392e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e89abba72f4f5b9a5ceae89f691ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/305 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.47213637828826904, 'eval_accuracy': 0.866885784716516, 'eval_runtime': 5.225, 'eval_samples_per_second': 465.837, 'eval_steps_per_second': 58.373, 'epoch': 1.0}\n",
      "{'loss': 0.27, 'learning_rate': 7.42216319905563e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "120e86274bb347e78f44ceb1b099a8d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/305 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.42057183384895325, 'eval_accuracy': 0.9018077239112572, 'eval_runtime': 4.4595, 'eval_samples_per_second': 545.8, 'eval_steps_per_second': 68.393, 'epoch': 2.0}\n",
      "{'loss': 0.1439, 'learning_rate': 2.06581083075107e-08, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c4c04a2cb443d599fa3b3434ae2adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/305 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5329641103744507, 'eval_accuracy': 0.900164338537387, 'eval_runtime': 4.3969, 'eval_samples_per_second': 553.573, 'eval_steps_per_second': 69.367, 'epoch': 3.0}\n",
      "{'train_runtime': 695.4093, 'train_samples_per_second': 86.595, 'train_steps_per_second': 10.828, 'train_loss': 0.30223845827626994, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7530, training_loss=0.30223845827626994, metrics={'train_runtime': 695.4093, 'train_samples_per_second': 86.595, 'train_steps_per_second': 10.828, 'train_loss': 0.30223845827626994, 'epoch': 3.0})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=list(train_dataset1),\n",
    "    eval_dataset=list(valid_dataset1),\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_accuracy,\n",
    ")\n",
    "# trainer.train()\n",
    "trainer.train(ignore_keys_for_eval=['last_hidden_state', 'hidden_states', 'attentions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baca992fab924b69a8f0ac8cbe4ef61e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/305 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3.0,\n",
      " 'eval_accuracy': 0.9018077239112572,\n",
      " 'eval_loss': 0.42057183384895325,\n",
      " 'eval_runtime': 5.1817,\n",
      " 'eval_samples_per_second': 469.73,\n",
      " 'eval_steps_per_second': 58.861}\n"
     ]
    }
   ],
   "source": [
    "# 検証セットでモデルを評価\n",
    "eval_metrics = trainer.evaluate(valid_dataset)\n",
    "pprint(eval_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
