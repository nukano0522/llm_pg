{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from datasets import load_dataset\n",
    "from transformers import BatchEncoding, AutoTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset('llm-book/JGLUE', name='JSTS', split='train')\n",
    "valid_dataset = load_dataset('llm-book/JGLUE', name='JSTS', split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence_pair_id': '100', 'yjcaptions_id': '103678-107788-107792', 'sentence1': '線路の上を黄色い電車が走っています。', 'sentence2': '線路の上を黄色い電車が走っています。', 'label': 4.800000190734863}\n",
      "{'sentence_pair_id': '100', 'yjcaptions_id': '133467-2721-2724', 'sentence1': 'ベンチに座っている人の前にハトが数羽います。', 'sentence2': 'ベンチに座った人の前に２羽のハトがいます。', 'label': 3.799999952316284}\n"
     ]
    }
   ],
   "source": [
    "idx = 100\n",
    "print(train_dataset[idx])\n",
    "print(valid_dataset[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model_name = \"cl-tohoku/bert-base-japanese-v3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12451/12451 [00:00<00:00, 15779.94it/s]\n",
      "100%|██████████| 1457/1457 [00:00<00:00, 13794.53it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import IterableDataset, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Dataset1(IterableDataset):\n",
    "    def __init__(self, ds):\n",
    "        self.features = [\n",
    "            {\n",
    "                'sentence_pair_id': row['sentence_pair_id'],\n",
    "                'yjcaptions_id': row['yjcaptions_id'],\n",
    "                'sentence1': row['sentence1'],\n",
    "                'sentence2': row['sentence2'],\n",
    "                'label': row['label']\n",
    "            } for row in tqdm(ds)\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.features)\n",
    "\n",
    "train_dataset1 = Dataset1(train_dataset)\n",
    "valid_dataset1 = Dataset1(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12451\n",
      "{'sentence_pair_id': '0', 'yjcaptions_id': '10005_480798-10996-92616', 'sentence1': '川べりでサーフボードを持った人たちがいます。', 'sentence2': 'トイレの壁に黒いタオルがかけられています。', 'label': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# datasetのサイズを確認\n",
    "print(len(train_dataset1))\n",
    "# datasetの中身を確認\n",
    "tmp = next(iter(train_dataset1))\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class DataCollator1():\n",
    "    def __init__(self, tokenizer, max_length=256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        # sentence1とsentence2を連結し、encodingsを返す\n",
    "        examples = {\n",
    "            'sentence1': list(map(lambda x: x['sentence1'], examples)),\n",
    "            'sentence2': list(map(lambda x: x['sentence2'], examples)),\n",
    "            'label': list(map(lambda x: x['label'], examples)),\n",
    "        }\n",
    "        encodings = self.tokenizer(\n",
    "                                   examples['sentence1'],\n",
    "                                   examples['sentence2'],\n",
    "                                   padding=True, \n",
    "                                   truncation=True,\n",
    "                                   max_length=self.max_length,\n",
    "                                   return_tensors='pt')\n",
    "\n",
    "        encodings['labels'] = torch.tensor(examples['label'])\n",
    "        return encodings\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_model_name)\n",
    "data_collator = DataCollator1(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([4, 32])\n",
      "token_type_ids torch.Size([4, 32])\n",
      "attention_mask torch.Size([4, 32])\n",
      "labels torch.Size([4])\n",
      "tensor([    2, 13341,   430, 13275,   500, 18967, 12867,   456, 16996,   456,\n",
      "          422, 12995,   385,     3, 19898,   500,  1428,  7213, 16629,   464,\n",
      "         1286,  9729,   722,   430, 12598,   494,   456,   422, 12995,   385,\n",
      "            3,     0])\n",
      "tensor([0.0000, 3.8000, 4.0000, 0.2000])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "loader = DataLoader(train_dataset1, collate_fn=data_collator, batch_size=4)\n",
    "batch = next(iter(loader))\n",
    "# batch = next(iter(loader))\n",
    "\n",
    "# batchの各keyのsizeを確認\n",
    "for k, v in batch.items():\n",
    "    print(k, v.size())\n",
    "\n",
    "# batchの中身を確認\n",
    "pprint(batch[\"input_ids\"][3])\n",
    "print(batch[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(dtype='float32', id=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.features[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v3 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(7.6491, grad_fn=<MseLossBackward0>), logits=tensor([[ 0.0274],\n",
       "        [-0.0238],\n",
       "        [ 0.0057],\n",
       "        [ 0.0597]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "transformers_model_name = \"cl-tohoku/bert-base-japanese-v3\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    transformers_model_name,\n",
    "    num_labels=1,\n",
    "    problem_type=\"regression\",\n",
    ")\n",
    "\n",
    "print(type(model).__name__)\n",
    "\n",
    "# モデルの出力を確認\n",
    "outputs = model(**batch)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output_jsts\",  # 結果の保存フォルダ\n",
    "    per_device_train_batch_size=32,  # 訓練時のバッチサイズ\n",
    "    per_device_eval_batch_size=32,  # 評価時のバッチサイズ\n",
    "    learning_rate=2e-5,  # 学習率\n",
    "    lr_scheduler_type=\"linear\",  # 学習率スケジューラの種類\n",
    "    warmup_ratio=0.1,  # 学習率のウォームアップの長さを指定\n",
    "    num_train_epochs=3,  # エポック数\n",
    "    save_strategy=\"epoch\",  # チェックポイントの保存タイミング\n",
    "    logging_strategy=\"epoch\",  # ロギングのタイミング\n",
    "    evaluation_strategy=\"epoch\",  # 検証セットによる評価のタイミング\n",
    "    load_best_model_at_end=True,  # 訓練後に開発セットで最良のモデルをロード\n",
    "    metric_for_best_model=\"spearmanr\",  # 最良のモデルを決定する評価指標\n",
    "    fp16=True,  # 自動混合精度演算の有効化\n",
    "    remove_unused_columns=False, # 入力データに含まれない列を削除するかどうか(https://qiita.com/m__k/items/2c4e476d7ac81a3a44af)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "def compute_correlation_metrics(\n",
    "    eval_pred: tuple[np.ndarray, np.ndarray]\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"予測スコアと正解スコアから各種相関係数を計算\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.squeeze(1)\n",
    "    return {\n",
    "        \"pearsonr\": pearsonr(predictions, labels).statistic,\n",
    "        \"spearmanr\": spearmanr(predictions, labels).statistic,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shuhei/.pyenv/versions/3.10.3/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ed10681b5f4e09b6893f652ec7ed83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.107, 'learning_rate': 1.4871794871794874e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "764ee6efe1eb4e00b9e01db2cce5132c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.46136826276779175, 'eval_pearsonr': 0.8956831678327875, 'eval_spearmanr': 0.8512333491469819, 'eval_runtime': 1.9739, 'eval_samples_per_second': 738.137, 'eval_steps_per_second': 23.304, 'epoch': 1.0}\n",
      "{'loss': 0.3723, 'learning_rate': 7.4833808167141505e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee36f099e8b4e8e8cfe428c505b2c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3786185085773468, 'eval_pearsonr': 0.9085512049549194, 'eval_spearmanr': 0.8669636297172041, 'eval_runtime': 1.9249, 'eval_samples_per_second': 756.924, 'eval_steps_per_second': 23.897, 'epoch': 2.0}\n",
      "{'loss': 0.2762, 'learning_rate': 7.597340930674265e-08, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26e1a1ca01a4b6b925d32952d9cab92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3605651557445526, 'eval_pearsonr': 0.9111519230333195, 'eval_spearmanr': 0.8713183009329197, 'eval_runtime': 1.9451, 'eval_samples_per_second': 749.075, 'eval_steps_per_second': 23.65, 'epoch': 3.0}\n",
      "{'train_runtime': 271.8541, 'train_samples_per_second': 137.401, 'train_steps_per_second': 4.304, 'train_loss': 0.5851514441335303, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1170, training_loss=0.5851514441335303, metrics={'train_runtime': 271.8541, 'train_samples_per_second': 137.401, 'train_steps_per_second': 4.304, 'train_loss': 0.5851514441335303, 'epoch': 3.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset1,\n",
    "    eval_dataset=valid_dataset1,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_correlation_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec05bd37a8e46339f5b8f76814fc6d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3.0,\n",
      " 'eval_loss': 0.3605651557445526,\n",
      " 'eval_pearsonr': 0.9111519230333195,\n",
      " 'eval_runtime': 2.029,\n",
      " 'eval_samples_per_second': 718.091,\n",
      " 'eval_spearmanr': 0.8713183009329197,\n",
      " 'eval_steps_per_second': 22.671}\n"
     ]
    }
   ],
   "source": [
    "# 検証セットでモデルを評価\n",
    "eval_metrics = trainer.evaluate(valid_dataset)\n",
    "pprint(eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
