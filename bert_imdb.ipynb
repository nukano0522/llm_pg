{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert_imdb.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1LjtnrCnWv_P-0U3pFrMSZJVFYyFcS5N6","authorship_tag":"ABX9TyPKqES3o6wNmYbswijA+eTS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MsAbHBeCF-Ef","executionInfo":{"status":"ok","timestamp":1658494306108,"user_tz":-540,"elapsed":18066,"user":{"displayName":"中野修平","userId":"17667180361716214935"}},"outputId":"1f4175a0-6b48-4149-fdeb-82ebee2ba9a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==4.5.0\n","  Downloading transformers-4.5.0-py3-none-any.whl (2.1 MB)\n","\u001b[K     |████████████████████████████████| 2.1 MB 7.2 MB/s \n","\u001b[?25hCollecting fugashi==1.1.0\n","  Downloading fugashi-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (486 kB)\n","\u001b[K     |████████████████████████████████| 486 kB 56.4 MB/s \n","\u001b[?25hCollecting ipadic==1.0.0\n","  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n","\u001b[K     |████████████████████████████████| 13.4 MB 37.6 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (2022.6.2)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 37.2 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (3.7.1)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 38.0 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (4.12.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (1.21.6)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (21.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (4.64.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.5.0) (3.8.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.5.0) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.0) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (2022.6.15)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0) (1.1.0)\n","Building wheels for collected packages: ipadic, sacremoses\n","  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556723 sha256=75083d7b15d6f4c657a131f226710caaef0ba78fc956f301dc23d3456ac91dd4\n","  Stored in directory: /root/.cache/pip/wheels/33/8b/99/cf0d27191876637cd3639a560f93aa982d7855ce826c94348b\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=0775d5c4a34a5f6824df02e3666bb74ce6934f43ba5706bc4e79e72dbd65c4fb\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built ipadic sacremoses\n","Installing collected packages: tokenizers, sacremoses, transformers, ipadic, fugashi\n","Successfully installed fugashi-1.1.0 ipadic-1.0.0 sacremoses-0.0.53 tokenizers-0.10.3 transformers-4.5.0\n"]}],"source":["!pip install transformers==4.5.0 fugashi==1.1.0 ipadic==1.0.0 "]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import string\n","import re\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoModel, AutoTokenizer, BertJapaneseTokenizer, BertModel\n","from torch import cuda\n","import sklearn.metrics as skm\n","from sklearn.model_selection import train_test_split\n","import torch.nn.functional as F\n","from transformers import logging\n"],"metadata":{"id":"pnCaB1WEG0dr","executionInfo":{"status":"ok","timestamp":1658494407767,"user_tz":-540,"elapsed":4448,"user":{"displayName":"中野修平","userId":"17667180361716214935"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["batch_size = 16\n","max_len = 512"],"metadata":{"id":"s0DTlUuXaQk0","executionInfo":{"status":"ok","timestamp":1658497272033,"user_tz":-540,"elapsed":486,"user":{"displayName":"中野修平","userId":"17667180361716214935"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["df_train = pd.read_csv(\"./drive/MyDrive/Colab_Notebooks/data/IMDb/IMDb_train.tsv\", sep=\"\\t\", header=None)\n","df_train = df_train.iloc[:, 0:2]\n","df_train.columns = [\"text\", \"label\"]\n","print(df_train.shape)\n","df_train.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":224},"id":"jPzhLL26GLCB","executionInfo":{"status":"ok","timestamp":1658497275712,"user_tz":-540,"elapsed":623,"user":{"displayName":"中野修平","userId":"17667180361716214935"}},"outputId":"7641ca68-e344-41bb-e37e-365b8dceeb9f"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["(25000, 2)\n"]},{"output_type":"execute_result","data":{"text/plain":["                                                text  label\n","0  The Unborn is a pretty good low-budget horror ...      1\n","1  Vincente Minnelli directed some of the most ce...      1\n","2  The first time I saw this, I didn't laugh too ...      1\n","3  This is a great movie for all Generation X'ers...      1\n","4  I first saw this absolutely riveting documenta...      1"],"text/html":["\n","  <div id=\"df-e8cf381a-020c-4252-a9dd-744d74b46176\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The Unborn is a pretty good low-budget horror ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Vincente Minnelli directed some of the most ce...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>The first time I saw this, I didn't laugh too ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>This is a great movie for all Generation X'ers...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>I first saw this absolutely riveting documenta...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e8cf381a-020c-4252-a9dd-744d74b46176')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e8cf381a-020c-4252-a9dd-744d74b46176 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e8cf381a-020c-4252-a9dd-744d74b46176');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["df_test = pd.read_csv(\"./drive/MyDrive/Colab_Notebooks/data/IMDb/IMDb_test.tsv\", sep=\"\\t\", header=None)\n","df_test = df_test.iloc[:, 0:2]\n","df_test.columns = [\"text\", \"label\"]\n","print(df_test.shape)\n","df_test.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":224},"id":"bYE3xlVPNAUa","executionInfo":{"status":"ok","timestamp":1658497276163,"user_tz":-540,"elapsed":461,"user":{"displayName":"中野修平","userId":"17667180361716214935"}},"outputId":"be2d633e-4290-4ec8-b3ec-5489c4bd37a8"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["(25000, 2)\n"]},{"output_type":"execute_result","data":{"text/plain":["                                                text  label\n","0  Susan Sarandon is, for lack of a better word, ...      1\n","1  Seeing Laurel without Hardy in a film seems st...      1\n","2  I was recently at a sleepover birthday party w...      1\n","3  This movie took me by surprise. The opening cr...      1\n","4  A widely unknown strange little western with m...      1"],"text/html":["\n","  <div id=\"df-bb9134d5-ef48-415d-bee8-fae00bffdc0e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Susan Sarandon is, for lack of a better word, ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Seeing Laurel without Hardy in a film seems st...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I was recently at a sleepover birthday party w...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>This movie took me by surprise. The opening cr...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>A widely unknown strange little western with m...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bb9134d5-ef48-415d-bee8-fae00bffdc0e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-bb9134d5-ef48-415d-bee8-fae00bffdc0e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-bb9134d5-ef48-415d-bee8-fae00bffdc0e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["df_train[\"label\"].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lNYGDgxARBPN","executionInfo":{"status":"ok","timestamp":1658497276431,"user_tz":-540,"elapsed":4,"user":{"displayName":"中野修平","userId":"17667180361716214935"}},"outputId":"d150a9b5-1e46-49cc-f031-9f7678bee1b4"},"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1    12500\n","0    12500\n","Name: label, dtype: int64"]},"metadata":{},"execution_count":50}]},{"cell_type":"code","source":["df_train[\"text\"].map(len).median()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KFjEF1KxRQ6M","executionInfo":{"status":"ok","timestamp":1658497277083,"user_tz":-540,"elapsed":9,"user":{"displayName":"中野修平","userId":"17667180361716214935"}},"outputId":"0bd04061-d515-4a2e-9f99-fc41bf9bd2e2"},"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["979.0"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["df_train[\"text\"].map(len)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7w6htWkkRnp_","executionInfo":{"status":"ok","timestamp":1658497277390,"user_tz":-540,"elapsed":3,"user":{"displayName":"中野修平","userId":"17667180361716214935"}},"outputId":"97e77c60-23a9-4f49-f533-e2afc18255b2"},"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0         639\n","1        4018\n","2         921\n","3        1582\n","4        1526\n","         ... \n","24995     472\n","24996     751\n","24997     964\n","24998    6041\n","24999    1405\n","Name: text, Length: 25000, dtype: int64"]},"metadata":{},"execution_count":52}]},{"cell_type":"markdown","source":["# 前処理"],"metadata":{"id":"P5EaZRBqNGky"}},{"cell_type":"code","source":["def preprocessing_text(text):\n","    # 改行コードを消去\n","    text = re.sub('<br />', '', text)\n","\n","    # カンマ、ピリオド以外の記号をスペースに置換\n","    for p in string.punctuation:\n","        if (p == \".\") or (p == \",\"):\n","            continue\n","        else:\n","            text = text.replace(p, \" \")\n","\n","    # ピリオドなどの前後にはスペースを入れておく\n","    text = text.replace(\".\", \" . \")\n","    text = text.replace(\",\", \" , \")\n","    return text\n"],"metadata":{"id":"JzXfSVD7NL3F","executionInfo":{"status":"ok","timestamp":1658497277928,"user_tz":-540,"elapsed":3,"user":{"displayName":"中野修平","userId":"17667180361716214935"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["df_train[\"text\"].map(preprocessing_text)\n","preprocessing_text(df_train[\"text\"][0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"id":"vFx55HH9NX0H","executionInfo":{"status":"ok","timestamp":1658497278948,"user_tz":-540,"elapsed":322,"user":{"displayName":"中野修平","userId":"17667180361716214935"}},"outputId":"e948ca1c-738c-455d-ef72-0caac49a4c76"},"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'The Unborn is a pretty good low budget horror movie exploiting the fears associated with pregnancy .  It s very well acted by the always good Brooke Adams and b movie stalwart James Karen ,  although the supporting cast is pretty average for a b grader .  The music ,  by Gary Numan of all people ,  is good too .  Henry Dominic s script is quite intelligent for this sort of thing ,  although there is a hint of misogyny about it .  Rodman Fender s direction is merely adequate ,  and there are some unnecessary cheap scares .  If you re a fan of Adams ,  whose movie career is nowhere near as illustrious as it should be ,  check it out  she s great ,  as always . '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":54}]},{"cell_type":"markdown","source":["# データセット作成"],"metadata":{"id":"cFHKTHZ4NRyr"}},{"cell_type":"code","source":["class CreateDataset(Dataset):\n","  def __init__(self, X, y, tokenizer, max_len):\n","    self.X = X\n","    self.y = y\n","    # self.uid = uid\n","    self.tokenizer = tokenizer\n","    self.max_len = max_len\n","\n","  def __len__(self):\n","    return len(self.y)\n","\n","  def encode(self, tokenizer, text):\n","\n","      # 前処理\n","      text = preprocessing_text(text)\n","      \n","\n","      inputs = tokenizer.encode_plus(\n","          text,\n","          add_special_tokens=True,\n","          max_length=self.max_len,\n","          padding = 'max_length',\n","          truncation = True\n","      )\n","      return inputs\n","\n","  def __getitem__(self, index):\n","    text = self.X[index]\n","    label = self.y[index]\n","    # userID = self.uid[index]\n","    ids = []\n","    mask = []\n","    inputs = self.encode(tokenizer=self.tokenizer, text=text)\n","    ids.append(torch.LongTensor(inputs['input_ids']))\n","    mask.append(torch.LongTensor(inputs['attention_mask']))\n","\n","    return {\n","      'ids': ids,\n","      'mask': mask,\n","      'label': label,\n","      'text':text,\n","      # 'userID':userID\n","    }"],"metadata":{"id":"B7aXkzLgG9za","executionInfo":{"status":"ok","timestamp":1658497278949,"user_tz":-540,"elapsed":8,"user":{"displayName":"中野修平","userId":"17667180361716214935"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["# tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"],"metadata":{"id":"bdoMudggG_T8","executionInfo":{"status":"ok","timestamp":1658497281109,"user_tz":-540,"elapsed":1831,"user":{"displayName":"中野修平","userId":"17667180361716214935"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["X = df_train[\"text\"].values\n","y = df_train[\"label\"].values"],"metadata":{"id":"X2yZh73HG_YC","executionInfo":{"status":"ok","timestamp":1658497281110,"user_tz":-540,"elapsed":19,"user":{"displayName":"中野修平","userId":"17667180361716214935"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["X_train, X_eval, y_train, y_eval = train_test_split(X, y, train_size=0.75)\n","\n","print(len(X_train))\n","print(len(y_train))\n","print(len(X_eval))\n","print(len(y_eval))\n","\n","X_test = df_test[\"text\"].values\n","y_test = df_test[\"label\"].values\n","print(len(X_test))\n","print(len(y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KuZghJ37RwyC","executionInfo":{"status":"ok","timestamp":1658497281110,"user_tz":-540,"elapsed":17,"user":{"displayName":"中野修平","userId":"17667180361716214935"}},"outputId":"807f1745-ecee-4d96-9c69-0ad4630eda2a"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["18750\n","18750\n","6250\n","6250\n","25000\n","25000\n"]}]},{"cell_type":"code","source":["dataset_train = CreateDataset(X_train, y_train, tokenizer, max_len=max_len)\n","dataset_eval = CreateDataset(X_eval, y_eval, tokenizer, max_len=max_len)\n","dataset_test = CreateDataset(X_test, y_test, tokenizer, max_len=max_len)\n","\n","print(dataset_train.__len__())\n","print(dataset_eval.__len__())\n","print(dataset_test.__len__())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F5PTf6oiG_bd","executionInfo":{"status":"ok","timestamp":1658497281111,"user_tz":-540,"elapsed":13,"user":{"displayName":"中野修平","userId":"17667180361716214935"}},"outputId":"05c9377b-b785-4243-b74c-025dc48fd9ad"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["18750\n","6250\n","25000\n"]}]},{"cell_type":"code","source":["dataset_train[1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pVD_X9ipXi6m","executionInfo":{"status":"ok","timestamp":1658497281835,"user_tz":-540,"elapsed":327,"user":{"displayName":"中野修平","userId":"17667180361716214935"}},"outputId":"9114397d-68a1-4aab-fc88-c7a19447f4b8"},"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'ids': [tensor([  101, 11121,  1106,   140, 23156,  1179,  1118,  1103,  2161,  1110,\n","           5203,  1141,  1104,  1109, 17751, 22087, 15966,  1116,  1115,  2825,\n","           3374,  1146,  1106,  1109,  7267,   119,  1135,  6467,  2117, 20164,\n","          26271,   117,  2750, 15463, 21643,   117,  8835,  8540,  4808,   117,\n","           1105,  2096, 10176,   117,   170,   153,  5821,  8401, 14613,   119,\n","           1327,  1132,  1284, 20801,   146,  9471,  4302,   119,  9656,  1195,\n","           2372,  8123,  1114,   170,  6844,  7277,  6094,  2227,  1104, 16455,\n","           1105,   151, 17294,  2340,  1272,  1135,  3982, 10865,  1111,  4552,\n","            119,  4981,  6819,  1141,  2096,  1109,  1798, 22087, 15966,  1116,\n","            117,  1122,  1110,  1141,  1104,  1109,  1798,   157, 24657,  1468,\n","           1106,  8553,  1112,   170,  3921,   119, 11336,  8178,  2354,  4902,\n","           1111,  6064,   119,   102,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0])],\n"," 'label': 1,\n"," 'mask': [tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0])],\n"," 'text': 'Return to Cabin by the Lake is Perhaps one of The Few Sequels that Can Live up to The Original. It Had Black Humor, Good Suspense, Nice Looking Girls, and Of Course, a Psycho Killer. What are We Missing? I Think Nothing. Except we Are Left with a Small Amount of Gore and Nudity because It Was Made for Television. Besides Being one Of The Best Sequels, it is one of The Best Thrillers to Watch as a Family. Recommended for Everyone.'}"]},"metadata":{},"execution_count":60}]},{"cell_type":"code","source":["dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, pin_memory=True)\n","dataloader_eval = DataLoader(dataset_eval, batch_size=batch_size, shuffle=True, pin_memory=True)\n","dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, pin_memory=True)\n","\n","# 辞書オブジェクトにまとめる\n","dataloaders_dict = {\"train\": dataloader_train, \"val\": dataloader_eval}"],"metadata":{"id":"BJUfrG01WT-U","executionInfo":{"status":"ok","timestamp":1658497282199,"user_tz":-540,"elapsed":6,"user":{"displayName":"中野修平","userId":"17667180361716214935"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["tmp = next(iter(dataloader_train))\n","print(tmp[\"ids\"][0].size())\n","print(tmp[\"label\"])\n","tmp[\"ids\"][0][0]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HrJDt5u-YTlI","executionInfo":{"status":"ok","timestamp":1658497283427,"user_tz":-540,"elapsed":10,"user":{"displayName":"中野修平","userId":"17667180361716214935"}},"outputId":"4c3cd40f-4474-4721-8822-a1a427f06793"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([16, 512])\n","tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0])\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([  101,   146,  1176,  1142,  1273,   170,  1974,   119,  1135,  1144,\n","          170,  7310,  8117,  1206,  1103,  5681,  1105,  3301,   170,  1642,\n","         1115,  1110,  2785,  8462,   117,  1103,  1642,  1104,  1103,  5250,\n","         3309,  6997,  1488,   119,  1109,  7631,   146,  1176,  1103,  1436,\n","         1649,  1108,  1103,  1236,  1115,  1103, 10919,  1402,  1108,  1167,\n","         1190,  1198,   170,  3582,  1111,  1103,  1642,   119,  1249,  1103,\n","         1401,  1500,  1103,  1488,  1103,  1642,  1104,  1117,  1676,   188,\n","         1266,  1107,  1103,  2350,  6941,  1116,  1104,  5144,  1161,   117,\n","         1103,  5290,  1104,  1447,  1105, 19971,  3316,  1126,  1593,  8854,\n","         9181,   119,  4434,  1108,  1177, 20731,  1115,   170,  3014, 10919,\n","         1125, 14511,  5415,  1105,  2764,   119,  8007,  1103,  1273,  1108,\n","         1304,  3903,   119,  1247,  1127,  4899,   117,  1649,   117,  1165,\n","         1122, 18691,  1181,  1113,  1315,  4105,   119,   119,   119, 20268,\n","         1113,   172, 26179,  1158,  1219,  1103,  2493,  1231,  6617,  6163,\n","         2741,   119,  1252,  2905,   117,   146,  3023, 18029,  1142,  1273,\n","          119,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0])"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","source":["# model = BertModel.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\", output_attentions=True, output_hidden_states=True)\n","model = BertModel.from_pretrained(\"bert-base-cased\", output_attentions=True, output_hidden_states=True)"],"metadata":{"id":"sJX70XFWG_ee","executionInfo":{"status":"ok","timestamp":1658497290509,"user_tz":-540,"elapsed":3165,"user":{"displayName":"中野修平","userId":"17667180361716214935"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["from torch import nn\n","\n","\n","class BertForLivedoor(nn.Module):\n","    '''BERTモデルにPosiNegaの2クラスを判定する部分をつなげたモデル'''\n","\n","    def __init__(self):\n","        super(BertForLivedoor, self).__init__()\n","\n","        # BERTモジュール\n","        self.bert = model  # 日本語学習済みのBERTモデル\n","\n","        # headにクラス予測を追加\n","        # 入力はBERTの出力特徴量の次元768、出力は2クラス\n","        self.cls = nn.Linear(in_features=768, out_features=2)\n","\n","        # 重み初期化処理\n","        nn.init.normal_(self.cls.weight, std=0.02)\n","        nn.init.normal_(self.cls.bias, 0)\n","\n","    def forward(self, input_ids):\n","        '''\n","        input_ids： [batch_size, sequence_length]の文章の単語IDの羅列\n","        '''\n","\n","        # BERTの基本モデル部分の順伝搬\n","        # 順伝搬させる\n","        result = self.bert(input_ids)  # reult は、sequence_output, pooled_output\n","\n","        # sequence_outputの先頭の単語ベクトルを抜き出す\n","        vec_0 = result[0]  # 最初の0がsequence_outputを示す\n","        vec_0 = vec_0[:, 0, :]  # 全バッチ。先頭0番目の単語(cls)の全768要素\n","        vec_0 = vec_0.view(-1, 768)  # sizeを[batch_size, hidden_size]に変換\n","        output = self.cls(vec_0)  # 全結合層\n","\n","        return output"],"metadata":{"id":"kt7Ox3ISQhyQ","executionInfo":{"status":"ok","timestamp":1658497290511,"user_tz":-540,"elapsed":20,"user":{"displayName":"中野修平","userId":"17667180361716214935"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["# モデル構築\n","net = BertForLivedoor()\n","\n","# 訓練モードに設定\n","net.train()\n","\n","print('ネットワーク設定完了')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oYAN0RMZRZ-F","executionInfo":{"status":"ok","timestamp":1658497290512,"user_tz":-540,"elapsed":16,"user":{"displayName":"中野修平","userId":"17667180361716214935"}},"outputId":"99a82c22-ed5c-4607-bc6e-59b71d87b9f2"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["ネットワーク設定完了\n"]}]},{"cell_type":"code","source":["# 勾配計算を最後のBertLayerモジュールと追加した分類アダプターのみ実行\n","\n","# 1. まず全部を、勾配計算Falseにしてしまう\n","for param in net.parameters():\n","    param.requires_grad = False\n","\n","# 2. BertLayerモジュールの最後を勾配計算ありに変更\n","for param in net.bert.encoder.layer[-1].parameters():\n","    param.requires_grad = True\n","\n","# 3. 識別器を勾配計算ありに変更\n","for param in net.cls.parameters():\n","    param.requires_grad = True"],"metadata":{"id":"yESzArCNRaee","executionInfo":{"status":"ok","timestamp":1658497290513,"user_tz":-540,"elapsed":13,"user":{"displayName":"中野修平","userId":"17667180361716214935"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["# 最適化手法の設定\n","import torch.optim as optim\n","\n","\n","# BERTの元の部分はファインチューニング\n","optimizer = optim.Adam([\n","    {'params': net.bert.encoder.layer[-1].parameters(), 'lr': 5e-5},\n","    {'params': net.cls.parameters(), 'lr': 1e-4}\n","])\n","\n","# 損失関数の設定\n","criterion = nn.CrossEntropyLoss()\n","# nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算"],"metadata":{"id":"Ze8n1lpURaoD","executionInfo":{"status":"ok","timestamp":1658497291745,"user_tz":-540,"elapsed":5,"user":{"displayName":"中野修平","userId":"17667180361716214935"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["# モデルを学習させる関数を作成\n","\n","\n","def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n","\n","    # GPUが使えるかを確認\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    print(\"使用デバイス：\", device)\n","    print('-----start-------')\n","\n","    # ネットワークをGPUへ\n","    net.to(device)\n","\n","    # ネットワークがある程度固定であれば、高速化させる\n","    torch.backends.cudnn.benchmark = True\n","\n","    # ミニバッチのサイズ\n","    batch_size = dataloaders_dict[\"train\"].batch_size\n","\n","    # epochのループ\n","    for epoch in range(num_epochs):\n","        # epochごとの訓練と検証のループ\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                net.train()  # モデルを訓練モードに\n","            else:\n","                net.eval()   # モデルを検証モードに\n","\n","            epoch_loss = 0.0  # epochの損失和\n","            epoch_corrects = 0  # epochの正解数\n","            iteration = 1\n","\n","            # データローダーからミニバッチを取り出すループ\n","            for batch in (dataloaders_dict[phase]):\n","                # batchはTextとLableの辞書型変数\n","\n","                # GPUが使えるならGPUにデータを送る\n","                inputs = batch[\"ids\"][0].to(device)  # 文章\n","                labels = batch[\"label\"].to(device)  # ラベル\n","\n","                # optimizerを初期化\n","                optimizer.zero_grad()\n","\n","                # 順伝搬（forward）計算\n","                with torch.set_grad_enabled(phase == 'train'):\n","\n","                    # BERTに入力\n","                    outputs = net(inputs)\n","\n","                    loss = criterion(outputs, labels)  # 損失を計算\n","\n","                    _, preds = torch.max(outputs, 1)  # ラベルを予測\n","\n","                    # 訓練時はバックプロパゲーション\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n","                            acc = (torch.sum(preds == labels.data)\n","                                   ).double()/batch_size\n","                            print('イテレーション {} || Loss: {:.4f} || 10iter. || 本イテレーションの正解率：{}'.format(\n","                                iteration, loss.item(),  acc))\n","\n","                    iteration += 1\n","\n","                    # 損失と正解数の合計を更新\n","                    epoch_loss += loss.item() * batch_size\n","                    epoch_corrects += torch.sum(preds == labels.data)\n","\n","            # epochごとのlossと正解率\n","            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n","            epoch_acc = epoch_corrects.double(\n","            ) / len(dataloaders_dict[phase].dataset)\n","\n","            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n","                                                                           phase, epoch_loss, epoch_acc))\n","\n","    return net"],"metadata":{"id":"dPcjornYRawV","executionInfo":{"status":"ok","timestamp":1658497297269,"user_tz":-540,"elapsed":10,"user":{"displayName":"中野修平","userId":"17667180361716214935"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["# 学習・検証を実行する。\n","num_epochs = 3\n","net_trained = train_model(net, dataloaders_dict,\n","                          criterion, optimizer, num_epochs=num_epochs)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HvIKi67TRa2w","outputId":"6e48ea10-9427-4153-c314-4993c8fc6c21"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["使用デバイス： cuda:0\n","-----start-------\n","イテレーション 10 || Loss: 0.6704 || 10iter. || 本イテレーションの正解率：0.625\n","イテレーション 20 || Loss: 0.7373 || 10iter. || 本イテレーションの正解率：0.5\n","イテレーション 30 || Loss: 0.6584 || 10iter. || 本イテレーションの正解率：0.5625\n","イテレーション 40 || Loss: 0.8347 || 10iter. || 本イテレーションの正解率：0.375\n","イテレーション 50 || Loss: 0.6714 || 10iter. || 本イテレーションの正解率：0.5625\n","イテレーション 60 || Loss: 0.6700 || 10iter. || 本イテレーションの正解率：0.4375\n","イテレーション 70 || Loss: 0.6947 || 10iter. || 本イテレーションの正解率：0.4375\n","イテレーション 80 || Loss: 0.6777 || 10iter. || 本イテレーションの正解率：0.6875\n","イテレーション 90 || Loss: 0.6885 || 10iter. || 本イテレーションの正解率：0.5\n","イテレーション 100 || Loss: 0.6169 || 10iter. || 本イテレーションの正解率：0.625\n","イテレーション 110 || Loss: 0.6189 || 10iter. || 本イテレーションの正解率：0.6875\n","イテレーション 120 || Loss: 0.5622 || 10iter. || 本イテレーションの正解率：0.8125\n","イテレーション 130 || Loss: 0.4013 || 10iter. || 本イテレーションの正解率：0.75\n","イテレーション 140 || Loss: 0.3209 || 10iter. || 本イテレーションの正解率：0.9375\n","イテレーション 150 || Loss: 0.3570 || 10iter. || 本イテレーションの正解率：0.9375\n","イテレーション 160 || Loss: 0.2851 || 10iter. || 本イテレーションの正解率：1.0\n","イテレーション 170 || Loss: 0.1641 || 10iter. || 本イテレーションの正解率：1.0\n","イテレーション 180 || Loss: 0.2386 || 10iter. || 本イテレーションの正解率：0.8125\n","イテレーション 190 || Loss: 0.2866 || 10iter. || 本イテレーションの正解率：0.75\n","イテレーション 200 || Loss: 0.3709 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 210 || Loss: 0.4269 || 10iter. || 本イテレーションの正解率：0.8125\n","イテレーション 220 || Loss: 0.3056 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 230 || Loss: 0.5179 || 10iter. || 本イテレーションの正解率：0.8125\n","イテレーション 240 || Loss: 0.4138 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 250 || Loss: 0.4831 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 260 || Loss: 0.2190 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 270 || Loss: 0.4898 || 10iter. || 本イテレーションの正解率：0.75\n","イテレーション 280 || Loss: 0.2373 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 290 || Loss: 0.3254 || 10iter. || 本イテレーションの正解率：0.9375\n","イテレーション 300 || Loss: 0.3176 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 310 || Loss: 0.4116 || 10iter. || 本イテレーションの正解率：0.75\n","イテレーション 320 || Loss: 0.3277 || 10iter. || 本イテレーションの正解率：0.8125\n","イテレーション 330 || Loss: 0.3116 || 10iter. || 本イテレーションの正解率：0.8125\n","イテレーション 340 || Loss: 0.4210 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 350 || Loss: 0.2539 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 360 || Loss: 0.4864 || 10iter. || 本イテレーションの正解率：0.8125\n","イテレーション 370 || Loss: 0.1083 || 10iter. || 本イテレーションの正解率：0.9375\n","イテレーション 380 || Loss: 0.4488 || 10iter. || 本イテレーションの正解率：0.8125\n","イテレーション 390 || Loss: 0.1743 || 10iter. || 本イテレーションの正解率：0.9375\n","イテレーション 400 || Loss: 0.4037 || 10iter. || 本イテレーションの正解率：0.8125\n","イテレーション 410 || Loss: 0.2922 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 420 || Loss: 0.2485 || 10iter. || 本イテレーションの正解率：0.9375\n","イテレーション 430 || Loss: 0.2513 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 440 || Loss: 0.4231 || 10iter. || 本イテレーションの正解率：0.75\n","イテレーション 450 || Loss: 0.4010 || 10iter. || 本イテレーションの正解率：0.8125\n","イテレーション 460 || Loss: 0.1314 || 10iter. || 本イテレーションの正解率：1.0\n","イテレーション 470 || Loss: 0.0759 || 10iter. || 本イテレーションの正解率：1.0\n","イテレーション 480 || Loss: 0.1381 || 10iter. || 本イテレーションの正解率：0.9375\n","イテレーション 490 || Loss: 0.2335 || 10iter. || 本イテレーションの正解率：0.8125\n","イテレーション 500 || Loss: 0.4716 || 10iter. || 本イテレーションの正解率：0.75\n","イテレーション 510 || Loss: 0.4611 || 10iter. || 本イテレーションの正解率：0.75\n","イテレーション 520 || Loss: 0.1667 || 10iter. || 本イテレーションの正解率：0.9375\n","イテレーション 530 || Loss: 0.1781 || 10iter. || 本イテレーションの正解率：0.9375\n","イテレーション 540 || Loss: 0.1659 || 10iter. || 本イテレーションの正解率：0.9375\n","イテレーション 550 || Loss: 0.3857 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 560 || Loss: 0.3185 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 570 || Loss: 0.5019 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 580 || Loss: 0.5865 || 10iter. || 本イテレーションの正解率：0.8125\n","イテレーション 590 || Loss: 0.5025 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 600 || Loss: 0.1417 || 10iter. || 本イテレーションの正解率：1.0\n","イテレーション 610 || Loss: 0.2470 || 10iter. || 本イテレーションの正解率：0.9375\n","イテレーション 620 || Loss: 0.2142 || 10iter. || 本イテレーションの正解率：0.9375\n","イテレーション 630 || Loss: 0.1924 || 10iter. || 本イテレーションの正解率：1.0\n","イテレーション 640 || Loss: 0.4365 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 650 || Loss: 0.2465 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 660 || Loss: 0.3852 || 10iter. || 本イテレーションの正解率：0.8125\n","イテレーション 670 || Loss: 0.1819 || 10iter. || 本イテレーションの正解率：0.9375\n","イテレーション 680 || Loss: 0.4522 || 10iter. || 本イテレーションの正解率：0.75\n","イテレーション 690 || Loss: 0.3111 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 700 || Loss: 0.3658 || 10iter. || 本イテレーションの正解率：0.9375\n","イテレーション 710 || Loss: 0.4154 || 10iter. || 本イテレーションの正解率：0.8125\n","イテレーション 720 || Loss: 0.6566 || 10iter. || 本イテレーションの正解率：0.8125\n","イテレーション 730 || Loss: 0.2208 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 740 || Loss: 0.5115 || 10iter. || 本イテレーションの正解率：0.8125\n","イテレーション 750 || Loss: 0.1710 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 760 || Loss: 0.2634 || 10iter. || 本イテレーションの正解率：0.9375\n","イテレーション 770 || Loss: 0.2915 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 780 || Loss: 0.1752 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 790 || Loss: 0.1406 || 10iter. || 本イテレーションの正解率：0.9375\n","イテレーション 800 || Loss: 0.3454 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 810 || Loss: 0.2267 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 820 || Loss: 0.1733 || 10iter. || 本イテレーションの正解率：1.0\n","イテレーション 830 || Loss: 0.3214 || 10iter. || 本イテレーションの正解率：0.8125\n","イテレーション 840 || Loss: 0.1127 || 10iter. || 本イテレーションの正解率：0.9375\n","イテレーション 850 || Loss: 0.4309 || 10iter. || 本イテレーションの正解率：0.6875\n","イテレーション 860 || Loss: 0.3187 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 870 || Loss: 0.2666 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 880 || Loss: 0.3087 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 890 || Loss: 0.1427 || 10iter. || 本イテレーションの正解率：0.9375\n","イテレーション 900 || Loss: 0.7718 || 10iter. || 本イテレーションの正解率：0.6875\n","イテレーション 910 || Loss: 0.2810 || 10iter. || 本イテレーションの正解率：0.8125\n","イテレーション 920 || Loss: 0.1331 || 10iter. || 本イテレーションの正解率：0.9375\n","イテレーション 930 || Loss: 0.1095 || 10iter. || 本イテレーションの正解率：0.9375\n","イテレーション 940 || Loss: 0.2364 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 950 || Loss: 0.2578 || 10iter. || 本イテレーションの正解率：0.9375\n","イテレーション 960 || Loss: 0.0516 || 10iter. || 本イテレーションの正解率：1.0\n","イテレーション 970 || Loss: 0.5027 || 10iter. || 本イテレーションの正解率：0.8125\n","イテレーション 980 || Loss: 0.3057 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 990 || Loss: 0.6790 || 10iter. || 本イテレーションの正解率：0.75\n","イテレーション 1000 || Loss: 0.2957 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 1010 || Loss: 0.2251 || 10iter. || 本イテレーションの正解率：0.9375\n","イテレーション 1020 || Loss: 0.0592 || 10iter. || 本イテレーションの正解率：1.0\n","イテレーション 1030 || Loss: 0.4203 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 1040 || Loss: 0.2446 || 10iter. || 本イテレーションの正解率：0.8125\n","イテレーション 1050 || Loss: 0.1542 || 10iter. || 本イテレーションの正解率：0.9375\n","イテレーション 1060 || Loss: 0.7287 || 10iter. || 本イテレーションの正解率：0.625\n","イテレーション 1070 || Loss: 0.4440 || 10iter. || 本イテレーションの正解率：0.8125\n","イテレーション 1080 || Loss: 0.3125 || 10iter. || 本イテレーションの正解率：0.9375\n","イテレーション 1090 || Loss: 0.1803 || 10iter. || 本イテレーションの正解率：1.0\n","イテレーション 1100 || Loss: 0.2449 || 10iter. || 本イテレーションの正解率：0.9375\n","イテレーション 1110 || Loss: 0.2115 || 10iter. || 本イテレーションの正解率：0.9375\n","イテレーション 1120 || Loss: 0.3264 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 1130 || Loss: 0.1974 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 1140 || Loss: 0.2127 || 10iter. || 本イテレーションの正解率：0.9375\n","イテレーション 1150 || Loss: 0.8434 || 10iter. || 本イテレーションの正解率：0.625\n","イテレーション 1160 || Loss: 0.2312 || 10iter. || 本イテレーションの正解率：0.875\n","イテレーション 1170 || Loss: 0.2701 || 10iter. || 本イテレーションの正解率：0.875\n","Epoch 1/3 | train |  Loss: 0.3440 Acc: 0.8408\n","Epoch 1/3 |  val  |  Loss: 0.2755 Acc: 0.8898\n","イテレーション 10 || Loss: 0.2917 || 10iter. || 本イテレーションの正解率：0.8125\n","イテレーション 20 || Loss: 0.1635 || 10iter. || 本イテレーションの正解率：0.9375\n"]}]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","# テストデータでの正解率を求める\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","net_trained.eval()   # モデルを検証モードに\n","net_trained.to(device)  # GPUが使えるならGPUへ送る\n","\n","# epochの正解数を記録する変数\n","epoch_corrects = 0\n","\n","# モデル評価用データ\n","labels_all = []\n","preds_all = []\n","\n","for batch in tqdm(dataloader_test):  # testデータのDataLoader\n","    # batchはTextとLableの辞書オブジェクト\n","    # GPUが使えるならGPUにデータを送る\n","    inputs = batch[\"ids\"][0].to(device)  # 文章\n","    labels = batch[\"label\"].to(device)  # ラベル\n","\n","    # 順伝搬（forward）計算\n","    with torch.set_grad_enabled(False):\n","\n","        # BertForLivedoorに入力\n","        outputs = net_trained(inputs)\n","\n","\n","        loss = criterion(outputs, labels)  # 損失を計算\n","        _, preds = torch.max(outputs, 1)  # ラベルを予測\n","        epoch_corrects += torch.sum(preds == labels.data)  # 正解数の合計を更新\n","\n","        # f1計算用\n","        labels_all.extend(batch[\"label\"].to('cpu').detach().numpy())\n","        preds_all.extend(preds.to('cpu').detach().numpy())\n","\n","# 正解率\n","epoch_acc = epoch_corrects.double() / len(dataloader_test.dataset)\n","\n","print('テストデータ{}個での正解率：{:.4f}'.format(len(dataloader_test.dataset), epoch_acc))"],"metadata":{"id":"XDzzCqhOZzEh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","\n","print(f\"正解率: {accuracy_score(labels_all, preds_all):.3f}\")\n","print(f\"適合率: {precision_score(labels_all, preds_all):.3f}\")\n","print(f\"再現率: {recall_score(labels_all, preds_all):.3f}\")\n","print(f\"F1: {f1_score(labels_all, preds_all):.3f}\")"],"metadata":{"id":"V8TcDWjva1Js"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"YrlVLnIhJIga"},"execution_count":null,"outputs":[]}]}