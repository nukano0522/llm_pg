{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "torch_bert_ner.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMudQ3ChlmcBBQ8ctJZsgvC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nukano0522/pytorch/blob/master/torch_bert_ner_sample_002.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_96F4lbjdWvR",
        "outputId": "8826ee5b-2339-4720-8a51-581ca929d405"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers==4.18.0 in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: fugashi==1.1.0 in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: ipadic==1.0.0 in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (0.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (0.0.53)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.18.0) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.18.0) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0) (2022.6.15)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.18.0 fugashi==1.1.0 ipadic==1.0.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import unicodedata\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertJapaneseTokenizer, BertForTokenClassification\n",
        "\n",
        "# 日本語学習済みモデル\n",
        "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'"
      ],
      "metadata": {
        "id": "n4mSZ7oYdkP8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NER_tokenizer(BertJapaneseTokenizer):\n",
        "       \n",
        "    def encode_plus_tagged(self, text, entities, max_length):\n",
        "        \"\"\"\n",
        "        文章とそれに含まれる固有表現が与えられた時に、\n",
        "        符号化とラベル列の作成を行う。\n",
        "        \"\"\"\n",
        "        # 固有表現の前後でtextを分割し、それぞれのラベルをつけておく。\n",
        "        entities = sorted(entities, key=lambda x: x['span'][0])\n",
        "        splitted = [] # 分割後の文字列を追加していく\n",
        "        position = 0\n",
        "        for entity in entities:\n",
        "            start = entity['span'][0]\n",
        "            end = entity['span'][1]\n",
        "            label = entity['type_id']\n",
        "            # 固有表現ではないものには0のラベルを付与\n",
        "            splitted.append({'text':text[position:start], 'label':0}) \n",
        "            # 固有表現には、固有表現のタイプに対応するIDをラベルとして付与\n",
        "            splitted.append({'text':text[start:end], 'label':label}) \n",
        "            position = end\n",
        "        splitted.append({'text': text[position:], 'label':0})\n",
        "        splitted = [ s for s in splitted if s['text'] ] # 長さ0の文字列は除く\n",
        "\n",
        "        # 分割されたそれぞれの文字列をトークン化し、ラベルをつける。\n",
        "        tokens = [] # トークンを追加していく\n",
        "        labels = [] # トークンのラベルを追加していく\n",
        "        for text_splitted in splitted:\n",
        "            text = text_splitted['text']\n",
        "            label = text_splitted['label']\n",
        "            tokens_splitted = self.tokenize(text)\n",
        "            labels_splitted = [label] * len(tokens_splitted)\n",
        "            tokens.extend(tokens_splitted)\n",
        "            labels.extend(labels_splitted)\n",
        "\n",
        "        # 符号化を行いBERTに入力できる形式にする。\n",
        "        input_ids = self.convert_tokens_to_ids(tokens)\n",
        "        encoding = self.prepare_for_model(\n",
        "            input_ids, \n",
        "            max_length=max_length, \n",
        "            padding='max_length', \n",
        "            truncation=True\n",
        "        ) # input_idsをencodingに変換\n",
        "        # 特殊トークン[CLS]、[SEP]のラベルを0にする。\n",
        "        labels = [0] + labels[:max_length-2] + [0] \n",
        "        # 特殊トークン[PAD]のラベルを0にする。\n",
        "        labels = labels + [0]*( max_length - len(labels) ) \n",
        "        encoding['labels'] = labels\n",
        "\n",
        "        return encoding\n",
        "\n",
        "    def encode_plus_untagged(\n",
        "        self, text, max_length=None, return_tensors=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        文章をトークン化し、それぞれのトークンの文章中の位置も特定しておく。\n",
        "        \"\"\"\n",
        "        # 文章のトークン化を行い、\n",
        "        # それぞれのトークンと文章中の文字列を対応づける。\n",
        "        tokens = [] # トークンを追加していく。\n",
        "        tokens_original = [] # トークンに対応する文章中の文字列を追加していく。\n",
        "        words = self.word_tokenizer.tokenize(text) # MeCabで単語に分割\n",
        "        for word in words:\n",
        "            # 単語をサブワードに分割\n",
        "            tokens_word = self.subword_tokenizer.tokenize(word) \n",
        "            tokens.extend(tokens_word)\n",
        "            if tokens_word[0] == '[UNK]': # 未知語への対応\n",
        "                tokens_original.append(word)\n",
        "            else:\n",
        "                tokens_original.extend([\n",
        "                    token.replace('##','') for token in tokens_word\n",
        "                ])\n",
        "\n",
        "        # 各トークンの文章中での位置を調べる。（空白の位置を考慮する）\n",
        "        position = 0\n",
        "        spans = [] # トークンの位置を追加していく。\n",
        "        for token in tokens_original:\n",
        "            l = len(token)\n",
        "            while 1:\n",
        "                if token != text[position:position+l]:\n",
        "                    position += 1\n",
        "                else:\n",
        "                    spans.append([position, position+l])\n",
        "                    position += l\n",
        "                    break\n",
        "\n",
        "        # 符号化を行いBERTに入力できる形式にする。\n",
        "        input_ids = self.convert_tokens_to_ids(tokens) \n",
        "        encoding = self.prepare_for_model(\n",
        "            input_ids, \n",
        "            max_length=max_length, \n",
        "            padding='max_length' if max_length else False, \n",
        "            truncation=True if max_length else False\n",
        "        )\n",
        "        sequence_length = len(encoding['input_ids'])\n",
        "        # 特殊トークン[CLS]に対するダミーのspanを追加。\n",
        "        spans = [[-1, -1]] + spans[:sequence_length-2] \n",
        "        # 特殊トークン[SEP]、[PAD]に対するダミーのspanを追加。\n",
        "        spans = spans + [[-1, -1]] * ( sequence_length - len(spans) ) \n",
        "\n",
        "        # 必要に応じてtorch.Tensorにする。\n",
        "        if return_tensors == 'pt':\n",
        "            encoding = { k: torch.tensor([v]) for k, v in encoding.items() }\n",
        "\n",
        "        return encoding, spans\n",
        "\n",
        "    def convert_bert_output_to_entities(self, text, labels, spans):\n",
        "        \"\"\"\n",
        "        文章、ラベル列の予測値、各トークンの位置から固有表現を得る。\n",
        "        \"\"\"\n",
        "        # labels, spansから特殊トークンに対応する部分を取り除く\n",
        "        labels = [label for label, span in zip(labels, spans) if span[0] != -1]\n",
        "        spans = [span for span in spans if span[0] != -1]\n",
        "\n",
        "        # 同じラベルが連続するトークンをまとめて、固有表現を抽出する。\n",
        "        entities = []\n",
        "        for label, group \\\n",
        "            in itertools.groupby(enumerate(labels), key=lambda x: x[1]):\n",
        "            \n",
        "            group = list(group)\n",
        "            start = spans[group[0][0]][0]\n",
        "            end = spans[group[-1][0]][1]\n",
        "\n",
        "            if label != 0: # ラベルが0以外ならば、新たな固有表現として追加。\n",
        "                entity = {\n",
        "                    \"name\": text[start:end],\n",
        "                    \"span\": [start, end],\n",
        "                    \"type_id\": label\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities"
      ],
      "metadata": {
        "id": "g1hs2LfUeKks"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zi9FtkEReV64",
        "outputId": "0aeb3276-bab7-40ed-b2a5-4dacfd93291e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
            "The class this function is called from is 'NER_tokenizer'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --branch v2.0 https://github.com/stockmarkteam/ner-wikipedia-dataset "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vo0PAgZ-fLGR",
        "outputId": "307f5cbc-1913-4b62-a901-8c032a92e21d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ner-wikipedia-dataset' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# データのロード\n",
        "dataset = json.load(open('ner-wikipedia-dataset/ner.json','r'))\n",
        "\n",
        "# 固有表現のタイプとIDを対応付る辞書 \n",
        "type_id_dict = {\n",
        "    \"人名\": 1,\n",
        "    \"法人名\": 2,\n",
        "    \"政治的組織名\": 3,\n",
        "    \"その他の組織名\": 4,\n",
        "    \"地名\": 5,\n",
        "    \"施設名\": 6,\n",
        "    \"製品名\": 7,\n",
        "    \"イベント名\": 8\n",
        "}\n",
        "\n",
        "# カテゴリーをラベルに変更、文字列の正規化する。\n",
        "for sample in dataset:\n",
        "    sample['text'] = unicodedata.normalize('NFKC', sample['text'])\n",
        "    for e in sample[\"entities\"]:\n",
        "        e['type_id'] = type_id_dict[e['type']]\n",
        "        del e['type']\n",
        "\n",
        "# データセットの分割\n",
        "# random.shuffle(dataset)\n",
        "n = len(dataset)\n",
        "n_train = int(n*0.6)\n",
        "n_val = int(n*0.2)\n",
        "dataset_train = dataset[:n_train]\n",
        "dataset_val = dataset[n_train:n_train+n_val]\n",
        "dataset_test = dataset[n_train+n_val:]\n",
        "\n",
        "print(f\"Length of train: {len(dataset_train)}\")\n",
        "print(f\"Length of val: {len(dataset_val)}\")\n",
        "print(f\"Length of test: {len(dataset_test)}\")"
      ],
      "metadata": {
        "id": "tKwEiwXmegfL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14749f63-fccd-4bd8-90c6-828da249e6dd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train: 3205\n",
            "Length of val: 1068\n",
            "Length of test: 1070\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CreateDataset(Dataset):\n",
        "  def __init__(self, dataset, tokenizer, max_length):\n",
        "    self.dataset = dataset\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_length = max_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    text = self.dataset[index][\"text\"]\n",
        "    entities = self.dataset[index][\"entities\"]\n",
        "    encoding = tokenizer.encode_plus_tagged(text, entities, max_length=self.max_length)\n",
        "\n",
        "    input_ids = torch.tensor(encoding[\"input_ids\"])\n",
        "    token_type_ids = torch.tensor(encoding[\"token_type_ids\"])\n",
        "    attention_mask = torch.tensor(encoding[\"attention_mask\"])\n",
        "    labels = torch.tensor(encoding[\"labels\"])\n",
        "\n",
        "    return {\n",
        "      \"input_ids\": input_ids,\n",
        "      \"token_type_ids\": token_type_ids,\n",
        "      \"attention_mask\": attention_mask,\n",
        "      \"labels\": labels\n",
        "    }"
      ],
      "metadata": {
        "id": "gfIb0iWBmx0I"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データセットの作成\n",
        "dataset_train_for_loader = CreateDataset(dataset_train, tokenizer, max_length=128)\n",
        "dataset_val_for_loader = CreateDataset(dataset_val, tokenizer, max_length=128)\n",
        "\n",
        "# データローダーの作成\n",
        "dataloader_train = DataLoader(dataset_train_for_loader, batch_size=32, shuffle=True, pin_memory=True)\n",
        "dataloader_val = DataLoader(dataset_val_for_loader, batch_size=256, shuffle=True, pin_memory=True)"
      ],
      "metadata": {
        "id": "1W8XVrShrgCQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 学習済みモデルのロード\n",
        "model = BertForTokenClassification.from_pretrained(MODEL_NAME, num_labels=9)\n",
        "# モデルをGPUへ転送\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soVEaylKgEk4",
        "outputId": "92a78f66-1a6f-41a5-e41c-f783ab9b7dc1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 最適化器\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=2e-5)"
      ],
      "metadata": {
        "id": "umICB77lhy6w"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QL4pkF070TZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, epoch):\n",
        "    \"\"\"訓練\n",
        "    \"\"\"\n",
        "    # 訓練モードで実行\n",
        "    model.train() \n",
        "    train_loss = 0\n",
        "    iteration = 0\n",
        "    for batch in dataloader_train:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss, logits = model(input_ids=input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=attention_mask, \n",
        "                            labels=labels,\n",
        "                            return_dict=False)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        if iteration%10 == 0:\n",
        "          print(f\"epoch{epoch}_iter{iteration}: loss_train: {loss}\")\n",
        "        iteration += 1\n",
        "\n",
        "    return train_loss\n",
        "\n",
        "\n",
        "def validation(model, epoch):\n",
        "    model.eval()# 訓練モードをオフ\n",
        "    val_loss = 0\n",
        "    iteration = 0\n",
        "    with torch.no_grad(): # 勾配を計算しない\n",
        "        for batch in dataloader_val:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            with torch.no_grad():        \n",
        "                loss, logits = model(input_ids, \n",
        "                                    token_type_ids=None, \n",
        "                                    attention_mask=attention_mask,\n",
        "                                    labels=labels,\n",
        "                                    return_dict=False)\n",
        "            val_loss += loss.item()\n",
        "        \n",
        "        print(f\"epoch{epoch}: loss_val_sum: {val_loss}\")\n",
        "    return val_loss"
      ],
      "metadata": {
        "id": "A7nIq1qxlwle"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習の実行\n",
        "max_epoch = 10\n",
        "train_loss_ = []\n",
        "test_loss_ = []\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "    train_ = train(model, epoch)\n",
        "    test_ = validation(model, epoch)\n",
        "    train_loss_.append(train_)\n",
        "    test_loss_.append(test_)\n",
        "\n",
        "# モデル保存\n",
        "torch.save(model.state_dict(), './model.pth')"
      ],
      "metadata": {
        "id": "ouqleipghzOi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "996d85fe-de0b-46fb-89cb-48737c046753"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch0_iter0: loss_train: 1.9821487665176392\n",
            "epoch0_iter10: loss_train: 0.31360679864883423\n",
            "epoch0_iter20: loss_train: 0.2091425210237503\n",
            "epoch0_iter30: loss_train: 0.18732473254203796\n",
            "epoch0_iter40: loss_train: 0.12550422549247742\n",
            "epoch0_iter50: loss_train: 0.08764815330505371\n",
            "epoch0_iter60: loss_train: 0.09843547642230988\n",
            "epoch0_iter70: loss_train: 0.06565511226654053\n",
            "epoch0_iter80: loss_train: 0.04269395023584366\n",
            "epoch0_iter90: loss_train: 0.05960932746529579\n",
            "epoch0_iter100: loss_train: 0.02866964414715767\n",
            "epoch0: loss_val_sum: 0.15024999529123306\n",
            "epoch1_iter0: loss_train: 0.02615644596517086\n",
            "epoch1_iter10: loss_train: 0.023908846080303192\n",
            "epoch1_iter20: loss_train: 0.032664716243743896\n",
            "epoch1_iter30: loss_train: 0.03454083204269409\n",
            "epoch1_iter40: loss_train: 0.02274646796286106\n",
            "epoch1_iter50: loss_train: 0.01513760257512331\n",
            "epoch1_iter60: loss_train: 0.026026498526334763\n",
            "epoch1_iter70: loss_train: 0.02764170616865158\n",
            "epoch1_iter80: loss_train: 0.038992442190647125\n",
            "epoch1_iter90: loss_train: 0.045251671224832535\n",
            "epoch1_iter100: loss_train: 0.07106966525316238\n",
            "epoch1: loss_val_sum: 0.1654446106404066\n",
            "epoch2_iter0: loss_train: 0.02010200545191765\n",
            "epoch2_iter10: loss_train: 0.0024461434222757816\n",
            "epoch2_iter20: loss_train: 0.014958774670958519\n",
            "epoch2_iter30: loss_train: 0.014628247357904911\n",
            "epoch2_iter40: loss_train: 0.019502531737089157\n",
            "epoch2_iter50: loss_train: 0.037330109626054764\n",
            "epoch2_iter60: loss_train: 0.027812890708446503\n",
            "epoch2_iter70: loss_train: 0.012800685130059719\n",
            "epoch2_iter80: loss_train: 0.02613438107073307\n",
            "epoch2_iter90: loss_train: 0.027888035401701927\n",
            "epoch2_iter100: loss_train: 0.02979426458477974\n",
            "epoch2: loss_val_sum: 0.17868146486580372\n",
            "epoch3_iter0: loss_train: 0.020320555195212364\n",
            "epoch3_iter10: loss_train: 0.009283478371798992\n",
            "epoch3_iter20: loss_train: 0.016480298712849617\n",
            "epoch3_iter30: loss_train: 0.011131268925964832\n",
            "epoch3_iter40: loss_train: 0.02901945449411869\n",
            "epoch3_iter50: loss_train: 0.010672381147742271\n",
            "epoch3_iter60: loss_train: 0.019545186311006546\n",
            "epoch3_iter70: loss_train: 0.010101815685629845\n",
            "epoch3_iter80: loss_train: 0.025989897549152374\n",
            "epoch3_iter90: loss_train: 0.019449574872851372\n",
            "epoch3_iter100: loss_train: 0.015440791845321655\n",
            "epoch3: loss_val_sum: 0.12919600494205952\n",
            "epoch4_iter0: loss_train: 0.004247862845659256\n",
            "epoch4_iter10: loss_train: 0.014709672890603542\n",
            "epoch4_iter20: loss_train: 0.00862001534551382\n",
            "epoch4_iter30: loss_train: 0.004211130551993847\n",
            "epoch4_iter40: loss_train: 0.0033907394390553236\n",
            "epoch4_iter50: loss_train: 0.007563779130578041\n",
            "epoch4_iter60: loss_train: 0.00773566635325551\n",
            "epoch4_iter70: loss_train: 0.02048674039542675\n",
            "epoch4_iter80: loss_train: 0.006793120410293341\n",
            "epoch4_iter90: loss_train: 0.004844432696700096\n",
            "epoch4_iter100: loss_train: 0.0009201499633491039\n",
            "epoch4: loss_val_sum: 0.17414224334061146\n",
            "epoch5_iter0: loss_train: 0.009388932958245277\n",
            "epoch5_iter10: loss_train: 0.00600445456802845\n",
            "epoch5_iter20: loss_train: 0.0041106389835476875\n",
            "epoch5_iter30: loss_train: 0.005786855239421129\n",
            "epoch5_iter40: loss_train: 0.006294769234955311\n",
            "epoch5_iter50: loss_train: 0.013437609188258648\n",
            "epoch5_iter60: loss_train: 0.011716200038790703\n",
            "epoch5_iter70: loss_train: 0.005692572798579931\n",
            "epoch5_iter80: loss_train: 0.002540534595027566\n",
            "epoch5_iter90: loss_train: 0.010620778426527977\n",
            "epoch5_iter100: loss_train: 0.0005718520260415971\n",
            "epoch5: loss_val_sum: 0.20579160004854202\n",
            "epoch6_iter0: loss_train: 0.010354501195251942\n",
            "epoch6_iter10: loss_train: 0.0018251386936753988\n",
            "epoch6_iter20: loss_train: 0.008038620464503765\n",
            "epoch6_iter30: loss_train: 0.0067350310273468494\n",
            "epoch6_iter40: loss_train: 0.002505198121070862\n",
            "epoch6_iter50: loss_train: 0.001551130204461515\n",
            "epoch6_iter60: loss_train: 0.003492775373160839\n",
            "epoch6_iter70: loss_train: 0.006823365576565266\n",
            "epoch6_iter80: loss_train: 0.017795680090785027\n",
            "epoch6_iter90: loss_train: 0.005394159350544214\n",
            "epoch6_iter100: loss_train: 0.002348479814827442\n",
            "epoch6: loss_val_sum: 0.20922989770770073\n",
            "epoch7_iter0: loss_train: 0.009123146533966064\n",
            "epoch7_iter10: loss_train: 0.005399302113801241\n",
            "epoch7_iter20: loss_train: 0.00880455318838358\n",
            "epoch7_iter30: loss_train: 0.0025944311637431383\n",
            "epoch7_iter40: loss_train: 0.00825922004878521\n",
            "epoch7_iter50: loss_train: 0.014386497437953949\n",
            "epoch7_iter60: loss_train: 0.005881494842469692\n",
            "epoch7_iter70: loss_train: 0.000865224632434547\n",
            "epoch7_iter80: loss_train: 0.0014800302451476455\n",
            "epoch7_iter90: loss_train: 0.0006298737716861069\n",
            "epoch7_iter100: loss_train: 0.00020237654098309577\n",
            "epoch7: loss_val_sum: 0.1771591603755951\n",
            "epoch8_iter0: loss_train: 0.0009731161990202963\n",
            "epoch8_iter10: loss_train: 0.01842259243130684\n",
            "epoch8_iter20: loss_train: 0.0003137138846796006\n",
            "epoch8_iter30: loss_train: 0.00959336943924427\n",
            "epoch8_iter40: loss_train: 0.008646619506180286\n",
            "epoch8_iter50: loss_train: 0.00901717133820057\n",
            "epoch8_iter60: loss_train: 0.033952753990888596\n",
            "epoch8_iter70: loss_train: 0.02228185534477234\n",
            "epoch8_iter80: loss_train: 0.003238285193219781\n",
            "epoch8_iter90: loss_train: 0.005893348716199398\n",
            "epoch8_iter100: loss_train: 0.016592953354120255\n",
            "epoch8: loss_val_sum: 0.16590697318315506\n",
            "epoch9_iter0: loss_train: 0.002462943783029914\n",
            "epoch9_iter10: loss_train: 0.004727514926344156\n",
            "epoch9_iter20: loss_train: 0.003443763591349125\n",
            "epoch9_iter30: loss_train: 0.01179075799882412\n",
            "epoch9_iter40: loss_train: 0.005830461625009775\n",
            "epoch9_iter50: loss_train: 0.0003468839859124273\n",
            "epoch9_iter60: loss_train: 0.028120556846261024\n",
            "epoch9_iter70: loss_train: 0.0020347065292298794\n",
            "epoch9_iter80: loss_train: 0.0027382569387555122\n",
            "epoch9_iter90: loss_train: 0.009146778844296932\n",
            "epoch9_iter100: loss_train: 0.0003891912929248065\n",
            "epoch9: loss_val_sum: 0.18400953896343708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text, tokenizer, bert_tc):\n",
        "    \"\"\"\n",
        "    BERTで固有表現抽出を行うための関数。\n",
        "    \"\"\"\n",
        "    # 符号化\n",
        "    encoding, spans = tokenizer.encode_plus_untagged(\n",
        "        text, return_tensors='pt'\n",
        "    )\n",
        "    encoding = { k: v.cuda() for k, v in encoding.items() }\n",
        "\n",
        "    # ラベルの予測値の計算\n",
        "    with torch.no_grad():\n",
        "        output = bert_tc(**encoding)\n",
        "        scores = output.logits\n",
        "        labels_predicted = scores[0].argmax(-1).cpu().numpy().tolist() \n",
        "\n",
        "    # ラベル列を固有表現に変換\n",
        "    entities = tokenizer.convert_bert_output_to_entities(\n",
        "        text, labels_predicted, spans\n",
        "    )\n",
        "\n",
        "    return entities\n",
        "\n",
        "# トークナイザのロード\n",
        "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# ファインチューニングしたモデルをロードし、GPUにのせる。\n",
        "# model = BertForTokenClassification().load_from_checkpoint(\n",
        "#     \"./model.pth\"\n",
        "# )\n",
        "# bert_tc = model.bert_tc.cuda()\n",
        "\n",
        "\n",
        "# 固有表現抽出\n",
        "# 注：以下ではコードのわかりやすさのために、1データづつ処理しているが、\n",
        "# バッチ化して処理を行った方が処理時間は短い\n",
        "entities_list = [] # 正解の固有表現を追加していく。\n",
        "entities_predicted_list = [] # 抽出された固有表現を追加していく。\n",
        "for sample in tqdm(dataset_test):\n",
        "    text = sample['text']\n",
        "    entities_predicted = predict(text, tokenizer, model) # BERTで予測\n",
        "    entities_list.append(sample['entities'])\n",
        "    entities_predicted_list.append( entities_predicted )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIRTHph5gJj4",
        "outputId": "db00257b-0718-407c-c351-4323f0b403cf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
            "The class this function is called from is 'NER_tokenizer'.\n",
            "100%|██████████| 1070/1070 [00:21<00:00, 50.90it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 1\n",
        "print(\"# 正解 #\")\n",
        "print(entities_list[i])\n",
        "print(\"# 推論 #\")\n",
        "print(entities_predicted_list[i])\n",
        "print(\"# もとの文章 #\")\n",
        "print(dataset_test[i][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70U_pGw1hNWy",
        "outputId": "f640f0a2-05ef-44d6-ed43-505e3ed99d64"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 正解 #\n",
            "[{'name': 'セカンダリースクール', 'span': [0, 10], 'type_id': 2}, {'name': '南アフリカ高等弁務局', 'span': [24, 34], 'type_id': 3}]\n",
            "# 推論 #\n",
            "[{'name': 'セカンダリースクール', 'span': [0, 10], 'type_id': 2}, {'name': '南アフリカ高等弁務局', 'span': [24, 34], 'type_id': 3}]\n",
            "# もとの文章 #\n",
            "セカンダリースクールを卒業後、ホテルなどで働いた南アフリカ高等弁務局の秘書官として従事した折に政治への興味を持ったとされる。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(entities_list, entities_predicted_list, type_id=None):\n",
        "    \"\"\"\n",
        "    正解と予測を比較し、モデルの固有表現抽出の性能を評価する。\n",
        "    type_idがNoneのときは、全ての固有表現のタイプに対して評価する。\n",
        "    type_idが整数を指定すると、その固有表現のタイプのIDに対して評価を行う。\n",
        "    \"\"\"\n",
        "    num_entities = 0 # 固有表現(正解)の個数\n",
        "    num_predictions = 0 # BERTにより予測された固有表現の個数\n",
        "    num_correct = 0 # BERTにより予測のうち正解であった固有表現の数\n",
        "\n",
        "    # それぞれの文章で予測と正解を比較。\n",
        "    # 予測は文章中の位置とタイプIDが一致すれば正解とみなす。\n",
        "    for entities, entities_predicted in zip(entities_list, entities_predicted_list):\n",
        "\n",
        "        if type_id:\n",
        "            entities = [ e for e in entities if e['type_id'] == type_id ]\n",
        "            entities_predicted = [ \n",
        "                e for e in entities_predicted if e['type_id'] == type_id\n",
        "            ]\n",
        "            \n",
        "        get_span_type = lambda e: (e['span'][0], e['span'][1], e['type_id'])\n",
        "        set_entities = set( get_span_type(e) for e in entities )\n",
        "        set_entities_predicted = set( get_span_type(e) for e in entities_predicted )\n",
        "\n",
        "        num_entities += len(entities)\n",
        "        num_predictions += len(entities_predicted)\n",
        "        num_correct += len( set_entities & set_entities_predicted )\n",
        "\n",
        "    # 指標を計算\n",
        "    precision = num_correct/num_predictions # 適合率\n",
        "    recall = num_correct/num_entities # 再現率\n",
        "    f_value = 2*precision*recall/(precision+recall) # F値\n",
        "\n",
        "    result = {\n",
        "        'num_entities': num_entities,\n",
        "        'num_predictions': num_predictions,\n",
        "        'num_correct': num_correct,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f_value': f_value\n",
        "    }\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "kesRewvRhVVd"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set型の集合演算の検証\n",
        "set_a = {(8, 11, 2)}\n",
        "set_b = {(8, 11, 2, 3)}\n",
        "set_a & set_b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtGsMrdx_0qi",
        "outputId": "3fe8de01-9b8e-400d-e595-b569f2295ce4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "set()"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 評価結果\n",
        "import pandas as pd\n",
        "eval_df = pd.DataFrame()\n",
        "for k, v in type_id_dict.items():\n",
        "  eval_res = evaluate_model(entities_list, entities_predicted_list, type_id=v)\n",
        "  eval_df[k] = eval_res.values()\n",
        "\n",
        "eval_res_all = evaluate_model(entities_list, entities_predicted_list, type_id=None)\n",
        "eval_df[\"ALL\"] = eval_res_all.values()\n",
        "\n",
        "eval_df.index = eval_res_all.keys()\n",
        "eval_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "1dzE6nRXA9Ws",
        "outputId": "a4325117-6b16-4a05-e88a-22d636c0f986"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                         人名         法人名      政治的組織名     その他の組織名          地名  \\\n",
              "num_entities     604.000000  504.000000  249.000000  222.000000  452.000000   \n",
              "num_predictions  640.000000  500.000000  262.000000  205.000000  482.000000   \n",
              "num_correct      583.000000  442.000000  214.000000  168.000000  411.000000   \n",
              "precision          0.910937    0.884000    0.816794    0.819512    0.852697   \n",
              "recall             0.965232    0.876984    0.859438    0.756757    0.909292   \n",
              "f_value            0.937299    0.880478    0.837573    0.786885    0.880086   \n",
              "\n",
              "                        施設名         製品名       イベント名          ALL  \n",
              "num_entities     222.000000  231.000000  203.000000  2687.000000  \n",
              "num_predictions  240.000000  209.000000  220.000000  2758.000000  \n",
              "num_correct      183.000000  168.000000  184.000000  2353.000000  \n",
              "precision          0.762500    0.803828    0.836364     0.853154  \n",
              "recall             0.824324    0.727273    0.906404     0.875698  \n",
              "f_value            0.792208    0.763636    0.869976     0.864279  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-07598f82-1c7e-4007-adf8-33609b67990b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>人名</th>\n",
              "      <th>法人名</th>\n",
              "      <th>政治的組織名</th>\n",
              "      <th>その他の組織名</th>\n",
              "      <th>地名</th>\n",
              "      <th>施設名</th>\n",
              "      <th>製品名</th>\n",
              "      <th>イベント名</th>\n",
              "      <th>ALL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>num_entities</th>\n",
              "      <td>604.000000</td>\n",
              "      <td>504.000000</td>\n",
              "      <td>249.000000</td>\n",
              "      <td>222.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>222.000000</td>\n",
              "      <td>231.000000</td>\n",
              "      <td>203.000000</td>\n",
              "      <td>2687.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>num_predictions</th>\n",
              "      <td>640.000000</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>262.000000</td>\n",
              "      <td>205.000000</td>\n",
              "      <td>482.000000</td>\n",
              "      <td>240.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>220.000000</td>\n",
              "      <td>2758.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>num_correct</th>\n",
              "      <td>583.000000</td>\n",
              "      <td>442.000000</td>\n",
              "      <td>214.000000</td>\n",
              "      <td>168.000000</td>\n",
              "      <td>411.000000</td>\n",
              "      <td>183.000000</td>\n",
              "      <td>168.000000</td>\n",
              "      <td>184.000000</td>\n",
              "      <td>2353.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.910937</td>\n",
              "      <td>0.884000</td>\n",
              "      <td>0.816794</td>\n",
              "      <td>0.819512</td>\n",
              "      <td>0.852697</td>\n",
              "      <td>0.762500</td>\n",
              "      <td>0.803828</td>\n",
              "      <td>0.836364</td>\n",
              "      <td>0.853154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.965232</td>\n",
              "      <td>0.876984</td>\n",
              "      <td>0.859438</td>\n",
              "      <td>0.756757</td>\n",
              "      <td>0.909292</td>\n",
              "      <td>0.824324</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.906404</td>\n",
              "      <td>0.875698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_value</th>\n",
              "      <td>0.937299</td>\n",
              "      <td>0.880478</td>\n",
              "      <td>0.837573</td>\n",
              "      <td>0.786885</td>\n",
              "      <td>0.880086</td>\n",
              "      <td>0.792208</td>\n",
              "      <td>0.763636</td>\n",
              "      <td>0.869976</td>\n",
              "      <td>0.864279</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-07598f82-1c7e-4007-adf8-33609b67990b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-07598f82-1c7e-4007-adf8-33609b67990b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-07598f82-1c7e-4007-adf8-33609b67990b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}