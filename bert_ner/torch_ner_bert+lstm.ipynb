{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "torch_bert_ner.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM2+VNV2BBikNlC35EHDDzm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ac04e298f58f453fa3aa9eaf01721aef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d2b782bc97dd489daa947357d1bb90e0",
              "IPY_MODEL_cc50fb097f8c4c4db522bc950e3dd968",
              "IPY_MODEL_01e3667e1dd3493ba321fb544a253f1e"
            ],
            "layout": "IPY_MODEL_2e0dd8b6a115448f87e76ea9a4eb5eb0"
          }
        },
        "d2b782bc97dd489daa947357d1bb90e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edebfca09daf4382b15b62df58248474",
            "placeholder": "​",
            "style": "IPY_MODEL_f20ce684fe68465f8e24faa9a8779bc1",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "cc50fb097f8c4c4db522bc950e3dd968": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6507b0ab651e46d9b7ef0bff28b33084",
            "max": 445021143,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6eb2713a2756466ea327f4ecb37038a4",
            "value": 445021143
          }
        },
        "01e3667e1dd3493ba321fb544a253f1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ccfe013d529c4aa7bbf69430f62f68a3",
            "placeholder": "​",
            "style": "IPY_MODEL_8da80447883e4bb7a3e487354119e445",
            "value": " 424M/424M [00:06&lt;00:00, 67.2MB/s]"
          }
        },
        "2e0dd8b6a115448f87e76ea9a4eb5eb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edebfca09daf4382b15b62df58248474": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f20ce684fe68465f8e24faa9a8779bc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6507b0ab651e46d9b7ef0bff28b33084": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6eb2713a2756466ea327f4ecb37038a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ccfe013d529c4aa7bbf69430f62f68a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8da80447883e4bb7a3e487354119e445": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nukano0522/pytorch/blob/master/bert_ner/torch_ner_bert%2Blstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_96F4lbjdWvR",
        "outputId": "cf108166-2013-4396-aad4-a25425177d58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.21.2\n",
            "  Downloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 7.8 MB/s \n",
            "\u001b[?25hCollecting fugashi==1.1.2\n",
            "  Downloading fugashi-1.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (568 kB)\n",
            "\u001b[K     |████████████████████████████████| 568 kB 73.5 MB/s \n",
            "\u001b[?25hCollecting ipadic==1.0.0\n",
            "  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.4 MB 36.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 76.2 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (4.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.21.2) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.21.2) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.21.2) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.2) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.2) (1.24.3)\n",
            "Building wheels for collected packages: ipadic\n",
            "  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556723 sha256=7a8e19f38e7474e8d00548ba79c6123233726dc6531f0468499972dccd65d3d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/8b/99/cf0d27191876637cd3639a560f93aa982d7855ce826c94348b\n",
            "Successfully built ipadic\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers, ipadic, fugashi\n",
            "Successfully installed fugashi-1.1.2 huggingface-hub-0.9.1 ipadic-1.0.0 tokenizers-0.12.1 transformers-4.21.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.21.2 fugashi==1.1.2 ipadic==1.0.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import unicodedata\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Optional, Tuple, Union\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertJapaneseTokenizer, BertForTokenClassification, BertModel\n",
        "from transformers.modeling_outputs import TokenClassifierOutput\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "from transformers import BertConfig, BertModel\n",
        "\n",
        "config = BertConfig()\n",
        "\n",
        "# 日本語学習済みモデル\n",
        "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'"
      ],
      "metadata": {
        "id": "n4mSZ7oYdkP8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def torch_fix_seed(seed=42):\n",
        "    # Python random\n",
        "    random.seed(seed)\n",
        "    # Numpy\n",
        "    np.random.seed(seed)\n",
        "    # Pytorch\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.use_deterministic_algorithms = True\n",
        "\n",
        "\n",
        "torch_fix_seed()"
      ],
      "metadata": {
        "id": "vgLXKtm8qc_a"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config.num_labels = 9\n",
        "print(config.num_labels)\n",
        "print(config.use_return_dict)\n",
        "config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSH8P4lMQj-B",
        "outputId": "827dfa85-826c-4370-9c0a-09e5b018877b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n",
            "True\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertConfig {\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"id2label\": {\n",
              "    \"0\": \"LABEL_0\",\n",
              "    \"1\": \"LABEL_1\",\n",
              "    \"2\": \"LABEL_2\",\n",
              "    \"3\": \"LABEL_3\",\n",
              "    \"4\": \"LABEL_4\",\n",
              "    \"5\": \"LABEL_5\",\n",
              "    \"6\": \"LABEL_6\",\n",
              "    \"7\": \"LABEL_7\",\n",
              "    \"8\": \"LABEL_8\"\n",
              "  },\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"label2id\": {\n",
              "    \"LABEL_0\": 0,\n",
              "    \"LABEL_1\": 1,\n",
              "    \"LABEL_2\": 2,\n",
              "    \"LABEL_3\": 3,\n",
              "    \"LABEL_4\": 4,\n",
              "    \"LABEL_5\": 5,\n",
              "    \"LABEL_6\": 6,\n",
              "    \"LABEL_7\": 7,\n",
              "    \"LABEL_8\": 8\n",
              "  },\n",
              "  \"layer_norm_eps\": 1e-12,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"bert\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"transformers_version\": \"4.21.2\",\n",
              "  \"type_vocab_size\": 2,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 30522\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# データのダウンロード\n",
        "!git clone --branch v2.0 https://github.com/stockmarkteam/ner-wikipedia-dataset "
      ],
      "metadata": {
        "id": "GSlvnbNOX9bP",
        "outputId": "f221baf1-b1ed-4dc4-821d-5b578215ae7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ner-wikipedia-dataset'...\n",
            "remote: Enumerating objects: 32, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 32 (delta 9), reused 11 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (32/32), done.\n",
            "Note: checking out 'f7ed83626d90e5a79f1af99775e4b8c6cba15295'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# データのロード\n",
        "dataset = json.load(open('ner-wikipedia-dataset/ner.json','r'))\n",
        "\n",
        "# 固有表現のタイプとIDを対応付る辞書 \n",
        "type_id_dict = {\n",
        "    \"人名\": 1,\n",
        "    \"法人名\": 2,\n",
        "    \"政治的組織名\": 3,\n",
        "    \"その他の組織名\": 4,\n",
        "    \"地名\": 5,\n",
        "    \"施設名\": 6,\n",
        "    \"製品名\": 7,\n",
        "    \"イベント名\": 8\n",
        "}\n",
        "\n",
        "# カテゴリーをラベルに変更、文字列の正規化する。\n",
        "for sample in dataset:\n",
        "    sample['text'] = unicodedata.normalize('NFKC', sample['text'])\n",
        "    for e in sample[\"entities\"]:\n",
        "        e['type_id'] = type_id_dict[e['type']]\n",
        "        del e['type']\n",
        "\n",
        "# データセットの分割\n",
        "n = len(dataset)\n",
        "n_train = int(n*0.6)\n",
        "n_val = int(n*0.2)\n",
        "dataset_train = dataset[:n_train]\n",
        "dataset_val = dataset[n_train:n_train+n_val]\n",
        "dataset_test = dataset[n_train+n_val:]\n",
        "\n",
        "print(f\"Length of train: {len(dataset_train)}\")\n",
        "print(f\"Length of val: {len(dataset_val)}\")\n",
        "print(f\"Length of test: {len(dataset_test)}\")"
      ],
      "metadata": {
        "id": "CmPwfQD0YNQe",
        "outputId": "677ac8f1-583a-4a8b-a0b9-c78747d90136",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train: 3205\n",
            "Length of val: 1068\n",
            "Length of test: 1070\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NER_tokenizer(BertJapaneseTokenizer):\n",
        "\n",
        "    def create_tokens_and_labels(self, splitted):\n",
        "        \"\"\"分割された文字列をトークン化し、ラベルを付与\n",
        "        Args：\n",
        "          splitted: 分割された文字列\n",
        "            例：\n",
        "            [{'text': 'レッドフォックス株式会社', 'label': 2},\n",
        "             {'text': 'は、', 'label': 0},\n",
        "             {'text': '東京都千代田区', 'label': 5},\n",
        "             {'text': 'に本社を置くITサービス企業である。', 'label': 0}]\n",
        "        Return:\n",
        "          tokens, labels\n",
        "            例：\n",
        "            ['レッド', 'フォックス', '株式会社', 'は', '、', '東京', '都', '千代田', '区', 'に', '本社', 'を', '置く', 'IT', 'サービス', '企業', 'で', 'ある', '。']\n",
        "            [2, 2, 2, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "        \"\"\"\n",
        "        tokens = [] # トークン格納用\n",
        "        labels = [] # トークンに対応するラベル格納用\n",
        "        for s in splitted:\n",
        "            text = s['text']\n",
        "            label = s['label']\n",
        "            tokens_splitted = self.tokenize(text) # BertJapaneseTokenizerのトークナイザを使ってトークンに分割\n",
        "            labels_splitted = [label] * len(tokens_splitted)\n",
        "            tokens.extend(tokens_splitted)\n",
        "            labels.extend(labels_splitted)\n",
        "        \n",
        "        return tokens, labels\n",
        "\n",
        "\n",
        "    def encoding_for_bert(self, tokens, labels):\n",
        "        \"\"\"符号化を行いBERTに入力できる形式にする\n",
        "        Args:\n",
        "          tokens: トークン列\n",
        "          labels: トークンに対応するラベルの列\n",
        "        Returns: \n",
        "          encoding: BERTに入力できる形式\n",
        "          例：\n",
        "          {'input_ids': [2, 3990, 13779, 1275, 9, 6, 391, 409, 9674, 280, 7, 2557, 11, 3045, 8267, 1645, 1189, 12, 31, 8, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
        "          　'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
        "          　'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
        "            'labels': [0, 2, 2, 2, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
        "\n",
        "        \"\"\"\n",
        "        encoding = self.encode_plus(\n",
        "            tokens, \n",
        "            max_length=max_length, \n",
        "            padding='max_length', \n",
        "            truncation=True\n",
        "        ) \n",
        "        # トークン[CLS]、[SEP]のラベルを0\n",
        "        labels = [0] + labels[:max_length-2] + [0] \n",
        "        # トークン[PAD]のラベルを0\n",
        "        labels = labels + [0]*( max_length - len(labels) ) \n",
        "        encoding['labels'] = labels\n",
        "\n",
        "        return encoding\n",
        "\n",
        "\n",
        "    def encode_plus_tagged(self, text, entities, max_length):\n",
        "        \"\"\"文章とそれに含まれる固有表現が与えられた時に、符号化とラベル列の作成\n",
        "        Args:\n",
        "          text: 元の文章\n",
        "          entities: 文章中の固有表現の位置(span)とラベル(type_id)の情報\n",
        "\n",
        "        \"\"\"\n",
        "        # 固有表現の前後でtextを分割し、それぞれのラベルをつけておく。\n",
        "        entities = sorted(entities, key=lambda x: x['span'][0]) # 固有表現の位置の昇順でソート\n",
        "        splitted = [] # 分割後の文字列格納用\n",
        "        position = 0\n",
        "        for entity in entities:\n",
        "            start = entity['span'][0]\n",
        "            end = entity['span'][1]\n",
        "            label = entity['type_id']\n",
        "            # 固有表現ではないものには0のラベルを付与\n",
        "            splitted.append({'text': text[position:start], 'label':0}) \n",
        "            # 固有表現には、固有表現のタイプに対応するIDをラベルとして付与\n",
        "            splitted.append({'text': text[start:end], 'label':label}) \n",
        "            position = end\n",
        "\n",
        "        # 最後の固有表現から文末に、0のラベルを付与\n",
        "        splitted.append({'text': text[position:], 'label':0})\n",
        "        # positionとspan[0]の値が同じだと空白文字にラベル0が付与されるため、長さ0の文字列は除く（例：{'text': '', 'label': 0}）\n",
        "        splitted = [ s for s in splitted if s['text'] ] \n",
        "\n",
        "        # 分割された文字列をトークン化し、ラベルを付与\n",
        "        tokens, labels = self.create_tokens_and_labels(splitted)\n",
        "\n",
        "        # 符号化を行いBERTに入力できる形式にする\n",
        "        encoding = self.encoding_for_bert(tokens, labels)\n",
        "\n",
        "        return encoding\n",
        "\n",
        "\n",
        "    def encode_plus_untagged(self, text, max_length=None, return_tensors=None):\n",
        "        \"\"\"文章をトークン化し、それぞれのトークンの文章中の位置も特定しておく。\n",
        "        \"\"\"\n",
        "        # 文章のトークン化を行い、\n",
        "        # それぞれのトークンと文章中の文字列を対応づける。\n",
        "        tokens = [] # トークン格納用\n",
        "        tokens_original = [] # トークンに対応する文章中の文字列格納用\n",
        "        words = self.word_tokenizer.tokenize(text) # MeCabで単語に分割\n",
        "        for word in words:\n",
        "            # 単語をサブワードに分割\n",
        "            tokens_word = self.subword_tokenizer.tokenize(word) \n",
        "            tokens.extend(tokens_word)\n",
        "            if tokens_word[0] == '[UNK]': # 未知語への対応\n",
        "                tokens_original.append(word)\n",
        "            else:\n",
        "                tokens_original.extend([\n",
        "                    token.replace('##','') for token in tokens_word\n",
        "                ])\n",
        "\n",
        "        # 各トークンの文章中での位置を調べる。（空白の位置を考慮する）\n",
        "        position = 0\n",
        "        spans = [] # トークンの位置を追加していく。\n",
        "        for token in tokens_original:\n",
        "            l = len(token)\n",
        "            while 1:\n",
        "                if token != text[position:position+l]:\n",
        "                    position += 1\n",
        "                else:\n",
        "                    spans.append([position, position+l])\n",
        "                    position += l\n",
        "                    break\n",
        "\n",
        "        # 符号化を行いBERTに入力できる形式にする。\n",
        "        input_ids = self.convert_tokens_to_ids(tokens) \n",
        "        encoding = self.encode_plus(\n",
        "            tokens, \n",
        "            max_length=max_length, \n",
        "            padding='max_length' if max_length else False, \n",
        "            truncation=True if max_length else False,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        sequence_length = len(encoding['input_ids'])\n",
        "        # 特殊トークン[CLS]に対するダミーのspanを追加。\n",
        "        spans = [[-1, -1]] + spans[:sequence_length-2] \n",
        "        # 特殊トークン[SEP]、[PAD]に対するダミーのspanを追加。\n",
        "        spans = spans + [[-1, -1]] * ( sequence_length - len(spans) ) \n",
        "\n",
        "        return encoding, spans\n",
        "\n",
        "\n",
        "    def convert_bert_output_to_entities(self, text, labels, spans):\n",
        "        \"\"\"\n",
        "        文章、ラベル列の予測値、各トークンの位置から固有表現を得る。\n",
        "        \"\"\"\n",
        "        # labels, spansから特殊トークンに対応する部分を取り除く\n",
        "        labels = [label for label, span in zip(labels, spans) if span[0] != -1]\n",
        "        spans = [span for span in spans if span[0] != -1]\n",
        "\n",
        "        # 同じラベルが連続するトークンをまとめて、固有表現を抽出する。\n",
        "        entities = []\n",
        "        for label, group \\\n",
        "            in itertools.groupby(enumerate(labels), key=lambda x: x[1]):\n",
        "            \n",
        "            group = list(group)\n",
        "            start = spans[group[0][0]][0]\n",
        "            end = spans[group[-1][0]][1]\n",
        "\n",
        "            if label != 0: # ラベルが0以外ならば、新たな固有表現として追加。\n",
        "                entity = {\n",
        "                    \"name\": text[start:end],\n",
        "                    \"span\": [start, end],\n",
        "                    \"type_id\": label\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities"
      ],
      "metadata": {
        "id": "g1hs2LfUeKks"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "data = dataset_test[2]\n",
        "pprint.pprint(data)\n",
        "text = data[\"text\"]\n",
        "\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "id": "ijQ3cd4s6ilI",
        "outputId": "046deabf-a99a-45ec-94a1-e45deda5b6c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'curid': '78589',\n",
            " 'entities': [{'name': '九州朝日放送', 'span': [16, 22], 'type_id': 2},\n",
            "              {'name': 'RKB毎日放送', 'span': [23, 30], 'type_id': 2},\n",
            "              {'name': '福岡放送', 'span': [31, 35], 'type_id': 2},\n",
            "              {'name': 'TVQ九州放送', 'span': [36, 43], 'type_id': 2},\n",
            "              {'name': '福岡県', 'span': [54, 57], 'type_id': 5},\n",
            "              {'name': '福岡', 'span': [82, 84], 'type_id': 5},\n",
            "              {'name': '佐賀', 'span': [85, 87], 'type_id': 5},\n",
            "              {'name': '北部九州', 'span': [110, 114], 'type_id': 5},\n",
            "              {'name': '九州北部地方', 'span': [119, 125], 'type_id': 5}],\n",
            " 'text': 'また、在福の民放テレビ局のうち、九州朝日放送・RKB毎日放送・福岡放送・TVQ九州放送は、法律上の放送区域は福岡県域であるものの、実際の視聴可能エリアと取材エリアは福岡・佐賀の両県にまたがるため、地域ニュースを扱う時に「北部九州」或いは「九州北部地方」という言葉を使うことが多い。'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 32\n",
        "tokens = [] # トークン格納用\n",
        "tokens_original = [] # トークンに対応する文章中の文字列格納用\n",
        "words = tokenizer.word_tokenizer.tokenize(text) # MeCabで単語に分割\n",
        "# words = tokenizer.tokenize(text) # Bertのトークナイザーで分割\n",
        "\n",
        "print(words)\n",
        "\n",
        "for word in words:\n",
        "    # 単語をサブワードに分割\n",
        "    tokens_word = tokenizer.subword_tokenizer.tokenize(word) \n",
        "    tokens.extend(tokens_word)\n",
        "    if tokens_word[0] == '[UNK]': # 未知語への対応\n",
        "        tokens_original.append(word)\n",
        "    else:\n",
        "        tokens_original.extend([\n",
        "            token.replace('##','') for token in tokens_word\n",
        "        ])\n",
        "\n",
        "print(tokens)\n",
        "print(tokens_original)\n",
        "\n",
        "# 各トークンの文章中での位置を調べる。（空白の位置を考慮する）\n",
        "position = 0\n",
        "spans = [] # トークンの位置を追加していく。\n",
        "for token in tokens_original:\n",
        "    l = len(token)\n",
        "    while 1:\n",
        "        if token != text[position:position+l]:\n",
        "            print(\"aaa\")\n",
        "            position += 1\n",
        "        else:\n",
        "            spans.append([position, position+l])\n",
        "            position += l\n",
        "            break\n",
        "\n",
        "print(spans)\n",
        "\n",
        "\n",
        "# 符号化を行いBERTに入力できる形式にする。\n",
        "input_ids = tokenizer.convert_tokens_to_ids(tokens) \n",
        "encoding = tokenizer.encode_plus(\n",
        "    tokens, \n",
        "    max_length=max_length, \n",
        "    padding='max_length' if max_length else False, \n",
        "    truncation=True if max_length else False\n",
        ")\n",
        "\n",
        "print(encoding)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "irbuGfi06icP",
        "outputId": "70cf8fc8-6a4a-42a8-9e69-c3d536b32cec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['また', '、', '在', '福', 'の', '民放', 'テレビ局', 'の', 'うち', '、', '九州朝日放送', '・', 'RKB', '毎日放送', '・', '福岡放送', '・', 'TVQ', '九州', '放送', 'は', '、', '法律', '上', 'の', '放送', '区域', 'は', '福岡', '県', '域', 'で', 'ある', 'ものの', '、', '実際', 'の', '視聴', '可能', 'エリア', 'と', '取材', 'エリア', 'は', '福岡', '・', '佐賀', 'の', '両', '県', 'に', 'またがる', 'ため', '、', '地域', 'ニュース', 'を', '扱う', '時', 'に', '「', '北部', '九州', '」', '或いは', '「', '九州', '北部', '地方', '」', 'という', '言葉', 'を', '使う', 'こと', 'が', '多い', '。']\n",
            "['また', '、', '在', '福', 'の', '民放', 'テレビ局', 'の', 'うち', '、', '九州', '##朝日放送', '・', 'RKB', '毎日放送', '・', '福岡', '##放送', '・', 'TV', '##Q', '九州', '放送', 'は', '、', '法律', '上', 'の', '放送', '区域', 'は', '福岡', '県', '域', 'で', 'ある', 'ものの', '、', '実際', 'の', '視聴', '可能', 'エリア', 'と', '取材', 'エリア', 'は', '福岡', '・', '佐賀', 'の', '両', '県', 'に', 'またがる', 'ため', '、', '地域', 'ニュース', 'を', '扱う', '時', 'に', '「', '北部', '九州', '」', '或いは', '「', '九州', '北部', '地方', '」', 'という', '言葉', 'を', '使う', 'こと', 'が', '多い', '。']\n",
            "['また', '、', '在', '福', 'の', '民放', 'テレビ局', 'の', 'うち', '、', '九州', '朝日放送', '・', 'RKB', '毎日放送', '・', '福岡', '放送', '・', 'TV', 'Q', '九州', '放送', 'は', '、', '法律', '上', 'の', '放送', '区域', 'は', '福岡', '県', '域', 'で', 'ある', 'ものの', '、', '実際', 'の', '視聴', '可能', 'エリア', 'と', '取材', 'エリア', 'は', '福岡', '・', '佐賀', 'の', '両', '県', 'に', 'またがる', 'ため', '、', '地域', 'ニュース', 'を', '扱う', '時', 'に', '「', '北部', '九州', '」', '或いは', '「', '九州', '北部', '地方', '」', 'という', '言葉', 'を', '使う', 'こと', 'が', '多い', '。']\n",
            "[[0, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 8], [8, 12], [12, 13], [13, 15], [15, 16], [16, 18], [18, 22], [22, 23], [23, 26], [26, 30], [30, 31], [31, 33], [33, 35], [35, 36], [36, 38], [38, 39], [39, 41], [41, 43], [43, 44], [44, 45], [45, 47], [47, 48], [48, 49], [49, 51], [51, 53], [53, 54], [54, 56], [56, 57], [57, 58], [58, 59], [59, 61], [61, 64], [64, 65], [65, 67], [67, 68], [68, 70], [70, 72], [72, 75], [75, 76], [76, 78], [78, 81], [81, 82], [82, 84], [84, 85], [85, 87], [87, 88], [88, 89], [89, 90], [90, 91], [91, 95], [95, 97], [97, 98], [98, 100], [100, 104], [104, 105], [105, 107], [107, 108], [108, 109], [109, 110], [110, 112], [112, 114], [114, 115], [115, 118], [118, 119], [119, 121], [121, 123], [123, 125], [125, 126], [126, 129], [129, 131], [131, 132], [132, 134], [134, 136], [136, 137], [137, 139], [139, 140]]\n",
            "{'input_ids': [2, 106, 6, 946, 674, 5, 12470, 9921, 5, 859, 6, 2446, 22903, 35, 24831, 11614, 35, 2176, 2200, 35, 3700, 29650, 2446, 333, 9, 6, 2409, 109, 5, 333, 3849, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zi9FtkEReV64",
        "outputId": "9b5990d0-9d77-45f5-984c-1688fc0256fc"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
            "The class this function is called from is 'NER_tokenizer'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "tmp = dataset_train[1]\n",
        "pprint.pprint(tmp)\n",
        "pprint.pprint(tokenizer.encode_plus_tagged(text=tmp[\"text\"], entities=tmp[\"entities\"], max_length=32), width=200)"
      ],
      "metadata": {
        "id": "tKwEiwXmegfL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed04527f-650a-42f2-ae3a-4106c2163fef"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'curid': '2415078',\n",
            " 'entities': [{'name': 'レッドフォックス株式会社', 'span': [0, 12], 'type_id': 2},\n",
            "              {'name': '東京都千代田区', 'span': [14, 21], 'type_id': 5}],\n",
            " 'text': 'レッドフォックス株式会社は、東京都千代田区に本社を置くITサービス企業である。'}\n",
            "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            " 'input_ids': [2, 3990, 13779, 1275, 9, 6, 391, 409, 9674, 280, 7, 2557, 11, 3045, 8267, 1645, 1189, 12, 31, 8, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            " 'labels': [0, 2, 2, 2, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CreateDataset(Dataset):\n",
        "  \"\"\"データセット作成\n",
        "  \"\"\"\n",
        "  def __init__(self, dataset, tokenizer, max_length):\n",
        "    self.dataset = dataset\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_length = max_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    text = self.dataset[index][\"text\"]\n",
        "    entities = self.dataset[index][\"entities\"]\n",
        "    encoding = tokenizer.encode_plus_tagged(text, entities, max_length=self.max_length)\n",
        "\n",
        "    input_ids = torch.tensor(encoding[\"input_ids\"])\n",
        "    token_type_ids = torch.tensor(encoding[\"token_type_ids\"])\n",
        "    attention_mask = torch.tensor(encoding[\"attention_mask\"])\n",
        "    labels = torch.tensor(encoding[\"labels\"])\n",
        "\n",
        "    return {\n",
        "      \"input_ids\": input_ids,\n",
        "      \"token_type_ids\": token_type_ids,\n",
        "      \"attention_mask\": attention_mask,\n",
        "      \"labels\": labels\n",
        "    }"
      ],
      "metadata": {
        "id": "gfIb0iWBmx0I"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データセットの作成\n",
        "dataset_train_for_loader = CreateDataset(dataset_train, tokenizer, max_length=128)\n",
        "dataset_val_for_loader = CreateDataset(dataset_val, tokenizer, max_length=128)\n",
        "\n",
        "# データローダーの作成\n",
        "dataloader_train = DataLoader(dataset_train_for_loader, batch_size=32, shuffle=True, pin_memory=True)\n",
        "dataloader_val = DataLoader(dataset_val_for_loader, batch_size=256, shuffle=True, pin_memory=True)\n",
        "\n",
        "dataloaders_dict = {\"train\": dataloader_train, \"val\": dataloader_val}"
      ],
      "metadata": {
        "id": "1W8XVrShrgCQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "model = BertModel.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\", add_pooling_layer=False, output_attentions=True, output_hidden_states=True)\n",
        "\n",
        "class MyBertForTokenClassification(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MyBertForTokenClassification, self).__init__()\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.bert = model\n",
        "\n",
        "        self.lstm = torch.nn.LSTM(input_size=config.hidden_size, hidden_size =config.hidden_size, batch_first=True, bidirectional=True)\n",
        "\n",
        "        classifier_dropout = (\n",
        "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
        "        )\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        \n",
        "        self.classifier= nn.Linear(config.hidden_size*2, config.num_labels)\n",
        "\n",
        "        # 重み初期化処理\n",
        "        nn.init.normal_(self.classifier.weight, std=0.02)\n",
        "        nn.init.normal_(self.classifier.bias, 0)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        token_type_ids,\n",
        "        attention_mask,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "        ):\n",
        "      \n",
        "        return_dict = return_dict if return_dict is not None else config.use_return_dict\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            # position_ids=position_ids,\n",
        "            # head_mask=head_mask,\n",
        "            # inputs_embeds=inputs_embeds,\n",
        "            # output_attentions=output_attentions,\n",
        "            # output_hidden_states=output_hidden_states,\n",
        "            # labels=labels,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        # print(outputs.keys())\n",
        "\n",
        "        # lstmout, _ = self.lstm(outputs[0], None)\n",
        "        lstmout, _ = self.lstm(outputs[\"last_hidden_state\"], None)\n",
        "\n",
        "        logits = self.classifier(self.dropout(lstmout))\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return TokenClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123,
          "referenced_widgets": [
            "ac04e298f58f453fa3aa9eaf01721aef",
            "d2b782bc97dd489daa947357d1bb90e0",
            "cc50fb097f8c4c4db522bc950e3dd968",
            "01e3667e1dd3493ba321fb544a253f1e",
            "2e0dd8b6a115448f87e76ea9a4eb5eb0",
            "edebfca09daf4382b15b62df58248474",
            "f20ce684fe68465f8e24faa9a8779bc1",
            "6507b0ab651e46d9b7ef0bff28b33084",
            "6eb2713a2756466ea327f4ecb37038a4",
            "ccfe013d529c4aa7bbf69430f62f68a3",
            "8da80447883e4bb7a3e487354119e445"
          ]
        },
        "id": "7qCdMMYEQvNz",
        "outputId": "04a2054a-e4ff-4ae5-b4ad-fc44dc5b424f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/424M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac04e298f58f453fa3aa9eaf01721aef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU使えるならGPU使う\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 学習済みモデルのロード\n",
        "# model_ = BertForTokenClassification.from_pretrained(MODEL_NAME, num_labels=9)\n",
        "model = MyBertForTokenClassification()\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soVEaylKgEk4",
        "outputId": "07e58d56-6056-4db7-99c8-d69c3f4a1747"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyBertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lstm): LSTM(768, 768, batch_first=True, bidirectional=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=1536, out_features=9, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 最適化器\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=2e-5)\n",
        "\n",
        "# モデルを学習させる関数を作成\n",
        "def train_model(net, dataloaders_dict, optimizer, num_epochs):\n",
        "\n",
        "    # GPUが使えるかを確認\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"使用デバイス：\", device)\n",
        "    print('-----start-------')\n",
        "\n",
        "    # ネットワークをGPUへ\n",
        "    net.to(device)\n",
        "\n",
        "    # ネットワークがある程度固定であれば、高速化させる\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # ミニバッチのサイズ\n",
        "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
        "\n",
        "    # epochのループ\n",
        "    for epoch in range(num_epochs):\n",
        "        # epochごとの訓練と検証のループ\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train()  # モデルを訓練モードに\n",
        "            else:\n",
        "                net.eval()   # モデルを検証モードに\n",
        "\n",
        "            epoch_loss = 0.0  # epochの損失和\n",
        "            iteration = 1\n",
        "\n",
        "            # データローダーからミニバッチを取り出すループ\n",
        "            for batch in (dataloaders_dict[phase]):\n",
        "                # batchはTextとLableの辞書型変数\n",
        "\n",
        "                # GPUが使えるならGPUにデータを送る\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "\n",
        "                # optimizerを初期化\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # 順伝搬（forward）計算\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "                    # BERTに入力\n",
        "                    output = net(input_ids=input_ids, \n",
        "                                          token_type_ids=None, \n",
        "                                          attention_mask=attention_mask, \n",
        "                                          labels=labels,\n",
        "                                          return_dict=True)\n",
        "                    \n",
        "                    loss = output[0]\n",
        "\n",
        "                    # 訓練時はバックプロパゲーション\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                        optimizer.step()\n",
        "\n",
        "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
        "                            print(f\"イテレーション {iteration} || Loss: {loss:.4f}\")\n",
        "\n",
        "                    iteration += 1\n",
        "\n",
        "                    # 損失の合計を更新\n",
        "                    epoch_loss += loss.item() * batch_size\n",
        "\n",
        "            # epochごとのloss\n",
        "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs} | phase {phase} |  Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    return net"
      ],
      "metadata": {
        "id": "QL4pkF070TZ8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習・検証を実行\n",
        "num_epochs = 5\n",
        "net_trained = train_model(model, dataloaders_dict, optimizer, num_epochs=num_epochs)"
      ],
      "metadata": {
        "id": "g8JlfJDj2fDw",
        "outputId": "77b9e51d-84c2-433b-f5c3-8ea1b7c9a1d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用デバイス： cuda:0\n",
            "-----start-------\n",
            "イテレーション 10 || Loss: 1.1337\n",
            "イテレーション 20 || Loss: 0.8798\n",
            "イテレーション 30 || Loss: 0.8016\n",
            "イテレーション 40 || Loss: 0.5322\n",
            "イテレーション 50 || Loss: 0.3890\n",
            "イテレーション 60 || Loss: 0.3726\n",
            "イテレーション 70 || Loss: 0.2967\n",
            "イテレーション 80 || Loss: 0.3514\n",
            "イテレーション 90 || Loss: 0.3020\n",
            "イテレーション 100 || Loss: 0.3128\n",
            "Epoch 1/5 | phase train |  Loss: 0.6079\n",
            "Epoch 1/5 | phase val |  Loss: 0.0280\n",
            "イテレーション 10 || Loss: 0.1672\n",
            "イテレーション 20 || Loss: 0.1204\n",
            "イテレーション 30 || Loss: 0.1100\n",
            "イテレーション 40 || Loss: 0.1729\n",
            "イテレーション 50 || Loss: 0.1437\n",
            "イテレーション 60 || Loss: 0.0998\n",
            "イテレーション 70 || Loss: 0.1918\n",
            "イテレーション 80 || Loss: 0.1235\n",
            "イテレーション 90 || Loss: 0.0870\n",
            "イテレーション 100 || Loss: 0.0910\n",
            "Epoch 2/5 | phase train |  Loss: 0.1327\n",
            "Epoch 2/5 | phase val |  Loss: 0.0153\n",
            "イテレーション 10 || Loss: 0.0896\n",
            "イテレーション 20 || Loss: 0.0400\n",
            "イテレーション 30 || Loss: 0.0675\n",
            "イテレーション 40 || Loss: 0.0837\n",
            "イテレーション 50 || Loss: 0.0604\n",
            "イテレーション 60 || Loss: 0.0566\n",
            "イテレーション 70 || Loss: 0.0358\n",
            "イテレーション 80 || Loss: 0.0286\n",
            "イテレーション 90 || Loss: 0.0684\n",
            "イテレーション 100 || Loss: 0.0280\n",
            "Epoch 3/5 | phase train |  Loss: 0.0568\n",
            "Epoch 3/5 | phase val |  Loss: 0.0125\n",
            "イテレーション 10 || Loss: 0.0241\n",
            "イテレーション 20 || Loss: 0.0158\n",
            "イテレーション 30 || Loss: 0.0440\n",
            "イテレーション 40 || Loss: 0.0382\n",
            "イテレーション 50 || Loss: 0.0349\n",
            "イテレーション 60 || Loss: 0.0262\n",
            "イテレーション 70 || Loss: 0.0141\n",
            "イテレーション 80 || Loss: 0.0277\n",
            "イテレーション 90 || Loss: 0.0204\n",
            "イテレーション 100 || Loss: 0.0527\n",
            "Epoch 4/5 | phase train |  Loss: 0.0327\n",
            "Epoch 4/5 | phase val |  Loss: 0.0117\n",
            "イテレーション 10 || Loss: 0.0230\n",
            "イテレーション 20 || Loss: 0.0114\n",
            "イテレーション 30 || Loss: 0.0159\n",
            "イテレーション 40 || Loss: 0.0189\n",
            "イテレーション 50 || Loss: 0.0091\n",
            "イテレーション 60 || Loss: 0.0294\n",
            "イテレーション 70 || Loss: 0.0129\n",
            "イテレーション 80 || Loss: 0.0052\n",
            "イテレーション 90 || Loss: 0.0163\n",
            "イテレーション 100 || Loss: 0.0219\n",
            "Epoch 5/5 | phase train |  Loss: 0.0206\n",
            "Epoch 5/5 | phase val |  Loss: 0.0122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデル保存\n",
        "# torch.save(net_trained.state_dict(), './model.pth')"
      ],
      "metadata": {
        "id": "0zhLxePHsGA3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # モデルロード\n",
        "# model = BertModel.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\", add_pooling_layer=False, output_attentions=True, output_hidden_states=True)\n",
        "# net_trained_ = MyBertForTokenClassification()\n",
        "# net_trained_.load_state_dict(torch.load('./model.pth'), strict=False)\n",
        "# net_trained_.to(device)"
      ],
      "metadata": {
        "id": "u2SPtvUcqgZ6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text, tokenizer, model):\n",
        "    \"\"\"BERTで固有表現抽出を行うための関数。\n",
        "    \"\"\"\n",
        "    # 符号化\n",
        "    encoding, spans = tokenizer.encode_plus_untagged(\n",
        "        text, return_tensors='pt'\n",
        "    )\n",
        "    encoding = { k: v.cuda() for k, v in encoding.items() }\n",
        "    # print(encoding)\n",
        "\n",
        "    # ラベルの予測値の計算\n",
        "    with torch.no_grad():\n",
        "        output = model(**encoding)\n",
        "        scores = output.logits\n",
        "        labels_predicted = scores[0].argmax(-1).cpu().numpy().tolist() \n",
        "\n",
        "    # ラベル列を固有表現に変換\n",
        "    entities = tokenizer.convert_bert_output_to_entities(\n",
        "        text, labels_predicted, spans\n",
        "    )\n",
        "\n",
        "    return entities\n",
        "\n",
        "# 固有表現抽出\n",
        "entities_list = [] # 正解の固有表現\n",
        "entities_predicted_list = [] # 予測された固有表現\n",
        "for sample in tqdm(dataset_test):\n",
        "    text = sample['text']\n",
        "    entities_predicted = predict(text, tokenizer, net_trained) # BERTで予測\n",
        "    entities_list.append(sample['entities'])\n",
        "    entities_predicted_list.append( entities_predicted )"
      ],
      "metadata": {
        "id": "iIRTHph5gJj4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "640d6112-b6a4-4d16-e29a-c36709fae9e0"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1070/1070 [00:10<00:00, 104.55it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 2\n",
        "print(\"# 正解 #\")\n",
        "print(entities_list[i])\n",
        "print(\"# 推論 #\")\n",
        "print(entities_predicted_list[i])\n",
        "print(\"# もとの文章 #\")\n",
        "print(dataset_test[i][\"text\"])"
      ],
      "metadata": {
        "id": "70U_pGw1hNWy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82da1bde-3adc-4d78-f30e-d3c414a92d91"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 正解 #\n",
            "[{'name': '九州朝日放送', 'span': [16, 22], 'type_id': 2}, {'name': 'RKB毎日放送', 'span': [23, 30], 'type_id': 2}, {'name': '福岡放送', 'span': [31, 35], 'type_id': 2}, {'name': 'TVQ九州放送', 'span': [36, 43], 'type_id': 2}, {'name': '福岡県', 'span': [54, 57], 'type_id': 5}, {'name': '福岡', 'span': [82, 84], 'type_id': 5}, {'name': '佐賀', 'span': [85, 87], 'type_id': 5}, {'name': '北部九州', 'span': [110, 114], 'type_id': 5}, {'name': '九州北部地方', 'span': [119, 125], 'type_id': 5}]\n",
            "# 推論 #\n",
            "[{'name': '福', 'span': [4, 5], 'type_id': 5}, {'name': '九州朝日放送', 'span': [16, 22], 'type_id': 2}, {'name': 'RKB毎日放送', 'span': [23, 30], 'type_id': 2}, {'name': '福岡放送', 'span': [31, 35], 'type_id': 2}, {'name': 'TVQ九州放送', 'span': [36, 43], 'type_id': 2}, {'name': '福岡県', 'span': [54, 57], 'type_id': 5}, {'name': '福岡', 'span': [82, 84], 'type_id': 5}, {'name': '佐賀', 'span': [85, 87], 'type_id': 5}, {'name': '県', 'span': [89, 90], 'type_id': 5}, {'name': '北部九州', 'span': [110, 114], 'type_id': 5}, {'name': '九州北部地方', 'span': [119, 125], 'type_id': 5}]\n",
            "# もとの文章 #\n",
            "また、在福の民放テレビ局のうち、九州朝日放送・RKB毎日放送・福岡放送・TVQ九州放送は、法律上の放送区域は福岡県域であるものの、実際の視聴可能エリアと取材エリアは福岡・佐賀の両県にまたがるため、地域ニュースを扱う時に「北部九州」或いは「九州北部地方」という言葉を使うことが多い。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(entities_list, entities_predicted_list, type_id=None):\n",
        "    \"\"\"\n",
        "    正解と予測を比較し、モデルの固有表現抽出の性能を評価する。\n",
        "    type_idがNoneのときは、全ての固有表現のタイプに対して評価する。\n",
        "    type_idが整数を指定すると、その固有表現のタイプのIDに対して評価を行う。\n",
        "    \"\"\"\n",
        "    num_entities = 0 # 固有表現(正解)の個数\n",
        "    num_predictions = 0 # BERTにより予測された固有表現の個数\n",
        "    num_correct = 0 # BERTにより予測のうち正解であった固有表現の数\n",
        "\n",
        "    # それぞれの文章で予測と正解を比較。\n",
        "    # 予測は文章中の位置とタイプIDが一致すれば正解とみなす。\n",
        "    for entities, entities_predicted in zip(entities_list, entities_predicted_list):\n",
        "\n",
        "        if type_id:\n",
        "            entities = [ e for e in entities if e['type_id'] == type_id ]\n",
        "            entities_predicted = [ \n",
        "                e for e in entities_predicted if e['type_id'] == type_id\n",
        "            ]\n",
        "            \n",
        "        get_span_type = lambda e: (e['span'][0], e['span'][1], e['type_id'])\n",
        "        set_entities = set( get_span_type(e) for e in entities )\n",
        "        set_entities_predicted = set( get_span_type(e) for e in entities_predicted )\n",
        "\n",
        "        num_entities += len(entities)\n",
        "        num_predictions += len(entities_predicted)\n",
        "        num_correct += len( set_entities & set_entities_predicted )\n",
        "\n",
        "    # 指標を計算\n",
        "    precision = num_correct/num_predictions # 適合率\n",
        "    recall = num_correct/num_entities # 再現率\n",
        "    f_value = 2*precision*recall/(precision+recall) # F値\n",
        "\n",
        "    result = {\n",
        "        'num_entities': num_entities,\n",
        "        'num_predictions': num_predictions,\n",
        "        'num_correct': num_correct,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f_value': f_value\n",
        "    }\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "kesRewvRhVVd"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 評価結果\n",
        "import pandas as pd\n",
        "eval_df = pd.DataFrame()\n",
        "for k, v in type_id_dict.items():\n",
        "  eval_res = evaluate_model(entities_list, entities_predicted_list, type_id=v)\n",
        "  eval_df[k] = eval_res.values()\n",
        "\n",
        "eval_res_all = evaluate_model(entities_list, entities_predicted_list, type_id=None)\n",
        "eval_df[\"ALL\"] = eval_res_all.values()\n",
        "\n",
        "eval_df.index = eval_res_all.keys()\n",
        "eval_df"
      ],
      "metadata": {
        "id": "1dzE6nRXA9Ws",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "a58957dd-c464-4eaf-e94c-78d38d58be6f"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                         人名         法人名      政治的組織名     その他の組織名          地名  \\\n",
              "num_entities     604.000000  504.000000  249.000000  222.000000  452.000000   \n",
              "num_predictions  615.000000  484.000000  259.000000  235.000000  490.000000   \n",
              "num_correct      570.000000  438.000000  210.000000  185.000000  411.000000   \n",
              "precision          0.926829    0.904959    0.810811    0.787234    0.838776   \n",
              "recall             0.943709    0.869048    0.843373    0.833333    0.909292   \n",
              "f_value            0.935193    0.886640    0.826772    0.809628    0.872611   \n",
              "\n",
              "                        施設名         製品名       イベント名          ALL  \n",
              "num_entities     222.000000  231.000000  203.000000  2687.000000  \n",
              "num_predictions  235.000000  243.000000  199.000000  2760.000000  \n",
              "num_correct      182.000000  182.000000  172.000000  2350.000000  \n",
              "precision          0.774468    0.748971    0.864322     0.851449  \n",
              "recall             0.819820    0.787879    0.847291     0.874581  \n",
              "f_value            0.796499    0.767932    0.855721     0.862860  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ca7f06fb-4ee2-4e4a-a666-d9c6a7d536d7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>人名</th>\n",
              "      <th>法人名</th>\n",
              "      <th>政治的組織名</th>\n",
              "      <th>その他の組織名</th>\n",
              "      <th>地名</th>\n",
              "      <th>施設名</th>\n",
              "      <th>製品名</th>\n",
              "      <th>イベント名</th>\n",
              "      <th>ALL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>num_entities</th>\n",
              "      <td>604.000000</td>\n",
              "      <td>504.000000</td>\n",
              "      <td>249.000000</td>\n",
              "      <td>222.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>222.000000</td>\n",
              "      <td>231.000000</td>\n",
              "      <td>203.000000</td>\n",
              "      <td>2687.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>num_predictions</th>\n",
              "      <td>615.000000</td>\n",
              "      <td>484.000000</td>\n",
              "      <td>259.000000</td>\n",
              "      <td>235.000000</td>\n",
              "      <td>490.000000</td>\n",
              "      <td>235.000000</td>\n",
              "      <td>243.000000</td>\n",
              "      <td>199.000000</td>\n",
              "      <td>2760.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>num_correct</th>\n",
              "      <td>570.000000</td>\n",
              "      <td>438.000000</td>\n",
              "      <td>210.000000</td>\n",
              "      <td>185.000000</td>\n",
              "      <td>411.000000</td>\n",
              "      <td>182.000000</td>\n",
              "      <td>182.000000</td>\n",
              "      <td>172.000000</td>\n",
              "      <td>2350.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.926829</td>\n",
              "      <td>0.904959</td>\n",
              "      <td>0.810811</td>\n",
              "      <td>0.787234</td>\n",
              "      <td>0.838776</td>\n",
              "      <td>0.774468</td>\n",
              "      <td>0.748971</td>\n",
              "      <td>0.864322</td>\n",
              "      <td>0.851449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.943709</td>\n",
              "      <td>0.869048</td>\n",
              "      <td>0.843373</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.909292</td>\n",
              "      <td>0.819820</td>\n",
              "      <td>0.787879</td>\n",
              "      <td>0.847291</td>\n",
              "      <td>0.874581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_value</th>\n",
              "      <td>0.935193</td>\n",
              "      <td>0.886640</td>\n",
              "      <td>0.826772</td>\n",
              "      <td>0.809628</td>\n",
              "      <td>0.872611</td>\n",
              "      <td>0.796499</td>\n",
              "      <td>0.767932</td>\n",
              "      <td>0.855721</td>\n",
              "      <td>0.862860</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ca7f06fb-4ee2-4e4a-a666-d9c6a7d536d7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ca7f06fb-4ee2-4e4a-a666-d9c6a7d536d7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ca7f06fb-4ee2-4e4a-a666-d9c6a7d536d7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JoOM0ky4CyHh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}