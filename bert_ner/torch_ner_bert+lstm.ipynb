{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "torch_bert_ner.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOeonG3Q+IZfy/o4irAhp48",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nukano0522/pytorch/blob/master/bert_ner/torch_ner_bert%2Blstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_96F4lbjdWvR",
        "outputId": "6d85d2d4-85b1-4f9e-ae0d-960f94c9f03b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers==4.21.2 in /usr/local/lib/python3.7/dist-packages (4.21.2)\n",
            "Requirement already satisfied: fugashi==1.1.2 in /usr/local/lib/python3.7/dist-packages (1.1.2)\n",
            "Requirement already satisfied: ipadic==1.0.0 in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (0.9.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (0.12.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (2022.6.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (4.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (3.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.21.2) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.21.2) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.21.2) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.2) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.2) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.21.2 fugashi==1.1.2 ipadic==1.0.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import unicodedata\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Optional, Tuple, Union\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertJapaneseTokenizer, BertForTokenClassification, BertModel\n",
        "from transformers.modeling_outputs import TokenClassifierOutput\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "from transformers import BertConfig, BertModel\n",
        "\n",
        "config = BertConfig()\n",
        "\n",
        "# 日本語学習済みモデル\n",
        "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'"
      ],
      "metadata": {
        "id": "n4mSZ7oYdkP8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def torch_fix_seed(seed=42):\n",
        "    # Python random\n",
        "    random.seed(seed)\n",
        "    # Numpy\n",
        "    np.random.seed(seed)\n",
        "    # Pytorch\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.use_deterministic_algorithms = True\n",
        "\n",
        "\n",
        "torch_fix_seed()"
      ],
      "metadata": {
        "id": "vgLXKtm8qc_a"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config.num_labels = 9\n",
        "print(config.num_labels)\n",
        "print(config.use_return_dict)\n",
        "config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSH8P4lMQj-B",
        "outputId": "c5a44058-5861-4d43-a113-7440576105a1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n",
            "True\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertConfig {\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"id2label\": {\n",
              "    \"0\": \"LABEL_0\",\n",
              "    \"1\": \"LABEL_1\",\n",
              "    \"2\": \"LABEL_2\",\n",
              "    \"3\": \"LABEL_3\",\n",
              "    \"4\": \"LABEL_4\",\n",
              "    \"5\": \"LABEL_5\",\n",
              "    \"6\": \"LABEL_6\",\n",
              "    \"7\": \"LABEL_7\",\n",
              "    \"8\": \"LABEL_8\"\n",
              "  },\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"label2id\": {\n",
              "    \"LABEL_0\": 0,\n",
              "    \"LABEL_1\": 1,\n",
              "    \"LABEL_2\": 2,\n",
              "    \"LABEL_3\": 3,\n",
              "    \"LABEL_4\": 4,\n",
              "    \"LABEL_5\": 5,\n",
              "    \"LABEL_6\": 6,\n",
              "    \"LABEL_7\": 7,\n",
              "    \"LABEL_8\": 8\n",
              "  },\n",
              "  \"layer_norm_eps\": 1e-12,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"bert\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"transformers_version\": \"4.21.2\",\n",
              "  \"type_vocab_size\": 2,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 30522\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# データのダウンロード\n",
        "!git clone --branch v2.0 https://github.com/stockmarkteam/ner-wikipedia-dataset "
      ],
      "metadata": {
        "id": "GSlvnbNOX9bP",
        "outputId": "a66b3cf9-04a3-4837-f193-79ce9ab7244d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ner-wikipedia-dataset' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# データのロード\n",
        "dataset = json.load(open('ner-wikipedia-dataset/ner.json','r'))\n",
        "\n",
        "# 固有表現のタイプとIDを対応付る辞書 \n",
        "type_id_dict = {\n",
        "    \"人名\": 1,\n",
        "    \"法人名\": 2,\n",
        "    \"政治的組織名\": 3,\n",
        "    \"その他の組織名\": 4,\n",
        "    \"地名\": 5,\n",
        "    \"施設名\": 6,\n",
        "    \"製品名\": 7,\n",
        "    \"イベント名\": 8\n",
        "}\n",
        "\n",
        "# カテゴリーをラベルに変更、文字列の正規化する。\n",
        "for sample in dataset:\n",
        "    sample['text'] = unicodedata.normalize('NFKC', sample['text'])\n",
        "    for e in sample[\"entities\"]:\n",
        "        e['type_id'] = type_id_dict[e['type']]\n",
        "        del e['type']\n",
        "\n",
        "# データセットの分割\n",
        "n = len(dataset)\n",
        "n_train = int(n*0.6)\n",
        "n_val = int(n*0.2)\n",
        "dataset_train = dataset[:n_train]\n",
        "dataset_val = dataset[n_train:n_train+n_val]\n",
        "dataset_test = dataset[n_train+n_val:]\n",
        "\n",
        "print(f\"Length of train: {len(dataset_train)}\")\n",
        "print(f\"Length of val: {len(dataset_val)}\")\n",
        "print(f\"Length of test: {len(dataset_test)}\")"
      ],
      "metadata": {
        "id": "CmPwfQD0YNQe",
        "outputId": "a5fac498-519f-450e-c735-ee30e709586f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train: 3205\n",
            "Length of val: 1068\n",
            "Length of test: 1070\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NerTokenizerForTrain(BertJapaneseTokenizer):\n",
        "\n",
        "  def create_tokens_and_labels(self, splitted):\n",
        "      \"\"\"分割された文字列をトークン化し、ラベルを付与\n",
        "      Args：\n",
        "        splitted: 分割された文字列\n",
        "          例：\n",
        "          [{'text': 'レッドフォックス株式会社', 'label': 2},\n",
        "          {'text': 'は、', 'label': 0},\n",
        "          {'text': '東京都千代田区', 'label': 5},\n",
        "          {'text': 'に本社を置くITサービス企業である。', 'label': 0}]\n",
        "      Return:\n",
        "        tokens, labels\n",
        "          例：\n",
        "          ['レッド', 'フォックス', '株式会社', 'は', '、', '東京', '都', '千代田', '区', 'に', '本社', 'を', '置く', 'IT', 'サービス', '企業', 'で', 'ある', '。']\n",
        "          [2, 2, 2, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "      \"\"\"\n",
        "      tokens = [] # トークン格納用\n",
        "      labels = [] # トークンに対応するラベル格納用\n",
        "      for s in splitted:\n",
        "          text = s['text']\n",
        "          label = s['label']\n",
        "          tokens_splitted = self.tokenize(text) # BertJapaneseTokenizerのトークナイザを使ってトークンに分割\n",
        "          labels_splitted = [label] * len(tokens_splitted)\n",
        "          tokens.extend(tokens_splitted)\n",
        "          labels.extend(labels_splitted)\n",
        "      \n",
        "      return tokens, labels\n",
        "\n",
        "\n",
        "  def encoding_for_bert(self, tokens, labels, max_length):\n",
        "      \"\"\"符号化を行いBERTに入力できる形式にする\n",
        "      Args:\n",
        "        tokens: トークン列\n",
        "        labels: トークンに対応するラベルの列\n",
        "      Returns: \n",
        "        encoding: BERTに入力できる形式\n",
        "        例：\n",
        "        {'input_ids': [2, 3990, 13779, 1275, 9, 6, 391, 409, 9674, 280, 7, 2557, 11, 3045, 8267, 1645, 1189, 12, 31, 8, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
        "        　'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
        "        　'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
        "          'labels': [0, 2, 2, 2, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
        "\n",
        "      \"\"\"\n",
        "      encoding = self.encode_plus(\n",
        "          tokens, \n",
        "          max_length=max_length, \n",
        "          padding='max_length', \n",
        "          truncation=True\n",
        "      ) \n",
        "      # トークン[CLS]、[SEP]のラベルを0\n",
        "      labels = [0] + labels[:max_length-2] + [0] \n",
        "      # トークン[PAD]のラベルを0\n",
        "      labels = labels + [0]*( max_length - len(labels) ) \n",
        "      encoding['labels'] = labels\n",
        "\n",
        "      return encoding\n",
        "\n",
        "\n",
        "  def encode_plus_tagged(self, text, entities, max_length):\n",
        "      \"\"\"文章とそれに含まれる固有表現が与えられた時に、符号化とラベル列の作成\n",
        "      Args:\n",
        "        text: 元の文章\n",
        "        entities: 文章中の固有表現の位置(span)とラベル(type_id)の情報\n",
        "\n",
        "      \"\"\"\n",
        "      # 固有表現の前後でtextを分割し、それぞれのラベルをつけておく。\n",
        "      entities = sorted(entities, key=lambda x: x['span'][0]) # 固有表現の位置の昇順でソート\n",
        "      splitted = [] # 分割後の文字列格納用\n",
        "      position = 0\n",
        "      for entity in entities:\n",
        "          start = entity['span'][0]\n",
        "          end = entity['span'][1]\n",
        "          label = entity['type_id']\n",
        "          # 固有表現ではないものには0のラベルを付与\n",
        "          splitted.append({'text': text[position:start], 'label':0}) \n",
        "          # 固有表現には、固有表現のタイプに対応するIDをラベルとして付与\n",
        "          splitted.append({'text': text[start:end], 'label':label}) \n",
        "          position = end\n",
        "\n",
        "      # 最後の固有表現から文末に、0のラベルを付与\n",
        "      splitted.append({'text': text[position:], 'label':0})\n",
        "      # positionとspan[0]の値が同じだと空白文字にラベル0が付与されるため、長さ0の文字列は除く（例：{'text': '', 'label': 0}）\n",
        "      splitted = [ s for s in splitted if s['text'] ] \n",
        "\n",
        "      # 分割された文字列をトークン化し、ラベルを付与\n",
        "      tokens, labels = self.create_tokens_and_labels(splitted)\n",
        "\n",
        "      # 符号化を行いBERTに入力できる形式にする\n",
        "      encoding = self.encoding_for_bert(tokens, labels, max_length)\n",
        "\n",
        "      return encoding"
      ],
      "metadata": {
        "id": "kLWfwehWE4I0"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NerTokenizerForTest(BertJapaneseTokenizer):\n",
        "\n",
        "    def encoding_for_bert(self, tokens, max_length):\n",
        "        \"\"\"符号化を行いBERTに入力できる形式にする\n",
        "        Args:\n",
        "          tokens: トークン列\n",
        "        Returns: \n",
        "          encoding: BERTに入力できる形式\n",
        "          例：\n",
        "          {'input_ids': [2, 106, 6, 946, 674, 5, 12470, 9921, 5, 859, 6, 2446, 22903, 35, 24831, 11614, 35, 2176, 2200, 35, 3700, 29650, 2446, 333, 9, 6, 2409, 109, 5, 333, 3849, 3], \n",
        "          'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
        "          'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
        "        \"\"\"\n",
        "        encoding = self.encode_plus(\n",
        "            tokens, \n",
        "            max_length=max_length, \n",
        "            padding='max_length', \n",
        "            truncation=True,\n",
        "            return_tensors = \"pt\"\n",
        "        ) \n",
        "\n",
        "        return encoding\n",
        "\n",
        "\n",
        "    def create_spans_of_token(self, tokens_original, encoding):\n",
        "        \"\"\" 各トークン（サブワード）の文章中での位置を調べる\n",
        "          Args:\n",
        "            tokens_original: トークン列をさらにサブワードに分割した列\n",
        "            encoding: \n",
        "            例：tokens_original\n",
        "              ['元々', 'は', '前作', '「', 'The', 'Apple', 's', '」', 'の', 'アウト', ...]\n",
        "          \n",
        "          Return:\n",
        "            spans: 各トークンの文章中の位置([CLS][PAD]などの特殊トークンはダミーで置き換える)\n",
        "            例：\n",
        "              [[-1, -1], [0, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 10], [10, 11], ...]\n",
        "        \"\"\"        \n",
        "        position = 0\n",
        "        spans = [] # トークンの位置を追加していく。\n",
        "        for token in tokens_original:\n",
        "            l = len(token)\n",
        "            while 1:\n",
        "                if token != text[position:position+l]:\n",
        "                    \"\"\"例：英語文章のように空白が混ざっていると下記のようにずれるケースがあることを考慮\n",
        "                          token: \"Digital\"\n",
        "                          text[position:position+l]: \" Digita\"\n",
        "                    \"\"\"\n",
        "                    position += 1\n",
        "                else:\n",
        "                    spans.append([position, position+l])\n",
        "                    position += l\n",
        "                    break\n",
        "\n",
        "        sequence_length = len(encoding['input_ids'])\n",
        "        # 特殊トークン[CLS]に対するダミーのspanを追加。\n",
        "        spans = [[-1, -1]] + spans[:sequence_length-2] \n",
        "        # 特殊トークン[SEP]、[PAD]に対するダミーのspanを追加。\n",
        "        spans = spans + [[-1, -1]] * ( sequence_length - len(spans) ) \n",
        "\n",
        "        return spans\n",
        "\n",
        "\n",
        "    def encode_plus_untagged(self, text, max_length=None, return_tensors=None):\n",
        "        \"\"\"文章をトークン化し、それぞれのトークンの文章中の位置も特定しておく。\n",
        "        \"\"\"\n",
        "        # 文章のトークン化を行い、\n",
        "        # それぞれのトークンと文章中の文字列を対応づける。\n",
        "        tokens = [] # トークン格納用\n",
        "        tokens_original = [] # トークンに対応する文章中の文字列格納用\n",
        "        words = self.word_tokenizer.tokenize(text) # MeCabで単語に分割\n",
        "        for word in words:\n",
        "            # 単語をサブワードに分割\n",
        "            tokens_word = self.subword_tokenizer.tokenize(word) \n",
        "            tokens.extend(tokens_word)\n",
        "            if tokens_word[0] == '[UNK]': # 未知語への対応\n",
        "                tokens_original.append(word)\n",
        "            else:\n",
        "                tokens_original.extend([\n",
        "                    token.replace('##','') for token in tokens_word\n",
        "                ])\n",
        "\n",
        "\n",
        "        # 符号化を行いBERTに入力できる形式にする\n",
        "        encoding = self.encoding_for_bert(tokens, max_length)\n",
        "\n",
        "        # 各トークン（サブワード）の文章中での位置を調べる\n",
        "        spans = self.create_spans_of_token(tokens_original, encoding)\n",
        "\n",
        "        return encoding, spans\n",
        "\n",
        "\n",
        "    def convert_bert_output_to_entities(self, text, labels, spans):\n",
        "        \"\"\"文章、ラベル列の予測値、各トークンの位置から固有表現を得る。\n",
        "        \"\"\"\n",
        "        # labels, spansから特殊トークンに対応する部分を取り除く\n",
        "        labels = [label for label, span in zip(labels, spans) if span[0] != -1]\n",
        "        spans = [span for span in spans if span[0] != -1]\n",
        "\n",
        "        \n",
        "        # for label, group in itertools.groupby(enumerate(labels), key=lambda x: x[1]):\n",
        "        #     \"\"\"\n",
        "        #     labels: [0, 0, 0, 3, 3, 5, 7, 7, 7, 0, 0, 0]\n",
        "        #     list(group): [(0, 0), (1, 0), (2, 0)]\n",
        "        #     \"\"\"\n",
        "           \n",
        "        #     group = list(group)\n",
        "        #     start = spans[group[0][0]][0]\n",
        "        #     end = spans[group[-1][0]][1]\n",
        "\n",
        "        # 同じラベルが連続するトークンをまとめて、固有表現を抽出する。\n",
        "        entities = []\n",
        "        position = 0\n",
        "        for label, group in itertools.groupby(labels):\n",
        "            \"\"\"\n",
        "            例：labelsは予測結果\n",
        "            labels: [0, 0, 0, 3, 3, 5, 7, 7, 7, 0, 0, 0]\n",
        "            \"\"\"\n",
        "            start_idx = position # 連続するラベルの先頭位置\n",
        "            end_idx = position + len(list(group)) - 1 # 連続するラベルの最終位置\n",
        "            \n",
        "            # (encode_plus_untaggedで計算した)spansから、文章中の位置を特定\n",
        "            start = spans[start_idx][0] \n",
        "            end = spans[end_idx][1]\n",
        "            \n",
        "            # 次のspanの位置に更新\n",
        "            position = end_idx + 1\n",
        "\n",
        "            if label != 0: # ラベルが0以外ならば、新たな固有表現として追加。\n",
        "                entity = {\n",
        "                    \"name\": text[start:end],\n",
        "                    \"span\": [start, end],\n",
        "                    \"type_id\": label\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities"
      ],
      "metadata": {
        "id": "g1hs2LfUeKks"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [0, 0, 0, 3, 3, 5, 7, 7, 7, 0, 0, 0]\n",
        "# for label, group in itertools.groupby(enumerate(labels), key=lambda x: x[1]):\n",
        "position = 0\n",
        "for label, group in itertools.groupby(labels):\n",
        "  group = list(group)\n",
        "  print(label, group)\n",
        "  start = position\n",
        "  end = position + len(group) - 1\n",
        "  position = end + 1\n",
        "  print(start)\n",
        "  print(end)"
      ],
      "metadata": {
        "id": "R8KgrDWdWRux",
        "outputId": "5ec66642-572a-43e0-b3c5-c7a6f2b22c5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [0, 0, 0]\n",
            "0\n",
            "2\n",
            "3 [3, 3]\n",
            "3\n",
            "4\n",
            "5 [5]\n",
            "5\n",
            "5\n",
            "7 [7, 7, 7]\n",
            "6\n",
            "8\n",
            "0 [0, 0, 0]\n",
            "9\n",
            "11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [0, 0, 0, 3, 3, 5, 7, 7, 7, 0, 0, 0]\n",
        "for label, group in itertools.groupby(enumerate(labels), key=lambda x: x[1]):\n",
        "  group = list(group)\n",
        "  print(label, group)\n",
        "  start = group[0][0]\n",
        "  end = group[-1][0]\n",
        "  print(start)\n",
        "  print(end)"
      ],
      "metadata": {
        "id": "GoIFpznvZzS3",
        "outputId": "70f0051e-419b-4789-b5bc-0b362ce9b321",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [(0, 0), (1, 0), (2, 0)]\n",
            "0\n",
            "2\n",
            "3 [(3, 3), (4, 3)]\n",
            "3\n",
            "4\n",
            "5 [(5, 5)]\n",
            "5\n",
            "5\n",
            "7 [(6, 7), (7, 7), (8, 7)]\n",
            "6\n",
            "8\n",
            "0 [(9, 0), (10, 0), (11, 0)]\n",
            "9\n",
            "11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 検証用 ###\n",
        "\n",
        "import pprint\n",
        "data = dataset_test[281]\n",
        "pprint.pprint(data)\n",
        "text = data[\"text\"]\n",
        "\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "max_length = 32\n",
        "tokens = [] # トークン格納用\n",
        "tokens_original = [] # トークンに対応する文章中の文字列格納用\n",
        "words = tokenizer.word_tokenizer.tokenize(text) # MeCabで単語に分割\n",
        "# words = tokenizer.tokenize(text) # Bertのトークナイザーで分割\n",
        "\n",
        "print(words)\n",
        "\n",
        "for word in words:\n",
        "    # 単語をサブワードに分割\n",
        "    tokens_word = tokenizer.subword_tokenizer.tokenize(word) \n",
        "    tokens.extend(tokens_word)\n",
        "    if tokens_word[0] == '[UNK]': # 未知語への対応\n",
        "        tokens_original.append(word)\n",
        "    else:\n",
        "        tokens_original.extend([\n",
        "            token.replace('##','') for token in tokens_word\n",
        "        ])\n",
        "\n",
        "print(tokens)\n",
        "print(tokens_original)\n",
        "\n",
        "# 符号化を行いBERTに入力できる形式にする。\n",
        "encoding = tokenizer.encode_plus(\n",
        "    tokens, \n",
        "    max_length=max_length, \n",
        "    padding='max_length' if max_length else False, \n",
        "    truncation=True if max_length else False\n",
        ")\n",
        "\n",
        "print(encoding)\n",
        "\n",
        "\n",
        "# 各トークンの文章中での位置を調べる。（空白の位置を考慮する）\n",
        "position = 0\n",
        "spans = [] # トークンの位置を追加していく。\n",
        "for token in tokens_original:\n",
        "    l = len(token)\n",
        "    while 1:\n",
        "        if token != text[position:position+l]:\n",
        "            print(token)\n",
        "            print(text[position:position+l])\n",
        "            position += 1\n",
        "        else:\n",
        "            spans.append([position, position+l])\n",
        "            position += l\n",
        "            break\n",
        "\n",
        "print(spans)\n",
        "\n",
        "sequence_length = len(encoding['input_ids'])\n",
        "# 特殊トークン[CLS]に対するダミーのspanを追加。\n",
        "spans = [[-1, -1]] + spans[:sequence_length-2] \n",
        "# 特殊トークン[SEP]、[PAD]に対するダミーのspanを追加。\n",
        "spans = spans + [[-1, -1]] * ( sequence_length - len(spans) ) \n",
        "\n",
        "print(spans)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irbuGfi06icP",
        "outputId": "2208335c-57da-4cb2-fda2-89041f381178"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ダニス・タノヴィッチ', 'は', 'ユーゴスラビア', '出身', 'の', '映画', '監督', '・', '脚本', '家', 'で', 'ある', '。']\n",
            "['ダニ', '##ス', '##・', '##タ', '##ノ', '##ヴィッチ', 'は', 'ユーゴスラビア', '出身', 'の', '映画', '監督', '・', '脚本', '家', 'で', 'ある', '。']\n",
            "['ダニ', 'ス', '・', 'タ', 'ノ', 'ヴィッチ', 'は', 'ユーゴスラビア', '出身', 'の', '映画', '監督', '・', '脚本', '家', 'で', 'ある', '。']\n",
            "{'input_ids': [2, 7828, 28466, 28472, 28502, 28710, 12399, 9, 9244, 750, 5, 450, 734, 35, 3011, 167, 12, 31, 8, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
            "[[0, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 10], [10, 11], [11, 18], [18, 20], [20, 21], [21, 23], [23, 25], [25, 26], [26, 28], [28, 29], [29, 30], [30, 32], [32, 33]]\n",
            "[[-1, -1], [0, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 10], [10, 11], [11, 18], [18, 20], [20, 21], [21, 23], [23, 25], [25, 26], [26, 28], [28, 29], [29, 30], [30, 32], [32, 33], [-1, -1], [-1, -1], [-1, -1], [-1, -1], [-1, -1], [-1, -1], [-1, -1], [-1, -1], [-1, -1], [-1, -1], [-1, -1], [-1, -1], [-1, -1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 訓練時に使うトークナイザーをロード\n",
        "tokenizer = NerTokenizerForTrain.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zi9FtkEReV64",
        "outputId": "e041516d-d622-43f1-b79f-43c6c12a3e1b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
            "The class this function is called from is 'NerTokenizerForTrain'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CreateDataset(Dataset):\n",
        "  \"\"\"データセット作成\n",
        "  \"\"\"\n",
        "  def __init__(self, dataset, tokenizer, max_length):\n",
        "    self.dataset = dataset\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_length = max_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    text = self.dataset[index][\"text\"]\n",
        "    entities = self.dataset[index][\"entities\"]\n",
        "    encoding = tokenizer.encode_plus_tagged(text, entities, max_length=self.max_length)\n",
        "\n",
        "    input_ids = torch.tensor(encoding[\"input_ids\"])\n",
        "    token_type_ids = torch.tensor(encoding[\"token_type_ids\"])\n",
        "    attention_mask = torch.tensor(encoding[\"attention_mask\"])\n",
        "    labels = torch.tensor(encoding[\"labels\"])\n",
        "\n",
        "    return {\n",
        "      \"input_ids\": input_ids,\n",
        "      \"token_type_ids\": token_type_ids,\n",
        "      \"attention_mask\": attention_mask,\n",
        "      \"labels\": labels\n",
        "    }"
      ],
      "metadata": {
        "id": "gfIb0iWBmx0I"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データセットの作成\n",
        "dataset_train_for_loader = CreateDataset(dataset_train, tokenizer, max_length=128)\n",
        "dataset_val_for_loader = CreateDataset(dataset_val, tokenizer, max_length=128)\n",
        "\n",
        "# データローダーの作成\n",
        "dataloader_train = DataLoader(dataset_train_for_loader, batch_size=32, shuffle=True, pin_memory=True)\n",
        "dataloader_val = DataLoader(dataset_val_for_loader, batch_size=256, shuffle=True, pin_memory=True)\n",
        "\n",
        "dataloaders_dict = {\"train\": dataloader_train, \"val\": dataloader_val}"
      ],
      "metadata": {
        "id": "1W8XVrShrgCQ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train_for_loader[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sdBHMbLIT_6",
        "outputId": "cafaabbc-f2f4-4386-8f52-f2ef6c3a1197"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([    2,  4049, 28680, 28535, 28697, 18687,    13,  1113,  1883,     5,\n",
              "          3614,  6657,  1091,     8,     3,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
              " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'labels': tensor([0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0])}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "model = BertModel.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\", add_pooling_layer=False, output_attentions=True, output_hidden_states=True)\n",
        "\n",
        "class MyBertForTokenClassification(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MyBertForTokenClassification, self).__init__()\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.bert = model\n",
        "\n",
        "        self.lstm = torch.nn.LSTM(input_size=config.hidden_size, hidden_size =config.hidden_size, batch_first=True, bidirectional=True)\n",
        "\n",
        "        classifier_dropout = (\n",
        "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
        "        )\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        \n",
        "        self.classifier= nn.Linear(config.hidden_size*2, config.num_labels)\n",
        "\n",
        "        # 重み初期化処理\n",
        "        nn.init.normal_(self.classifier.weight, std=0.02)\n",
        "        nn.init.normal_(self.classifier.bias, 0)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        token_type_ids,\n",
        "        attention_mask,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "        ):\n",
        "      \n",
        "        return_dict = return_dict if return_dict is not None else config.use_return_dict\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        # print(outputs.keys())\n",
        "\n",
        "        # lstmout, _ = self.lstm(outputs[0], None)\n",
        "        lstmout, _ = self.lstm(outputs[\"last_hidden_state\"], None)\n",
        "\n",
        "        logits = self.classifier(self.dropout(lstmout))\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return TokenClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qCdMMYEQvNz",
        "outputId": "ad4c9dad-622a-4cdb-a3de-81252ead71ab"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'bert.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU使えるならGPU使う\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 学習済みモデルのロード\n",
        "# model_ = BertForTokenClassification.from_pretrained(MODEL_NAME, num_labels=9)\n",
        "model = MyBertForTokenClassification()\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soVEaylKgEk4",
        "outputId": "e743fa9c-02bf-4a82-d81c-5def41a35de9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyBertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lstm): LSTM(768, 768, batch_first=True, bidirectional=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=1536, out_features=9, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 最適化器\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=2e-5)\n",
        "\n",
        "# モデルを学習させる関数を作成\n",
        "def train_model(net, dataloaders_dict, optimizer, num_epochs):\n",
        "\n",
        "    # GPUが使えるかを確認\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"使用デバイス：\", device)\n",
        "    print('-----start-------')\n",
        "\n",
        "    # ネットワークをGPUへ\n",
        "    net.to(device)\n",
        "\n",
        "    # ネットワークがある程度固定であれば、高速化させる\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # ミニバッチのサイズ\n",
        "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
        "\n",
        "    # epochのループ\n",
        "    for epoch in range(num_epochs):\n",
        "        # epochごとの訓練と検証のループ\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train()  # モデルを訓練モードに\n",
        "            else:\n",
        "                net.eval()   # モデルを検証モードに\n",
        "\n",
        "            epoch_loss = 0.0  # epochの損失和\n",
        "            iteration = 1\n",
        "\n",
        "            # データローダーからミニバッチを取り出すループ\n",
        "            for batch in (dataloaders_dict[phase]):\n",
        "                # batchはTextとLableの辞書型変数\n",
        "\n",
        "                # GPUが使えるならGPUにデータを送る\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "\n",
        "                # optimizerを初期化\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # 順伝搬（forward）計算\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "                    # BERTに入力\n",
        "                    output = net(input_ids=input_ids, \n",
        "                                          token_type_ids=None, \n",
        "                                          attention_mask=attention_mask, \n",
        "                                          labels=labels,\n",
        "                                          return_dict=True)\n",
        "                    \n",
        "                    loss = output[0]\n",
        "\n",
        "                    # 訓練時はバックプロパゲーション\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                        optimizer.step()\n",
        "\n",
        "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
        "                            print(f\"イテレーション {iteration} || Loss: {loss:.4f}\")\n",
        "\n",
        "                    iteration += 1\n",
        "\n",
        "                    # 損失の合計を更新\n",
        "                    epoch_loss += loss.item() * batch_size\n",
        "\n",
        "            # epochごとのloss\n",
        "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs} | phase {phase} |  Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    return net"
      ],
      "metadata": {
        "id": "QL4pkF070TZ8"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習・検証を実行\n",
        "num_epochs = 5\n",
        "net_trained = train_model(model, dataloaders_dict, optimizer, num_epochs=num_epochs)"
      ],
      "metadata": {
        "id": "g8JlfJDj2fDw",
        "outputId": "6da506d4-5c68-4abb-ea01-fa8830bdab99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用デバイス： cuda:0\n",
            "-----start-------\n",
            "イテレーション 10 || Loss: 0.6668\n",
            "イテレーション 20 || Loss: 0.4154\n",
            "イテレーション 30 || Loss: 0.2560\n",
            "イテレーション 40 || Loss: 0.2459\n",
            "イテレーション 50 || Loss: 0.1604\n",
            "イテレーション 60 || Loss: 0.1445\n",
            "イテレーション 70 || Loss: 0.1274\n",
            "イテレーション 80 || Loss: 0.1325\n",
            "イテレーション 90 || Loss: 0.0953\n",
            "イテレーション 100 || Loss: 0.0813\n",
            "Epoch 1/5 | phase train |  Loss: 0.3044\n",
            "Epoch 1/5 | phase val |  Loss: 0.0106\n",
            "イテレーション 10 || Loss: 0.0828\n",
            "イテレーション 20 || Loss: 0.0689\n",
            "イテレーション 30 || Loss: 0.0787\n",
            "イテレーション 40 || Loss: 0.0458\n",
            "イテレーション 50 || Loss: 0.0274\n",
            "イテレーション 60 || Loss: 0.0342\n",
            "イテレーション 70 || Loss: 0.0302\n",
            "イテレーション 80 || Loss: 0.0284\n",
            "イテレーション 90 || Loss: 0.0422\n",
            "イテレーション 100 || Loss: 0.0283\n",
            "Epoch 2/5 | phase train |  Loss: 0.0492\n",
            "Epoch 2/5 | phase val |  Loss: 0.0046\n",
            "イテレーション 10 || Loss: 0.0342\n",
            "イテレーション 20 || Loss: 0.0138\n",
            "イテレーション 30 || Loss: 0.0224\n",
            "イテレーション 40 || Loss: 0.0140\n",
            "イテレーション 50 || Loss: 0.0223\n",
            "イテレーション 60 || Loss: 0.0220\n",
            "イテレーション 70 || Loss: 0.0142\n",
            "イテレーション 80 || Loss: 0.0109\n",
            "イテレーション 90 || Loss: 0.0288\n",
            "イテレーション 100 || Loss: 0.0200\n",
            "Epoch 3/5 | phase train |  Loss: 0.0196\n",
            "Epoch 3/5 | phase val |  Loss: 0.0041\n",
            "イテレーション 10 || Loss: 0.0128\n",
            "イテレーション 20 || Loss: 0.0040\n",
            "イテレーション 30 || Loss: 0.0057\n",
            "イテレーション 40 || Loss: 0.0043\n",
            "イテレーション 50 || Loss: 0.0112\n",
            "イテレーション 60 || Loss: 0.0160\n",
            "イテレーション 70 || Loss: 0.0111\n",
            "イテレーション 80 || Loss: 0.0070\n",
            "イテレーション 90 || Loss: 0.0093\n",
            "イテレーション 100 || Loss: 0.0075\n",
            "Epoch 4/5 | phase train |  Loss: 0.0108\n",
            "Epoch 4/5 | phase val |  Loss: 0.0038\n",
            "イテレーション 10 || Loss: 0.0065\n",
            "イテレーション 20 || Loss: 0.0074\n",
            "イテレーション 30 || Loss: 0.0079\n",
            "イテレーション 40 || Loss: 0.0081\n",
            "イテレーション 50 || Loss: 0.0058\n",
            "イテレーション 60 || Loss: 0.0090\n",
            "イテレーション 70 || Loss: 0.0042\n",
            "イテレーション 80 || Loss: 0.0053\n",
            "イテレーション 90 || Loss: 0.0077\n",
            "イテレーション 100 || Loss: 0.0035\n",
            "Epoch 5/5 | phase train |  Loss: 0.0070\n",
            "Epoch 5/5 | phase val |  Loss: 0.0039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデル保存\n",
        "# torch.save(net_trained.state_dict(), './model.pth')"
      ],
      "metadata": {
        "id": "0zhLxePHsGA3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # モデルロード\n",
        "# model = BertModel.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\", add_pooling_layer=False, output_attentions=True, output_hidden_states=True)\n",
        "# net_trained_ = MyBertForTokenClassification()\n",
        "# net_trained_.load_state_dict(torch.load('./model.pth'), strict=False)\n",
        "# net_trained_.to(device)"
      ],
      "metadata": {
        "id": "u2SPtvUcqgZ6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# テスト時に使うトークナイザーをロード\n",
        "tokenizer = NerTokenizerForTest.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIWIuvPxJKfU",
        "outputId": "6119eb6a-3d04-4acf-d49b-f4efb9122b5d"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
            "The class this function is called from is 'NerTokenizerForTest'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text, tokenizer, model):\n",
        "    \"\"\"BERTで固有表現抽出を行うための関数。\n",
        "    \"\"\"\n",
        "    # 符号化\n",
        "    encoding, spans = tokenizer.encode_plus_untagged(\n",
        "        text, return_tensors='pt'\n",
        "    )\n",
        "    encoding = { k: v.cuda() for k, v in encoding.items() }\n",
        "    # print(encoding)\n",
        "\n",
        "    # ラベルの予測値の計算\n",
        "    with torch.no_grad():\n",
        "        output = model(**encoding)\n",
        "        scores = output.logits\n",
        "        labels_predicted = scores[0].argmax(-1).cpu().numpy().tolist() \n",
        "\n",
        "    # ラベル列を固有表現に変換\n",
        "    entities = tokenizer.convert_bert_output_to_entities(\n",
        "        text, labels_predicted, spans\n",
        "    )\n",
        "\n",
        "    return entities\n",
        "\n",
        "# 固有表現抽出\n",
        "entities_list = [] # 正解の固有表現\n",
        "entities_predicted_list = [] # 予測された固有表現\n",
        "for sample in tqdm(dataset_test):\n",
        "    text = sample['text']\n",
        "    entities_predicted = predict(text, tokenizer, net_trained) # BERTで予測\n",
        "    entities_list.append(sample['entities'])\n",
        "    entities_predicted_list.append( entities_predicted )"
      ],
      "metadata": {
        "id": "iIRTHph5gJj4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30350ce9-1fc0-42bd-d7c2-98092e07932a"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1070/1070 [00:53<00:00, 20.01it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 4\n",
        "print(\"# 正解 #\")\n",
        "print(entities_list[i])\n",
        "print(\"# 推論 #\")\n",
        "print(entities_predicted_list[i])\n",
        "print(\"# もとの文章 #\")\n",
        "print(dataset_test[i][\"text\"])"
      ],
      "metadata": {
        "id": "70U_pGw1hNWy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc92c727-4bc8-486f-dc8c-c0403cf80135"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 正解 #\n",
            "[{'name': 'ジョンソン', 'span': [0, 5], 'type_id': 1}, {'name': 'フランス', 'span': [6, 10], 'type_id': 5}, {'name': 'ペリエ・ヴィッテル', 'span': [11, 20], 'type_id': 2}, {'name': 'アジア地区', 'span': [23, 28], 'type_id': 5}]\n",
            "# 推論 #\n",
            "[{'name': 'ジョンソン', 'span': [0, 5], 'type_id': 1}, {'name': 'フランス', 'span': [6, 10], 'type_id': 5}, {'name': 'ペリエ・ヴィッテル', 'span': [11, 20], 'type_id': 2}, {'name': 'アジア', 'span': [23, 26], 'type_id': 5}]\n",
            "# もとの文章 #\n",
            "ジョンソンはフランスのペリエ・ヴィッテルにて、アジア地区シニアマネジャーを務めた。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(entities_list, entities_predicted_list, type_id=None):\n",
        "    \"\"\"\n",
        "    正解と予測を比較し、モデルの固有表現抽出の性能を評価する。\n",
        "    type_idがNoneのときは、全ての固有表現のタイプに対して評価する。\n",
        "    type_idが整数を指定すると、その固有表現のタイプのIDに対して評価を行う。\n",
        "    \"\"\"\n",
        "    num_entities = 0 # 固有表現(正解)の個数\n",
        "    num_predictions = 0 # BERTにより予測された固有表現の個数\n",
        "    num_correct = 0 # BERTにより予測のうち正解であった固有表現の数\n",
        "\n",
        "    # それぞれの文章で予測と正解を比較。\n",
        "    # 予測は文章中の位置とタイプIDが一致すれば正解とみなす。\n",
        "    for entities, entities_predicted in zip(entities_list, entities_predicted_list):\n",
        "\n",
        "        if type_id:\n",
        "            entities = [ e for e in entities if e['type_id'] == type_id ]\n",
        "            entities_predicted = [ \n",
        "                e for e in entities_predicted if e['type_id'] == type_id\n",
        "            ]\n",
        "            \n",
        "        get_span_type = lambda e: (e['span'][0], e['span'][1], e['type_id'])\n",
        "        set_entities = set( get_span_type(e) for e in entities )\n",
        "        set_entities_predicted = set( get_span_type(e) for e in entities_predicted )\n",
        "\n",
        "        num_entities += len(entities)\n",
        "        num_predictions += len(entities_predicted)\n",
        "        num_correct += len( set_entities & set_entities_predicted )\n",
        "\n",
        "    # 指標を計算\n",
        "    precision = num_correct/num_predictions # 適合率\n",
        "    recall = num_correct/num_entities # 再現率\n",
        "    f_value = 2*precision*recall/(precision+recall) # F値\n",
        "\n",
        "    result = {\n",
        "        'num_entities': num_entities,\n",
        "        'num_predictions': num_predictions,\n",
        "        'num_correct': num_correct,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f_value': f_value\n",
        "    }\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "kesRewvRhVVd"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 評価結果\n",
        "import pandas as pd\n",
        "eval_df = pd.DataFrame()\n",
        "for k, v in type_id_dict.items():\n",
        "  eval_res = evaluate_model(entities_list, entities_predicted_list, type_id=v)\n",
        "  eval_df[k] = eval_res.values()\n",
        "\n",
        "eval_res_all = evaluate_model(entities_list, entities_predicted_list, type_id=None)\n",
        "eval_df[\"ALL\"] = eval_res_all.values()\n",
        "\n",
        "eval_df.index = eval_res_all.keys()\n",
        "eval_df"
      ],
      "metadata": {
        "id": "1dzE6nRXA9Ws",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "1f6b15b5-da0c-41e4-fe19-f244949d096e"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                         人名         法人名      政治的組織名     その他の組織名          地名  \\\n",
              "num_entities     604.000000  504.000000  249.000000  222.000000  452.000000   \n",
              "num_predictions  613.000000  508.000000  254.000000  213.000000  456.000000   \n",
              "num_correct      576.000000  457.000000  205.000000  181.000000  398.000000   \n",
              "precision          0.939641    0.899606    0.807087    0.849765    0.872807   \n",
              "recall             0.953642    0.906746    0.823293    0.815315    0.880531   \n",
              "f_value            0.946590    0.903162    0.815109    0.832184    0.876652   \n",
              "\n",
              "                        施設名         製品名       イベント名          ALL  \n",
              "num_entities     222.000000  231.000000  203.000000  2687.000000  \n",
              "num_predictions  236.000000  248.000000  201.000000  2729.000000  \n",
              "num_correct      190.000000  189.000000  179.000000  2375.000000  \n",
              "precision          0.805085    0.762097    0.890547     0.870282  \n",
              "recall             0.855856    0.818182    0.881773     0.883885  \n",
              "f_value            0.829694    0.789144    0.886139     0.877031  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0ce99056-3f7e-4765-8dd0-58eab639b647\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>人名</th>\n",
              "      <th>法人名</th>\n",
              "      <th>政治的組織名</th>\n",
              "      <th>その他の組織名</th>\n",
              "      <th>地名</th>\n",
              "      <th>施設名</th>\n",
              "      <th>製品名</th>\n",
              "      <th>イベント名</th>\n",
              "      <th>ALL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>num_entities</th>\n",
              "      <td>604.000000</td>\n",
              "      <td>504.000000</td>\n",
              "      <td>249.000000</td>\n",
              "      <td>222.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>222.000000</td>\n",
              "      <td>231.000000</td>\n",
              "      <td>203.000000</td>\n",
              "      <td>2687.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>num_predictions</th>\n",
              "      <td>613.000000</td>\n",
              "      <td>508.000000</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>213.000000</td>\n",
              "      <td>456.000000</td>\n",
              "      <td>236.000000</td>\n",
              "      <td>248.000000</td>\n",
              "      <td>201.000000</td>\n",
              "      <td>2729.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>num_correct</th>\n",
              "      <td>576.000000</td>\n",
              "      <td>457.000000</td>\n",
              "      <td>205.000000</td>\n",
              "      <td>181.000000</td>\n",
              "      <td>398.000000</td>\n",
              "      <td>190.000000</td>\n",
              "      <td>189.000000</td>\n",
              "      <td>179.000000</td>\n",
              "      <td>2375.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.939641</td>\n",
              "      <td>0.899606</td>\n",
              "      <td>0.807087</td>\n",
              "      <td>0.849765</td>\n",
              "      <td>0.872807</td>\n",
              "      <td>0.805085</td>\n",
              "      <td>0.762097</td>\n",
              "      <td>0.890547</td>\n",
              "      <td>0.870282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.953642</td>\n",
              "      <td>0.906746</td>\n",
              "      <td>0.823293</td>\n",
              "      <td>0.815315</td>\n",
              "      <td>0.880531</td>\n",
              "      <td>0.855856</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.881773</td>\n",
              "      <td>0.883885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_value</th>\n",
              "      <td>0.946590</td>\n",
              "      <td>0.903162</td>\n",
              "      <td>0.815109</td>\n",
              "      <td>0.832184</td>\n",
              "      <td>0.876652</td>\n",
              "      <td>0.829694</td>\n",
              "      <td>0.789144</td>\n",
              "      <td>0.886139</td>\n",
              "      <td>0.877031</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0ce99056-3f7e-4765-8dd0-58eab639b647')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0ce99056-3f7e-4765-8dd0-58eab639b647 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0ce99056-3f7e-4765-8dd0-58eab639b647');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dSkpQsDxSfYN"
      },
      "execution_count": 97,
      "outputs": []
    }
  ]
}