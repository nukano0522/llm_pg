{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "torch_bert_ner.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNnI1kxogi3hHnxDMunhLfH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ffae0f924aa440b1b9e926b5a3c0a6d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dcd38d358c45489f829d3a0a21165c9b",
              "IPY_MODEL_8ed5afedb5774460a576ac4817baf5a7",
              "IPY_MODEL_2c4fddb8f21c4d88a74f9a318ae17636"
            ],
            "layout": "IPY_MODEL_c6332e68efd7490eb4f75a160b9c7cbd"
          }
        },
        "dcd38d358c45489f829d3a0a21165c9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d070e58f764046ecb467b8efeb8c6144",
            "placeholder": "​",
            "style": "IPY_MODEL_6c71cd10d0504ff09bfee3298a18ca32",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "8ed5afedb5774460a576ac4817baf5a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3620deae78cd4d979d9ce54750bce018",
            "max": 445021143,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70894a4ccf1a42f28e81b92b0e794b80",
            "value": 445021143
          }
        },
        "2c4fddb8f21c4d88a74f9a318ae17636": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_793dc76665ed4460b448751e1643ae28",
            "placeholder": "​",
            "style": "IPY_MODEL_351ff9984c744a878140d0f40307849b",
            "value": " 424M/424M [00:10&lt;00:00, 29.2MB/s]"
          }
        },
        "c6332e68efd7490eb4f75a160b9c7cbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d070e58f764046ecb467b8efeb8c6144": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c71cd10d0504ff09bfee3298a18ca32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3620deae78cd4d979d9ce54750bce018": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70894a4ccf1a42f28e81b92b0e794b80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "793dc76665ed4460b448751e1643ae28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "351ff9984c744a878140d0f40307849b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nukano0522/pytorch/blob/master/bert_ner/torch_ner_bert%2Blstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_96F4lbjdWvR",
        "outputId": "c21ba731-9457-47b8-81a0-cbf9a771b05d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers==4.21.2 in /usr/local/lib/python3.7/dist-packages (4.21.2)\n",
            "Requirement already satisfied: fugashi==1.1.2 in /usr/local/lib/python3.7/dist-packages (1.1.2)\n",
            "Requirement already satisfied: ipadic==1.0.0 in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (0.9.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (4.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.21.2) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.21.2) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.21.2) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.2) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.2) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.21.2 fugashi==1.1.2 ipadic==1.0.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import unicodedata\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Optional, Tuple, Union\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertJapaneseTokenizer, BertForTokenClassification, BertModel\n",
        "from transformers.modeling_outputs import TokenClassifierOutput\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "from transformers import BertConfig, BertModel\n",
        "\n",
        "config = BertConfig()\n",
        "\n",
        "# 日本語学習済みモデル\n",
        "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'"
      ],
      "metadata": {
        "id": "n4mSZ7oYdkP8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def torch_fix_seed(seed=42):\n",
        "    # Python random\n",
        "    random.seed(seed)\n",
        "    # Numpy\n",
        "    np.random.seed(seed)\n",
        "    # Pytorch\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.use_deterministic_algorithms = True\n",
        "\n",
        "\n",
        "torch_fix_seed()"
      ],
      "metadata": {
        "id": "vgLXKtm8qc_a"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config.num_labels = 9\n",
        "print(config.num_labels)\n",
        "print(config.use_return_dict)\n",
        "config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSH8P4lMQj-B",
        "outputId": "6ea60310-4dcc-4548-d448-b99add55ed04"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n",
            "True\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertConfig {\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"id2label\": {\n",
              "    \"0\": \"LABEL_0\",\n",
              "    \"1\": \"LABEL_1\",\n",
              "    \"2\": \"LABEL_2\",\n",
              "    \"3\": \"LABEL_3\",\n",
              "    \"4\": \"LABEL_4\",\n",
              "    \"5\": \"LABEL_5\",\n",
              "    \"6\": \"LABEL_6\",\n",
              "    \"7\": \"LABEL_7\",\n",
              "    \"8\": \"LABEL_8\"\n",
              "  },\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"label2id\": {\n",
              "    \"LABEL_0\": 0,\n",
              "    \"LABEL_1\": 1,\n",
              "    \"LABEL_2\": 2,\n",
              "    \"LABEL_3\": 3,\n",
              "    \"LABEL_4\": 4,\n",
              "    \"LABEL_5\": 5,\n",
              "    \"LABEL_6\": 6,\n",
              "    \"LABEL_7\": 7,\n",
              "    \"LABEL_8\": 8\n",
              "  },\n",
              "  \"layer_norm_eps\": 1e-12,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"bert\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"transformers_version\": \"4.21.2\",\n",
              "  \"type_vocab_size\": 2,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 30522\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# データのダウンロード\n",
        "!git clone --branch v2.0 https://github.com/stockmarkteam/ner-wikipedia-dataset "
      ],
      "metadata": {
        "id": "GSlvnbNOX9bP",
        "outputId": "90d5d1ef-4042-464c-8eaa-93244d1f52c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ner-wikipedia-dataset' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# データのロード\n",
        "dataset = json.load(open('ner-wikipedia-dataset/ner.json','r'))\n",
        "\n",
        "# 固有表現のタイプとIDを対応付る辞書 \n",
        "type_id_dict = {\n",
        "    \"人名\": 1,\n",
        "    \"法人名\": 2,\n",
        "    \"政治的組織名\": 3,\n",
        "    \"その他の組織名\": 4,\n",
        "    \"地名\": 5,\n",
        "    \"施設名\": 6,\n",
        "    \"製品名\": 7,\n",
        "    \"イベント名\": 8\n",
        "}\n",
        "\n",
        "# カテゴリーをラベルに変更、文字列の正規化する。\n",
        "for sample in dataset:\n",
        "    sample['text'] = unicodedata.normalize('NFKC', sample['text'])\n",
        "    for e in sample[\"entities\"]:\n",
        "        e['type_id'] = type_id_dict[e['type']]\n",
        "        del e['type']\n",
        "\n",
        "# データセットの分割\n",
        "n = len(dataset)\n",
        "n_train = int(n*0.6)\n",
        "n_val = int(n*0.2)\n",
        "dataset_train = dataset[:n_train]\n",
        "dataset_val = dataset[n_train:n_train+n_val]\n",
        "dataset_test = dataset[n_train+n_val:]\n",
        "\n",
        "print(f\"Length of train: {len(dataset_train)}\")\n",
        "print(f\"Length of val: {len(dataset_val)}\")\n",
        "print(f\"Length of test: {len(dataset_test)}\")"
      ],
      "metadata": {
        "id": "CmPwfQD0YNQe",
        "outputId": "3fe5cd67-9503-4342-8fa3-adfad1eb9e8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train: 3205\n",
            "Length of val: 1068\n",
            "Length of test: 1070\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "data = dataset_train[1]\n",
        "pprint.pprint(data)\n",
        "\n",
        "entities = data[\"entities\"]\n",
        "\n",
        "splitted = []\n",
        "position = 0\n",
        "\n",
        "text = data[\"text\"]\n",
        "print(text)\n",
        "\n",
        "for entity in entities:\n",
        "    start = entity['span'][0]\n",
        "    end = entity['span'][1]\n",
        "    label = entity['type_id']\n",
        "    # 固有表現ではないものには0のラベルを付与\n",
        "    splitted.append({'text': text[position:start], 'label':0}) \n",
        "    # 固有表現には、固有表現のタイプに対応するIDをラベルとして付与\n",
        "    splitted.append({'text': text[start:end], 'label':label}) \n",
        "    position = end\n",
        "\n",
        "# 最後の固有表現から文末に、0のラベルを付与\n",
        "splitted.append({'text': text[position:], 'label':0})\n",
        "splitted = [ s for s in splitted if s['text'] ] # 長さ0の文字列は除く\n",
        "\n",
        "splitted"
      ],
      "metadata": {
        "id": "WQ1dGRWzIG_4",
        "outputId": "d8a25329-d954-44f0-c8eb-c5908b4aaad5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'curid': '2415078',\n",
            " 'entities': [{'name': 'レッドフォックス株式会社', 'span': [0, 12], 'type_id': 2},\n",
            "              {'name': '東京都千代田区', 'span': [14, 21], 'type_id': 5}],\n",
            " 'text': 'レッドフォックス株式会社は、東京都千代田区に本社を置くITサービス企業である。'}\n",
            "レッドフォックス株式会社は、東京都千代田区に本社を置くITサービス企業である。\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'text': 'レッドフォックス株式会社', 'label': 2},\n",
              " {'text': 'は、', 'label': 0},\n",
              " {'text': '東京都千代田区', 'label': 5},\n",
              " {'text': 'に本社を置くITサービス企業である。', 'label': 0}]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "max_length = 32\n",
        "tokens = [] # トークンを追加していく\n",
        "labels = [] # トークンのラベルを追加していく\n",
        "for s in splitted:\n",
        "    text = s['text']\n",
        "    label = s['label']\n",
        "    tokens_splitted = tokenizer.tokenize(text)\n",
        "    labels_splitted = [label] * len(tokens_splitted)\n",
        "    tokens.extend(tokens_splitted)\n",
        "    labels.extend(labels_splitted)\n",
        "\n",
        "print(tokens)\n",
        "print(labels)\n",
        "\n",
        "# 符号化を行いBERTに入力できる形式にする。\n",
        "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "# input_idsをencodingに変換\n",
        "# encoding = tokenizer.prepare_for_model(\n",
        "encoding = tokenizer.encode_plus(\n",
        "    input_ids, \n",
        "    max_length=max_length, \n",
        "    padding='max_length', \n",
        "    truncation=True)\n",
        "\n",
        "\n",
        "# 特殊トークン[CLS]、[SEP]のラベルを0\n",
        "labels = [0] + labels[:max_length-2] + [0] \n",
        "# 特殊トークン[PAD]のラベルを0\n",
        "labels = labels + [0]*( max_length - len(labels) ) \n",
        "encoding['labels'] = labels\n",
        "\n",
        "print(input_ids)\n",
        "print(encoding)\n",
        "print(tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"]))"
      ],
      "metadata": {
        "id": "navpNInSWJ3l",
        "outputId": "151c0de8-4487-4906-d4a6-85b9253c16ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['レッド', 'フォックス', '株式会社', 'は', '、', '東京', '都', '千代田', '区', 'に', '本社', 'を', '置く', 'IT', 'サービス', '企業', 'で', 'ある', '。']\n",
            "[2, 2, 2, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[3990, 13779, 1275, 9, 6, 391, 409, 9674, 280, 7, 2557, 11, 3045, 8267, 1645, 1189, 12, 31, 8]\n",
            "{'input_ids': [2, 3990, 13779, 1275, 9, 6, 391, 409, 9674, 280, 7, 2557, 11, 3045, 8267, 1645, 1189, 12, 31, 8, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [0, 2, 2, 2, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
            "['[CLS]', 'レッド', 'フォックス', '株式会社', 'は', '、', '東京', '都', '千代田', '区', 'に', '本社', 'を', '置く', 'IT', 'サービス', '企業', 'で', 'ある', '。', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NER_tokenizer(BertJapaneseTokenizer):\n",
        "\n",
        "    def create_tokens_and_labels(self, splitted):\n",
        "        \"\"\"分割された文字列をトークン化し、ラベルを付与\n",
        "        Args：\n",
        "          splitted: 分割された文字列\n",
        "            例：\n",
        "            [{'text': 'レッドフォックス株式会社', 'label': 2},\n",
        "             {'text': 'は、', 'label': 0},\n",
        "             {'text': '東京都千代田区', 'label': 5},\n",
        "             {'text': 'に本社を置くITサービス企業である。', 'label': 0}]\n",
        "        Return:\n",
        "          tokens, labels\n",
        "            例：\n",
        "            ['レッド', 'フォックス', '株式会社', 'は', '、', '東京', '都', '千代田', '区', 'に', '本社', 'を', '置く', 'IT', 'サービス', '企業', 'で', 'ある', '。']\n",
        "            [2, 2, 2, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "        \"\"\"\n",
        "        tokens = [] # トークン格納用\n",
        "        labels = [] # トークンに対応するラベル格納用\n",
        "        for s in splitted:\n",
        "            text = s['text']\n",
        "            label = s['label']\n",
        "            tokens_splitted = self.tokenize(text) # BertJapaneseTokenizerのトークナイザを使ってトークンに分割\n",
        "            labels_splitted = [label] * len(tokens_splitted)\n",
        "            tokens.extend(tokens_splitted)\n",
        "            labels.extend(labels_splitted)\n",
        "        \n",
        "        return tokens, labels\n",
        "\n",
        "\n",
        "    def encoding_for_bert(self, tokens, labels):\n",
        "        \"\"\"符号化を行いBERTに入力できる形式にする\n",
        "        Args:\n",
        "          tokens: トークン列\n",
        "          labels: トークンに対応するラベルの列\n",
        "        Returns: \n",
        "          encoding: BERTに入力できる形式\n",
        "          例：\n",
        "          {'input_ids': [2, 3990, 13779, 1275, 9, 6, 391, 409, 9674, 280, 7, 2557, 11, 3045, 8267, 1645, 1189, 12, 31, 8, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
        "          　'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
        "          　'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
        "            'labels': [0, 2, 2, 2, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
        "\n",
        "        \"\"\"\n",
        "        input_ids = self.convert_tokens_to_ids(tokens)\n",
        "        # input_idsをencodingに変換\n",
        "        encoding = self.encode_plus(\n",
        "            input_ids, \n",
        "            max_length=max_length, \n",
        "            padding='max_length', \n",
        "            truncation=True\n",
        "        ) \n",
        "        # 特殊トークン[CLS]、[SEP]のラベルを0\n",
        "        labels = [0] + labels[:max_length-2] + [0] \n",
        "        # 特殊トークン[PAD]のラベルを0\n",
        "        labels = labels + [0]*( max_length - len(labels) ) \n",
        "        encoding['labels'] = labels\n",
        "\n",
        "        return encoding\n",
        "\n",
        "\n",
        "    def encode_plus_tagged(self, text, entities, max_length):\n",
        "        \"\"\"文章とそれに含まれる固有表現が与えられた時に、符号化とラベル列の作成\n",
        "        Args:\n",
        "          text: 元の文章\n",
        "          entities: 文章中の固有表現の位置(span)とラベル(type_id)の情報\n",
        "\n",
        "        \"\"\"\n",
        "        # 固有表現の前後でtextを分割し、それぞれのラベルをつけておく。\n",
        "        entities = sorted(entities, key=lambda x: x['span'][0]) # 固有表現の位置の昇順でソート\n",
        "        splitted = [] # 分割後の文字列格納用\n",
        "        position = 0\n",
        "        for entity in entities:\n",
        "            start = entity['span'][0]\n",
        "            end = entity['span'][1]\n",
        "            label = entity['type_id']\n",
        "            # 固有表現ではないものには0のラベルを付与\n",
        "            splitted.append({'text': text[position:start], 'label':0}) \n",
        "            # 固有表現には、固有表現のタイプに対応するIDをラベルとして付与\n",
        "            splitted.append({'text': text[start:end], 'label':label}) \n",
        "            position = end\n",
        "\n",
        "        # 最後の固有表現から文末に、0のラベルを付与\n",
        "        splitted.append({'text': text[position:], 'label':0})\n",
        "        # positionとspan[0]の値が同じだと空白文字にラベル0が付与されるため、長さ0の文字列は除く（例：{'text': '', 'label': 0}）\n",
        "        splitted = [ s for s in splitted if s['text'] ] \n",
        "\n",
        "        # 分割された文字列をトークン化し、ラベルを付与\n",
        "        tokens, labels = self.create_tokens_and_labels(splitted)\n",
        "\n",
        "\n",
        "\n",
        "        return encoding\n",
        "\n",
        "\n",
        "    def encode_plus_untagged(\n",
        "        self, text, max_length=None, return_tensors=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        文章をトークン化し、それぞれのトークンの文章中の位置も特定しておく。\n",
        "        \"\"\"\n",
        "        # 文章のトークン化を行い、\n",
        "        # それぞれのトークンと文章中の文字列を対応づける。\n",
        "        tokens = [] # トークンを追加していく。\n",
        "        tokens_original = [] # トークンに対応する文章中の文字列を追加していく。\n",
        "        words = self.word_tokenizer.tokenize(text) # MeCabで単語に分割\n",
        "        for word in words:\n",
        "            # 単語をサブワードに分割\n",
        "            tokens_word = self.subword_tokenizer.tokenize(word) \n",
        "            tokens.extend(tokens_word)\n",
        "            if tokens_word[0] == '[UNK]': # 未知語への対応\n",
        "                tokens_original.append(word)\n",
        "            else:\n",
        "                tokens_original.extend([\n",
        "                    token.replace('##','') for token in tokens_word\n",
        "                ])\n",
        "\n",
        "        # 各トークンの文章中での位置を調べる。（空白の位置を考慮する）\n",
        "        position = 0\n",
        "        spans = [] # トークンの位置を追加していく。\n",
        "        for token in tokens_original:\n",
        "            l = len(token)\n",
        "            while 1:\n",
        "                if token != text[position:position+l]:\n",
        "                    position += 1\n",
        "                else:\n",
        "                    spans.append([position, position+l])\n",
        "                    position += l\n",
        "                    break\n",
        "\n",
        "        # 符号化を行いBERTに入力できる形式にする。\n",
        "        input_ids = self.convert_tokens_to_ids(tokens) \n",
        "        encoding = self.prepare_for_model(\n",
        "            input_ids, \n",
        "            max_length=max_length, \n",
        "            padding='max_length' if max_length else False, \n",
        "            truncation=True if max_length else False\n",
        "        )\n",
        "        sequence_length = len(encoding['input_ids'])\n",
        "        # 特殊トークン[CLS]に対するダミーのspanを追加。\n",
        "        spans = [[-1, -1]] + spans[:sequence_length-2] \n",
        "        # 特殊トークン[SEP]、[PAD]に対するダミーのspanを追加。\n",
        "        spans = spans + [[-1, -1]] * ( sequence_length - len(spans) ) \n",
        "\n",
        "        # 必要に応じてtorch.Tensorにする。\n",
        "        if return_tensors == 'pt':\n",
        "            encoding = { k: torch.tensor([v]) for k, v in encoding.items() }\n",
        "\n",
        "        return encoding, spans\n",
        "\n",
        "\n",
        "    def convert_bert_output_to_entities(self, text, labels, spans):\n",
        "        \"\"\"\n",
        "        文章、ラベル列の予測値、各トークンの位置から固有表現を得る。\n",
        "        \"\"\"\n",
        "        # labels, spansから特殊トークンに対応する部分を取り除く\n",
        "        labels = [label for label, span in zip(labels, spans) if span[0] != -1]\n",
        "        spans = [span for span in spans if span[0] != -1]\n",
        "\n",
        "        # 同じラベルが連続するトークンをまとめて、固有表現を抽出する。\n",
        "        entities = []\n",
        "        for label, group \\\n",
        "            in itertools.groupby(enumerate(labels), key=lambda x: x[1]):\n",
        "            \n",
        "            group = list(group)\n",
        "            start = spans[group[0][0]][0]\n",
        "            end = spans[group[-1][0]][1]\n",
        "\n",
        "            if label != 0: # ラベルが0以外ならば、新たな固有表現として追加。\n",
        "                entity = {\n",
        "                    \"name\": text[start:end],\n",
        "                    \"span\": [start, end],\n",
        "                    \"type_id\": label\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities"
      ],
      "metadata": {
        "id": "g1hs2LfUeKks"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zi9FtkEReV64",
        "outputId": "c0c2ace6-6714-4578-fedd-116f638aa763"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
            "The class this function is called from is 'NER_tokenizer'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "tmp = dataset_train[1]\n",
        "pprint.pprint(tmp)\n",
        "pprint.pprint(tokenizer.encode_plus_tagged(text=tmp[\"text\"], entities=tmp[\"entities\"], max_length=32), width=200)"
      ],
      "metadata": {
        "id": "tKwEiwXmegfL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7d1f168-9a47-46e2-8c55-1a762601b8ec"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'curid': '2415078',\n",
            " 'entities': [{'name': 'レッドフォックス株式会社', 'span': [0, 12], 'type_id': 2},\n",
            "              {'name': '東京都千代田区', 'span': [14, 21], 'type_id': 5}],\n",
            " 'text': 'レッドフォックス株式会社は、東京都千代田区に本社を置くITサービス企業である。'}\n",
            "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            " 'input_ids': [2, 3990, 13779, 1275, 9, 6, 391, 409, 9674, 280, 7, 2557, 11, 3045, 8267, 1645, 1189, 12, 31, 8, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            " 'labels': [0, 2, 2, 2, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CreateDataset(Dataset):\n",
        "  \"\"\"データセット作成\n",
        "  \"\"\"\n",
        "  def __init__(self, dataset, tokenizer, max_length):\n",
        "    self.dataset = dataset\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_length = max_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    text = self.dataset[index][\"text\"]\n",
        "    entities = self.dataset[index][\"entities\"]\n",
        "    encoding = tokenizer.encode_plus_tagged(text, entities, max_length=self.max_length)\n",
        "\n",
        "    input_ids = torch.tensor(encoding[\"input_ids\"])\n",
        "    token_type_ids = torch.tensor(encoding[\"token_type_ids\"])\n",
        "    attention_mask = torch.tensor(encoding[\"attention_mask\"])\n",
        "    labels = torch.tensor(encoding[\"labels\"])\n",
        "\n",
        "    return {\n",
        "      \"input_ids\": input_ids,\n",
        "      \"token_type_ids\": token_type_ids,\n",
        "      \"attention_mask\": attention_mask,\n",
        "      \"labels\": labels\n",
        "    }"
      ],
      "metadata": {
        "id": "gfIb0iWBmx0I"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データセットの作成\n",
        "dataset_train_for_loader = CreateDataset(dataset_train, tokenizer, max_length=128)\n",
        "dataset_val_for_loader = CreateDataset(dataset_val, tokenizer, max_length=128)\n",
        "\n",
        "# データローダーの作成\n",
        "dataloader_train = DataLoader(dataset_train_for_loader, batch_size=32, shuffle=True, pin_memory=True)\n",
        "dataloader_val = DataLoader(dataset_val_for_loader, batch_size=256, shuffle=True, pin_memory=True)\n",
        "\n",
        "dataloaders_dict = {\"train\": dataloader_train, \"val\": dataloader_val}"
      ],
      "metadata": {
        "id": "1W8XVrShrgCQ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset_train_for_loader[1]\n",
        "print(data)\n",
        "tokenizer.convert_ids_to_tokens(data[\"input_ids\"])[0:10]"
      ],
      "metadata": {
        "id": "qIU32EinKFR4",
        "outputId": "891fb175-789a-4566-993e-97545b6aeaf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([    2,  3990, 13779,  1275,     9,     6,   391,   409,  9674,   280,\n",
            "            7,  2557,    11,  3045,  8267,  1645,  1189,    12,    31,     8,\n",
            "            3,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([0, 2, 2, 2, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]', 'レッド', 'フォックス', '株式会社', 'は', '、', '東京', '都', '千代田', '区']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "model = BertModel.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\", add_pooling_layer=False, output_attentions=True, output_hidden_states=True)\n",
        "\n",
        "class MyBertForTokenClassification(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MyBertForTokenClassification, self).__init__()\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.bert = model\n",
        "\n",
        "        self.lstm = torch.nn.LSTM(input_size=config.hidden_size, hidden_size =config.hidden_size, batch_first=True, bidirectional=True)\n",
        "\n",
        "        classifier_dropout = (\n",
        "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
        "        )\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        \n",
        "        self.classifier= nn.Linear(config.hidden_size*2, config.num_labels)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        # self.post_init()\n",
        "\n",
        "        # 重み初期化処理\n",
        "        nn.init.normal_(self.classifier.weight, std=0.02)\n",
        "        nn.init.normal_(self.classifier.bias, 0)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        token_type_ids,\n",
        "        attention_mask,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "        ):\n",
        "      \n",
        "        return_dict = return_dict if return_dict is not None else config.use_return_dict\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            # position_ids=position_ids,\n",
        "            # head_mask=head_mask,\n",
        "            # inputs_embeds=inputs_embeds,\n",
        "            # output_attentions=output_attentions,\n",
        "            # output_hidden_states=output_hidden_states,\n",
        "            # labels=labels,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        # print(outputs[0].size())\n",
        "        lstmout, _ = self.lstm(outputs[0], None)\n",
        "\n",
        "        logits = self.classifier(self.dropout(lstmout))\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return TokenClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124,
          "referenced_widgets": [
            "ffae0f924aa440b1b9e926b5a3c0a6d4",
            "dcd38d358c45489f829d3a0a21165c9b",
            "8ed5afedb5774460a576ac4817baf5a7",
            "2c4fddb8f21c4d88a74f9a318ae17636",
            "c6332e68efd7490eb4f75a160b9c7cbd",
            "d070e58f764046ecb467b8efeb8c6144",
            "6c71cd10d0504ff09bfee3298a18ca32",
            "3620deae78cd4d979d9ce54750bce018",
            "70894a4ccf1a42f28e81b92b0e794b80",
            "793dc76665ed4460b448751e1643ae28",
            "351ff9984c744a878140d0f40307849b"
          ]
        },
        "id": "7qCdMMYEQvNz",
        "outputId": "7f668493-4ad6-45b7-b06a-681b250f9fc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/424M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ffae0f924aa440b1b9e926b5a3c0a6d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.bias', 'bert.pooler.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU使えるならGPU使う\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 学習済みモデルのロード\n",
        "# model_ = BertForTokenClassification.from_pretrained(MODEL_NAME, num_labels=9)\n",
        "model = MyBertForTokenClassification()\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soVEaylKgEk4",
        "outputId": "ac010de0-ea41-4012-ecca-616e0702f0ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyBertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lstm): LSTM(768, 768, batch_first=True, bidirectional=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=1536, out_features=9, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 最適化器\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=2e-5)\n",
        "\n",
        "# モデルを学習させる関数を作成\n",
        "def train_model(net, dataloaders_dict, optimizer, num_epochs):\n",
        "\n",
        "    # GPUが使えるかを確認\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"使用デバイス：\", device)\n",
        "    print('-----start-------')\n",
        "\n",
        "    # ネットワークをGPUへ\n",
        "    net.to(device)\n",
        "\n",
        "    # ネットワークがある程度固定であれば、高速化させる\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # ミニバッチのサイズ\n",
        "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
        "\n",
        "    # epochのループ\n",
        "    for epoch in range(num_epochs):\n",
        "        # epochごとの訓練と検証のループ\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train()  # モデルを訓練モードに\n",
        "            else:\n",
        "                net.eval()   # モデルを検証モードに\n",
        "\n",
        "            epoch_loss = 0.0  # epochの損失和\n",
        "            iteration = 1\n",
        "\n",
        "            # データローダーからミニバッチを取り出すループ\n",
        "            for batch in (dataloaders_dict[phase]):\n",
        "                # batchはTextとLableの辞書型変数\n",
        "\n",
        "                # GPUが使えるならGPUにデータを送る\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "\n",
        "                # optimizerを初期化\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # 順伝搬（forward）計算\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "                    # BERTに入力\n",
        "                    output = net(input_ids=input_ids, \n",
        "                                          token_type_ids=None, \n",
        "                                          attention_mask=attention_mask, \n",
        "                                          labels=labels,\n",
        "                                          return_dict=False)\n",
        "                    \n",
        "                    loss = output[0]\n",
        "\n",
        "                    # 訓練時はバックプロパゲーション\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                        optimizer.step()\n",
        "\n",
        "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
        "                            print(f\"イテレーション {iteration} || Loss: {loss:.4f}\")\n",
        "\n",
        "                    iteration += 1\n",
        "\n",
        "                    # 損失の合計を更新\n",
        "                    epoch_loss += loss.item() * batch_size\n",
        "\n",
        "            # epochごとのloss\n",
        "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs} | phase {phase} |  Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    return net"
      ],
      "metadata": {
        "id": "QL4pkF070TZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習・検証を実行\n",
        "num_epochs = 5\n",
        "net_trained = train_model(model, dataloaders_dict, optimizer, num_epochs=num_epochs)"
      ],
      "metadata": {
        "id": "g8JlfJDj2fDw",
        "outputId": "21e9b7e7-1f40-4a58-dbbb-5299e395c2ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用デバイス： cuda:0\n",
            "-----start-------\n",
            "イテレーション 10 || Loss: 0.7730\n",
            "イテレーション 20 || Loss: 0.3011\n",
            "イテレーション 30 || Loss: 0.3711\n",
            "イテレーション 40 || Loss: 0.2171\n",
            "イテレーション 50 || Loss: 0.1450\n",
            "イテレーション 60 || Loss: 0.1777\n",
            "イテレーション 70 || Loss: 0.1209\n",
            "イテレーション 80 || Loss: 0.1212\n",
            "イテレーション 90 || Loss: 0.1282\n",
            "イテレーション 100 || Loss: 0.1286\n",
            "Epoch 1/5 | phase train |  Loss: 0.3255\n",
            "Epoch 1/5 | phase val |  Loss: 0.0119\n",
            "イテレーション 10 || Loss: 0.0746\n",
            "イテレーション 20 || Loss: 0.0565\n",
            "イテレーション 30 || Loss: 0.0560\n",
            "イテレーション 40 || Loss: 0.0706\n",
            "イテレーション 50 || Loss: 0.0584\n",
            "イテレーション 60 || Loss: 0.0371\n",
            "イテレーション 70 || Loss: 0.0586\n",
            "イテレーション 80 || Loss: 0.0581\n",
            "イテレーション 90 || Loss: 0.0275\n",
            "イテレーション 100 || Loss: 0.0234\n",
            "Epoch 2/5 | phase train |  Loss: 0.0536\n",
            "Epoch 2/5 | phase val |  Loss: 0.0051\n",
            "イテレーション 10 || Loss: 0.0252\n",
            "イテレーション 20 || Loss: 0.0185\n",
            "イテレーション 30 || Loss: 0.0216\n",
            "イテレーション 40 || Loss: 0.0216\n",
            "イテレーション 50 || Loss: 0.0222\n",
            "イテレーション 60 || Loss: 0.0264\n",
            "イテレーション 70 || Loss: 0.0167\n",
            "イテレーション 80 || Loss: 0.0212\n",
            "イテレーション 90 || Loss: 0.0230\n",
            "イテレーション 100 || Loss: 0.0141\n",
            "Epoch 3/5 | phase train |  Loss: 0.0200\n",
            "Epoch 3/5 | phase val |  Loss: 0.0035\n",
            "イテレーション 10 || Loss: 0.0113\n",
            "イテレーション 20 || Loss: 0.0068\n",
            "イテレーション 30 || Loss: 0.0086\n",
            "イテレーション 40 || Loss: 0.0145\n",
            "イテレーション 50 || Loss: 0.0070\n",
            "イテレーション 60 || Loss: 0.0107\n",
            "イテレーション 70 || Loss: 0.0104\n",
            "イテレーション 80 || Loss: 0.0130\n",
            "イテレーション 90 || Loss: 0.0065\n",
            "イテレーション 100 || Loss: 0.0152\n",
            "Epoch 4/5 | phase train |  Loss: 0.0109\n",
            "Epoch 4/5 | phase val |  Loss: 0.0037\n",
            "イテレーション 10 || Loss: 0.0091\n",
            "イテレーション 20 || Loss: 0.0030\n",
            "イテレーション 30 || Loss: 0.0073\n",
            "イテレーション 40 || Loss: 0.0052\n",
            "イテレーション 50 || Loss: 0.0040\n",
            "イテレーション 60 || Loss: 0.0066\n",
            "イテレーション 70 || Loss: 0.0053\n",
            "イテレーション 80 || Loss: 0.0105\n",
            "イテレーション 90 || Loss: 0.0063\n",
            "イテレーション 100 || Loss: 0.0030\n",
            "Epoch 5/5 | phase train |  Loss: 0.0067\n",
            "Epoch 5/5 | phase val |  Loss: 0.0036\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデル保存\n",
        "# torch.save(net_trained.state_dict(), './model.pth')"
      ],
      "metadata": {
        "id": "0zhLxePHsGA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # モデルロード\n",
        "# model = BertModel.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\", add_pooling_layer=False, output_attentions=True, output_hidden_states=True)\n",
        "# net_trained_ = MyBertForTokenClassification()\n",
        "# net_trained_.load_state_dict(torch.load('./model.pth'), strict=False)\n",
        "# net_trained_.to(device)"
      ],
      "metadata": {
        "id": "u2SPtvUcqgZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JyIry1u_sUjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text, tokenizer, model):\n",
        "    \"\"\"BERTで固有表現抽出を行うための関数。\n",
        "    \"\"\"\n",
        "    # 符号化\n",
        "    encoding, spans = tokenizer.encode_plus_untagged(\n",
        "        text, return_tensors='pt'\n",
        "    )\n",
        "    encoding = { k: v.cuda() for k, v in encoding.items() }\n",
        "    # print(encoding)\n",
        "\n",
        "    # ラベルの予測値の計算\n",
        "    with torch.no_grad():\n",
        "        output = model(**encoding)\n",
        "        scores = output.logits\n",
        "        labels_predicted = scores[0].argmax(-1).cpu().numpy().tolist() \n",
        "\n",
        "    # ラベル列を固有表現に変換\n",
        "    entities = tokenizer.convert_bert_output_to_entities(\n",
        "        text, labels_predicted, spans\n",
        "    )\n",
        "\n",
        "    return entities\n",
        "\n",
        "# 固有表現抽出\n",
        "entities_list = [] # 正解の固有表現\n",
        "entities_predicted_list = [] # 予測された固有表現\n",
        "for sample in tqdm(dataset_test):\n",
        "    text = sample['text']\n",
        "    entities_predicted = predict(text, tokenizer, net_trained) # BERTで予測\n",
        "    entities_list.append(sample['entities'])\n",
        "    entities_predicted_list.append( entities_predicted )"
      ],
      "metadata": {
        "id": "iIRTHph5gJj4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c201bb14-b11a-4e03-e483-249f77e19d5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1070/1070 [00:16<00:00, 63.43it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 4\n",
        "print(\"# 正解 #\")\n",
        "print(entities_list[i])\n",
        "print(\"# 推論 #\")\n",
        "print(entities_predicted_list[i])\n",
        "print(\"# もとの文章 #\")\n",
        "print(dataset_test[i][\"text\"])"
      ],
      "metadata": {
        "id": "70U_pGw1hNWy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db361a60-4a3c-4ffd-dbf7-9e0a0805e0a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 正解 #\n",
            "[{'name': 'ジョンソン', 'span': [0, 5], 'type_id': 1}, {'name': 'フランス', 'span': [6, 10], 'type_id': 5}, {'name': 'ペリエ・ヴィッテル', 'span': [11, 20], 'type_id': 2}, {'name': 'アジア地区', 'span': [23, 28], 'type_id': 5}]\n",
            "# 推論 #\n",
            "[{'name': 'ジョンソン', 'span': [0, 5], 'type_id': 1}, {'name': 'フランス', 'span': [6, 10], 'type_id': 5}, {'name': 'ペリエ・ヴィッテル', 'span': [11, 20], 'type_id': 4}]\n",
            "# もとの文章 #\n",
            "ジョンソンはフランスのペリエ・ヴィッテルにて、アジア地区シニアマネジャーを務めた。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(entities_list, entities_predicted_list, type_id=None):\n",
        "    \"\"\"\n",
        "    正解と予測を比較し、モデルの固有表現抽出の性能を評価する。\n",
        "    type_idがNoneのときは、全ての固有表現のタイプに対して評価する。\n",
        "    type_idが整数を指定すると、その固有表現のタイプのIDに対して評価を行う。\n",
        "    \"\"\"\n",
        "    num_entities = 0 # 固有表現(正解)の個数\n",
        "    num_predictions = 0 # BERTにより予測された固有表現の個数\n",
        "    num_correct = 0 # BERTにより予測のうち正解であった固有表現の数\n",
        "\n",
        "    # それぞれの文章で予測と正解を比較。\n",
        "    # 予測は文章中の位置とタイプIDが一致すれば正解とみなす。\n",
        "    for entities, entities_predicted in zip(entities_list, entities_predicted_list):\n",
        "\n",
        "        if type_id:\n",
        "            entities = [ e for e in entities if e['type_id'] == type_id ]\n",
        "            entities_predicted = [ \n",
        "                e for e in entities_predicted if e['type_id'] == type_id\n",
        "            ]\n",
        "            \n",
        "        get_span_type = lambda e: (e['span'][0], e['span'][1], e['type_id'])\n",
        "        set_entities = set( get_span_type(e) for e in entities )\n",
        "        set_entities_predicted = set( get_span_type(e) for e in entities_predicted )\n",
        "\n",
        "        num_entities += len(entities)\n",
        "        num_predictions += len(entities_predicted)\n",
        "        num_correct += len( set_entities & set_entities_predicted )\n",
        "\n",
        "    # 指標を計算\n",
        "    precision = num_correct/num_predictions # 適合率\n",
        "    recall = num_correct/num_entities # 再現率\n",
        "    f_value = 2*precision*recall/(precision+recall) # F値\n",
        "\n",
        "    result = {\n",
        "        'num_entities': num_entities,\n",
        "        'num_predictions': num_predictions,\n",
        "        'num_correct': num_correct,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f_value': f_value\n",
        "    }\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "kesRewvRhVVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 評価結果\n",
        "import pandas as pd\n",
        "eval_df = pd.DataFrame()\n",
        "for k, v in type_id_dict.items():\n",
        "  eval_res = evaluate_model(entities_list, entities_predicted_list, type_id=v)\n",
        "  eval_df[k] = eval_res.values()\n",
        "\n",
        "eval_res_all = evaluate_model(entities_list, entities_predicted_list, type_id=None)\n",
        "eval_df[\"ALL\"] = eval_res_all.values()\n",
        "\n",
        "eval_df.index = eval_res_all.keys()\n",
        "eval_df"
      ],
      "metadata": {
        "id": "1dzE6nRXA9Ws",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "07e97073-116d-43f6-ab79-a0a813cc8553"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                         人名         法人名      政治的組織名     その他の組織名          地名  \\\n",
              "num_entities     604.000000  504.000000  249.000000  222.000000  452.000000   \n",
              "num_predictions  615.000000  493.000000  255.000000  227.000000  476.000000   \n",
              "num_correct      573.000000  446.000000  210.000000  186.000000  408.000000   \n",
              "precision          0.931707    0.904665    0.823529    0.819383    0.857143   \n",
              "recall             0.948675    0.884921    0.843373    0.837838    0.902655   \n",
              "f_value            0.940115    0.894684    0.833333    0.828508    0.879310   \n",
              "\n",
              "                        施設名         製品名       イベント名          ALL  \n",
              "num_entities     222.000000  231.000000  203.000000  2687.000000  \n",
              "num_predictions  231.000000  266.000000  201.000000  2764.000000  \n",
              "num_correct      182.000000  197.000000  173.000000  2375.000000  \n",
              "precision          0.787879    0.740602    0.860697     0.859262  \n",
              "recall             0.819820    0.852814    0.852217     0.883885  \n",
              "f_value            0.803532    0.792757    0.856436     0.871400  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-521900be-d38c-4f17-b4e1-02ac22dbd843\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>人名</th>\n",
              "      <th>法人名</th>\n",
              "      <th>政治的組織名</th>\n",
              "      <th>その他の組織名</th>\n",
              "      <th>地名</th>\n",
              "      <th>施設名</th>\n",
              "      <th>製品名</th>\n",
              "      <th>イベント名</th>\n",
              "      <th>ALL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>num_entities</th>\n",
              "      <td>604.000000</td>\n",
              "      <td>504.000000</td>\n",
              "      <td>249.000000</td>\n",
              "      <td>222.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>222.000000</td>\n",
              "      <td>231.000000</td>\n",
              "      <td>203.000000</td>\n",
              "      <td>2687.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>num_predictions</th>\n",
              "      <td>615.000000</td>\n",
              "      <td>493.000000</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>227.000000</td>\n",
              "      <td>476.000000</td>\n",
              "      <td>231.000000</td>\n",
              "      <td>266.000000</td>\n",
              "      <td>201.000000</td>\n",
              "      <td>2764.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>num_correct</th>\n",
              "      <td>573.000000</td>\n",
              "      <td>446.000000</td>\n",
              "      <td>210.000000</td>\n",
              "      <td>186.000000</td>\n",
              "      <td>408.000000</td>\n",
              "      <td>182.000000</td>\n",
              "      <td>197.000000</td>\n",
              "      <td>173.000000</td>\n",
              "      <td>2375.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.931707</td>\n",
              "      <td>0.904665</td>\n",
              "      <td>0.823529</td>\n",
              "      <td>0.819383</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.787879</td>\n",
              "      <td>0.740602</td>\n",
              "      <td>0.860697</td>\n",
              "      <td>0.859262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.948675</td>\n",
              "      <td>0.884921</td>\n",
              "      <td>0.843373</td>\n",
              "      <td>0.837838</td>\n",
              "      <td>0.902655</td>\n",
              "      <td>0.819820</td>\n",
              "      <td>0.852814</td>\n",
              "      <td>0.852217</td>\n",
              "      <td>0.883885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_value</th>\n",
              "      <td>0.940115</td>\n",
              "      <td>0.894684</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.828508</td>\n",
              "      <td>0.879310</td>\n",
              "      <td>0.803532</td>\n",
              "      <td>0.792757</td>\n",
              "      <td>0.856436</td>\n",
              "      <td>0.871400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-521900be-d38c-4f17-b4e1-02ac22dbd843')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-521900be-d38c-4f17-b4e1-02ac22dbd843 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-521900be-d38c-4f17-b4e1-02ac22dbd843');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "veGtGf8_4EEF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}