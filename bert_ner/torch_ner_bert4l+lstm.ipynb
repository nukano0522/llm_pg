{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "torch_bert_ner.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPKiXVbYFCncr4ZylUgS9Gi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nukano0522/pytorch/blob/master/bert_ner/torch_ner_bert4l%2Blstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_96F4lbjdWvR",
        "outputId": "a64dc53d-f435-4448-dc68-dc1cb9cbb737"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers==4.21.2 in /usr/local/lib/python3.7/dist-packages (4.21.2)\n",
            "Requirement already satisfied: fugashi==1.1.2 in /usr/local/lib/python3.7/dist-packages (1.1.2)\n",
            "Requirement already satisfied: ipadic==1.0.0 in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (3.8.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (0.9.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.21.2) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.21.2) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.21.2) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.2) (2022.6.15)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.21.2 fugashi==1.1.2 ipadic==1.0.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import unicodedata\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Optional, Tuple, Union\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertJapaneseTokenizer, BertModel\n",
        "from transformers.modeling_outputs import TokenClassifierOutput\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "from transformers import BertModel\n",
        "\n",
        "# 日本語学習済みモデル\n",
        "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'"
      ],
      "metadata": {
        "id": "n4mSZ7oYdkP8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def torch_fix_seed(seed=42):\n",
        "    # Python random\n",
        "    random.seed(seed)\n",
        "    # Numpy\n",
        "    np.random.seed(seed)\n",
        "    # Pytorch\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.use_deterministic_algorithms = True\n",
        "\n",
        "\n",
        "torch_fix_seed()"
      ],
      "metadata": {
        "id": "LAddRe74rHJT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データのダウンロード\n",
        "!git clone --branch v2.0 https://github.com/stockmarkteam/ner-wikipedia-dataset "
      ],
      "metadata": {
        "id": "GSlvnbNOX9bP",
        "outputId": "b7c65c7b-d7b9-418c-a6ac-da7dc675a691",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ner-wikipedia-dataset' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# データのロード\n",
        "dataset = json.load(open('ner-wikipedia-dataset/ner.json','r'))\n",
        "\n",
        "# 固有表現のタイプとIDを対応付る辞書 \n",
        "type_id_dict = {\n",
        "    \"人名\": 1,\n",
        "    \"法人名\": 2,\n",
        "    \"政治的組織名\": 3,\n",
        "    \"その他の組織名\": 4,\n",
        "    \"地名\": 5,\n",
        "    \"施設名\": 6,\n",
        "    \"製品名\": 7,\n",
        "    \"イベント名\": 8\n",
        "}\n",
        "\n",
        "# カテゴリーをラベルに変更、文字列の正規化する。\n",
        "for sample in dataset:\n",
        "    sample['text'] = unicodedata.normalize('NFKC', sample['text'])\n",
        "    for e in sample[\"entities\"]:\n",
        "        e['type_id'] = type_id_dict[e['type']]\n",
        "        del e['type']\n",
        "\n",
        "# データセットの分割\n",
        "n = len(dataset)\n",
        "n_train = int(n*0.6)\n",
        "n_val = int(n*0.2)\n",
        "dataset_train = dataset[:n_train]\n",
        "dataset_val = dataset[n_train:n_train+n_val]\n",
        "dataset_test = dataset[n_train+n_val:]\n",
        "\n",
        "print(f\"Length of train: {len(dataset_train)}\")\n",
        "print(f\"Length of val: {len(dataset_val)}\")\n",
        "print(f\"Length of test: {len(dataset_test)}\")"
      ],
      "metadata": {
        "id": "CmPwfQD0YNQe",
        "outputId": "6659ac8f-d2b8-4ebe-eb5b-1924b4b20aba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train: 3205\n",
            "Length of val: 1068\n",
            "Length of test: 1070\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NerTokenizerForTrain(BertJapaneseTokenizer):\n",
        "\n",
        "  def create_tokens_and_labels(self, splitted):\n",
        "      \"\"\"分割された文字列をトークン化し、ラベルを付与\n",
        "      Args：\n",
        "        splitted: 分割された文字列\n",
        "          例：\n",
        "          [{'text': 'レッドフォックス株式会社', 'label': 2},\n",
        "          {'text': 'は、', 'label': 0},\n",
        "          {'text': '東京都千代田区', 'label': 5},\n",
        "          {'text': 'に本社を置くITサービス企業である。', 'label': 0}]\n",
        "      Return:\n",
        "        tokens, labels\n",
        "          例：\n",
        "          ['レッド', 'フォックス', '株式会社', 'は', '、', '東京', '都', '千代田', '区', 'に', '本社', 'を', '置く', 'IT', 'サービス', '企業', 'で', 'ある', '。']\n",
        "          [2, 2, 2, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "      \"\"\"\n",
        "      tokens = [] # トークン格納用\n",
        "      labels = [] # トークンに対応するラベル格納用\n",
        "      for s in splitted:\n",
        "          text = s['text']\n",
        "          label = s['label']\n",
        "          tokens_splitted = self.tokenize(text) # BertJapaneseTokenizerのトークナイザを使ってトークンに分割\n",
        "          labels_splitted = [label] * len(tokens_splitted)\n",
        "          tokens.extend(tokens_splitted)\n",
        "          labels.extend(labels_splitted)\n",
        "      \n",
        "      return tokens, labels\n",
        "\n",
        "\n",
        "  def encoding_for_bert(self, tokens, labels, max_length):\n",
        "      \"\"\"符号化を行いBERTに入力できる形式にする\n",
        "      Args:\n",
        "        tokens: トークン列\n",
        "        labels: トークンに対応するラベルの列\n",
        "      Returns: \n",
        "        encoding: BERTに入力できる形式\n",
        "        例：\n",
        "        {'input_ids': [2, 3990, 13779, 1275, 9, 6, 391, 409, 9674, 280, 7, 2557, 11, 3045, 8267, 1645, 1189, 12, 31, 8, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
        "        　'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
        "        　'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
        "          'labels': [0, 2, 2, 2, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
        "\n",
        "      \"\"\"\n",
        "      encoding = self.encode_plus(\n",
        "          tokens, \n",
        "          max_length=max_length, \n",
        "          padding='max_length', \n",
        "          truncation=True\n",
        "      ) \n",
        "      # トークン[CLS]、[SEP]のラベルを0\n",
        "      labels = [0] + labels[:max_length-2] + [0] \n",
        "      # トークン[PAD]のラベルを0\n",
        "      labels = labels + [0]*( max_length - len(labels) ) \n",
        "      encoding['labels'] = labels\n",
        "\n",
        "      return encoding\n",
        "\n",
        "\n",
        "  def encode_plus_tagged(self, text, entities, max_length):\n",
        "      \"\"\"文章とそれに含まれる固有表現が与えられた時に、符号化とラベル列の作成\n",
        "      Args:\n",
        "        text: 元の文章\n",
        "        entities: 文章中の固有表現の位置(span)とラベル(type_id)の情報\n",
        "\n",
        "      \"\"\"\n",
        "      # 固有表現の前後でtextを分割し、それぞれのラベルをつけておく。\n",
        "      entities = sorted(entities, key=lambda x: x['span'][0]) # 固有表現の位置の昇順でソート\n",
        "      splitted = [] # 分割後の文字列格納用\n",
        "      position = 0\n",
        "      for entity in entities:\n",
        "          start = entity['span'][0]\n",
        "          end = entity['span'][1]\n",
        "          label = entity['type_id']\n",
        "          # 固有表現ではないものには0のラベルを付与\n",
        "          splitted.append({'text': text[position:start], 'label':0}) \n",
        "          # 固有表現には、固有表現のタイプに対応するIDをラベルとして付与\n",
        "          splitted.append({'text': text[start:end], 'label':label}) \n",
        "          position = end\n",
        "\n",
        "      # 最後の固有表現から文末に、0のラベルを付与\n",
        "      splitted.append({'text': text[position:], 'label':0})\n",
        "      # positionとspan[0]の値が同じだと空白文字にラベル0が付与されるため、長さ0の文字列は除く（例：{'text': '', 'label': 0}）\n",
        "      splitted = [ s for s in splitted if s['text'] ] \n",
        "\n",
        "      # 分割された文字列をトークン化し、ラベルを付与\n",
        "      tokens, labels = self.create_tokens_and_labels(splitted)\n",
        "\n",
        "      # 符号化を行いBERTに入力できる形式にする\n",
        "      encoding = self.encoding_for_bert(tokens, labels, max_length)\n",
        "\n",
        "      return encoding"
      ],
      "metadata": {
        "id": "g1hs2LfUeKks"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NerTokenizerForTest(BertJapaneseTokenizer):\n",
        "\n",
        "    def encoding_for_bert(self, tokens, max_length):\n",
        "        \"\"\"符号化を行いBERTに入力できる形式にする\n",
        "        Args:\n",
        "          tokens: トークン列\n",
        "        Returns: \n",
        "          encoding: BERTに入力できる形式\n",
        "          例：\n",
        "          {'input_ids': [2, 106, 6, 946, 674, 5, 12470, 9921, 5, 859, 6, 2446, 22903, 35, 24831, 11614, 35, 2176, 2200, 35, 3700, 29650, 2446, 333, 9, 6, 2409, 109, 5, 333, 3849, 3], \n",
        "          'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
        "          'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
        "        \"\"\"\n",
        "        encoding = self.encode_plus(\n",
        "            tokens, \n",
        "            max_length=max_length, \n",
        "            padding='max_length', \n",
        "            truncation=True,\n",
        "            return_tensors = \"pt\"\n",
        "        ) \n",
        "\n",
        "        return encoding\n",
        "\n",
        "\n",
        "    def create_spans_of_token(self, tokens_original, encoding):\n",
        "        \"\"\" 各トークン（サブワード）の文章中での位置を調べる\n",
        "          Args:\n",
        "            tokens_original: トークン列をさらにサブワードに分割した列\n",
        "            encoding: \n",
        "            例：tokens_original\n",
        "              ['元々', 'は', '前作', '「', 'The', 'Apple', 's', '」', 'の', 'アウト', ...]\n",
        "          \n",
        "          Return:\n",
        "            spans: 各トークンの文章中の位置([CLS][PAD]などの特殊トークンはダミーで置き換える)\n",
        "            例：\n",
        "              [[-1, -1], [0, 2], [2, 3], [3, 5], [5, 6], [6, 9], [10, 15], [15, 16], ...]\n",
        "        \"\"\"        \n",
        "        position = 0\n",
        "        spans = [] # トークンの位置を追加していく。\n",
        "        for token in tokens_original:\n",
        "            l = len(token)\n",
        "            while 1:\n",
        "                if token != text[position:position+l]:\n",
        "                    \"\"\"例：英語文章のように空白が混ざっていると下記のようにずれるケースがあることを考慮\n",
        "                          token: \"Digital\"\n",
        "                          text[position:position+l]: \" Digita\"\n",
        "                    \"\"\"\n",
        "                    position += 1\n",
        "                else:\n",
        "                    spans.append([position, position+l])\n",
        "                    position += l\n",
        "                    break\n",
        "\n",
        "        sequence_length = len(encoding['input_ids'])\n",
        "        # 特殊トークン[CLS]に対するダミーのspanを追加。\n",
        "        spans = [[-1, -1]] + spans[:sequence_length-2] \n",
        "        # 特殊トークン[SEP]、[PAD]に対するダミーのspanを追加。\n",
        "        spans = spans + [[-1, -1]] * ( sequence_length - len(spans) ) \n",
        "\n",
        "        return spans\n",
        "\n",
        "\n",
        "    def encode_plus_untagged(self, text, max_length=None):\n",
        "        \"\"\"文章をトークン化し、それぞれのトークンの文章中の位置も特定しておく。\n",
        "        \"\"\"\n",
        "        # 文章のトークン化を行い、\n",
        "        # それぞれのトークンと文章中の文字列を対応づける。\n",
        "        tokens = [] # トークン格納用\n",
        "        tokens_original = [] # トークンに対応する文章中の文字列格納用\n",
        "        words = self.word_tokenizer.tokenize(text) # MeCabで単語に分割\n",
        "        for word in words:\n",
        "            # 単語をサブワードに分割\n",
        "            tokens_word = self.subword_tokenizer.tokenize(word) \n",
        "            tokens.extend(tokens_word)\n",
        "            if tokens_word[0] == '[UNK]': # 未知語への対応\n",
        "                tokens_original.append(word)\n",
        "            else:\n",
        "                tokens_original.extend([\n",
        "                    token.replace('##','') for token in tokens_word\n",
        "                ])\n",
        "\n",
        "\n",
        "        # 符号化を行いBERTに入力できる形式にする\n",
        "        encoding = self.encoding_for_bert(tokens, max_length)\n",
        "\n",
        "        # 各トークン（サブワード）の文章中での位置を調べる\n",
        "        spans = self.create_spans_of_token(tokens_original, encoding)\n",
        "\n",
        "        return encoding, spans\n",
        "\n",
        "\n",
        "    def convert_bert_output_to_entities(self, text, labels, spans):\n",
        "        \"\"\"文章、ラベル列の予測値、各トークンの位置から固有表現を得る。\n",
        "        \"\"\"\n",
        "        # labels, spansから特殊トークンに対応する部分を取り除く\n",
        "        labels = [label for label, span in zip(labels, spans) if span[0] != -1]\n",
        "        spans = [span for span in spans if span[0] != -1]\n",
        "\n",
        "        # 同じラベルが連続するトークンをまとめて、固有表現を抽出する。\n",
        "        entities = []\n",
        "        position = 0\n",
        "        for label, group in itertools.groupby(labels):\n",
        "            \"\"\"\n",
        "            例：labelsは予測結果\n",
        "            labels: [0, 0, 0, 3, 3, 5, 7, 7, 7, 0, 0, 0]\n",
        "            \"\"\"\n",
        "            start_idx = position # 連続するラベルの先頭位置\n",
        "            end_idx = position + len(list(group)) - 1 # 連続するラベルの最終位置\n",
        "            \n",
        "            # (encode_plus_untaggedで計算した)spansから、文章中の位置を特定\n",
        "            start = spans[start_idx][0] \n",
        "            end = spans[end_idx][1]\n",
        "            \n",
        "            # 次のspanの位置に更新\n",
        "            position = end_idx + 1\n",
        "\n",
        "            if label != 0: # ラベルが0以外ならば、新たな固有表現として追加。\n",
        "                entity = {\n",
        "                    \"name\": text[start:end],\n",
        "                    \"span\": [start, end],\n",
        "                    \"type_id\": label\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities"
      ],
      "metadata": {
        "id": "SzOlDjDtnk6f"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 訓練時に使うトークナイザーをロード\n",
        "tokenizer = NerTokenizerForTrain.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zi9FtkEReV64",
        "outputId": "2313f1c6-38b0-4750-d01f-7b8cf219f745"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
            "The class this function is called from is 'NerTokenizerForTrain'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "tmp = dataset_train[1]\n",
        "pprint.pprint(tmp)\n",
        "encoding_ = tokenizer.encode_plus_tagged(text=tmp[\"text\"], entities=tmp[\"entities\"], max_length=32)\n",
        "pprint.pprint(encoding_, width=200)\n",
        "print(len(encoding_[\"input_ids\"]))"
      ],
      "metadata": {
        "id": "tKwEiwXmegfL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcf71e34-3d6f-484e-bd0d-25b3e31c412d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'curid': '2415078',\n",
            " 'entities': [{'name': 'レッドフォックス株式会社', 'span': [0, 12], 'type_id': 2},\n",
            "              {'name': '東京都千代田区', 'span': [14, 21], 'type_id': 5}],\n",
            " 'text': 'レッドフォックス株式会社は、東京都千代田区に本社を置くITサービス企業である。'}\n",
            "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            " 'input_ids': [2, 3990, 13779, 1275, 9, 6, 391, 409, 9674, 280, 7, 2557, 11, 3045, 8267, 1645, 1189, 12, 31, 8, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            " 'labels': [0, 2, 2, 2, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
            "32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CreateDataset(Dataset):\n",
        "  \"\"\"データセット作成\n",
        "  \"\"\"\n",
        "  def __init__(self, dataset, tokenizer, max_length):\n",
        "    self.dataset = dataset\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_length = max_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    text = self.dataset[index][\"text\"]\n",
        "    entities = self.dataset[index][\"entities\"]\n",
        "    encoding = tokenizer.encode_plus_tagged(text, entities, max_length=self.max_length)\n",
        "\n",
        "    input_ids = torch.tensor(encoding[\"input_ids\"])\n",
        "    token_type_ids = torch.tensor(encoding[\"token_type_ids\"])\n",
        "    attention_mask = torch.tensor(encoding[\"attention_mask\"])\n",
        "    labels = torch.tensor(encoding[\"labels\"])\n",
        "\n",
        "    return {\n",
        "      \"input_ids\": input_ids,\n",
        "      \"token_type_ids\": token_type_ids,\n",
        "      \"attention_mask\": attention_mask,\n",
        "      \"labels\": labels\n",
        "    }"
      ],
      "metadata": {
        "id": "gfIb0iWBmx0I"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データセットの作成\n",
        "dataset_train_for_loader = CreateDataset(dataset_train, tokenizer, max_length=128)\n",
        "dataset_val_for_loader = CreateDataset(dataset_val, tokenizer, max_length=128)\n",
        "\n",
        "# データローダーの作成\n",
        "dataloader_train = DataLoader(dataset_train_for_loader, batch_size=32, shuffle=True, pin_memory=True)\n",
        "dataloader_val = DataLoader(dataset_val_for_loader, batch_size=256, shuffle=True, pin_memory=True)\n",
        "\n",
        "dataloaders_dict = {\"train\": dataloader_train, \"val\": dataloader_val}"
      ],
      "metadata": {
        "id": "1W8XVrShrgCQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertConfig\n",
        "config = BertConfig()\n",
        "\n",
        "config.num_labels = 9\n",
        "print(config.num_labels)\n",
        "print(config.use_return_dict)\n",
        "config"
      ],
      "metadata": {
        "id": "TJcJI0y2pjsS",
        "outputId": "25eafc7a-430f-446b-d266-40f1bfcf1fb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n",
            "True\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertConfig {\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"id2label\": {\n",
              "    \"0\": \"LABEL_0\",\n",
              "    \"1\": \"LABEL_1\",\n",
              "    \"2\": \"LABEL_2\",\n",
              "    \"3\": \"LABEL_3\",\n",
              "    \"4\": \"LABEL_4\",\n",
              "    \"5\": \"LABEL_5\",\n",
              "    \"6\": \"LABEL_6\",\n",
              "    \"7\": \"LABEL_7\",\n",
              "    \"8\": \"LABEL_8\"\n",
              "  },\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"label2id\": {\n",
              "    \"LABEL_0\": 0,\n",
              "    \"LABEL_1\": 1,\n",
              "    \"LABEL_2\": 2,\n",
              "    \"LABEL_3\": 3,\n",
              "    \"LABEL_4\": 4,\n",
              "    \"LABEL_5\": 5,\n",
              "    \"LABEL_6\": 6,\n",
              "    \"LABEL_7\": 7,\n",
              "    \"LABEL_8\": 8\n",
              "  },\n",
              "  \"layer_norm_eps\": 1e-12,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"bert\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"transformers_version\": \"4.21.2\",\n",
              "  \"type_vocab_size\": 2,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 30522\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "model = BertModel.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\", add_pooling_layer=False, output_attentions=True, output_hidden_states=True)\n",
        "\n",
        "class MyBertForTokenClassification(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MyBertForTokenClassification, self).__init__()\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.bert = model\n",
        "\n",
        "        self.lstm = torch.nn.LSTM(input_size=config.hidden_size*4, hidden_size =config.hidden_size, batch_first=True, bidirectional=True)\n",
        "\n",
        "        classifier_dropout = (\n",
        "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
        "        )\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        \n",
        "        self.classifier= nn.Linear(config.hidden_size*2, config.num_labels)\n",
        "\n",
        "        # 重み初期化処理\n",
        "        nn.init.normal_(self.classifier.weight, std=0.02)\n",
        "        nn.init.normal_(self.classifier.bias, 0)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        token_type_ids,\n",
        "        attention_mask,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "        ):\n",
        "      \n",
        "        return_dict = return_dict if return_dict is not None else config.use_return_dict\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        # 隠れ層をすべて取得（https://huggingface.co/docs/transformers/v4.21.3/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions）\n",
        "        # return_dict=Trueの指定が必要\n",
        "        hidden_states = outputs[\"hidden_states\"]\n",
        "\n",
        "        # bertの最終4層の隠れ層\n",
        "        sequence_output = torch.cat([hidden_states[-1*i] for i in range(1, 4+1)], dim=-1)\n",
        "\n",
        "        lstmout, _ = self.lstm(sequence_output, None)\n",
        "\n",
        "        logits = self.classifier(self.dropout(lstmout))\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return TokenClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qCdMMYEQvNz",
        "outputId": "547eea07-c7ac-49e2-b9e6-40b7795b47c9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU使えるならGPU使う\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 学習済みモデルのロード\n",
        "print(\"loading...\")\n",
        "model = MyBertForTokenClassification()\n",
        "model.to(device)\n",
        "print(\"...complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soVEaylKgEk4",
        "outputId": "734eb046-aa09-4294-bada-db21ebf448c9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading...\n",
            "...complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 勾配計算を最後のBertLayerモジュールと追加した分類アダプターのみ実行\n",
        "\n",
        "# 1. まず全部を、勾配計算Falseにしてしまう\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 2. 各モジュールの一部の層を勾配計算ありに変更\n",
        "for i in range(1, 5):\n",
        "  for param in model.bert.encoder.layer[-1*i].parameters():\n",
        "      param.requires_grad = True\n",
        "\n",
        "for param in model.lstm.parameters():\n",
        "      param.requires_grad =True\n",
        "\n",
        "# 3. 識別器を勾配計算ありに変更\n",
        "for param in model.classifier.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "XG91C1NwExSz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 最適化に渡すパラメータ\n",
        "model_params = [{'params': model.bert.encoder.layer[-1*i].parameters(), 'lr': 5e-5} for i in range(1,5)]\n",
        "\n",
        "lstm_params = [{'params':model.lstm.parameters(),'lr':5e-5}]\n",
        "model_params.extend(lstm_params)\n",
        "\n",
        "linear_params = [{'params': model.classifier.parameters(), 'lr': 1e-4}]\n",
        "model_params.extend(linear_params)"
      ],
      "metadata": {
        "id": "xbrV5vCxGCAl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 最適化器\n",
        "# optimizer = torch.optim.Adam(params=model.parameters(), lr=2e-5)\n",
        "optimizer = torch.optim.Adam(model_params)\n",
        "\n",
        "# モデルを学習させる関数を作成\n",
        "def train_model(net, dataloaders_dict, optimizer, num_epochs):\n",
        "\n",
        "    # GPUが使えるかを確認\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"使用デバイス：\", device)\n",
        "    print('-----start-------')\n",
        "\n",
        "    # ネットワークをGPUへ\n",
        "    net.to(device)\n",
        "\n",
        "    # ネットワークがある程度固定であれば、高速化させる\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # ミニバッチのサイズ\n",
        "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
        "\n",
        "    # epochのループ\n",
        "    for epoch in range(num_epochs):\n",
        "        # epochごとの訓練と検証のループ\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train()  # モデルを訓練モードに\n",
        "            else:\n",
        "                net.eval()   # モデルを検証モードに\n",
        "\n",
        "            epoch_loss = 0.0  # epochの損失和\n",
        "            iteration = 1\n",
        "\n",
        "            # データローダーからミニバッチを取り出すループ\n",
        "            for batch in (dataloaders_dict[phase]):\n",
        "                # batchはTextとLableの辞書型変数\n",
        "\n",
        "                # GPUが使えるならGPUにデータを送る\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "\n",
        "                # optimizerを初期化\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # 順伝搬（forward）計算\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "                    # BERTに入力\n",
        "                    output = net(input_ids=input_ids, \n",
        "                                          token_type_ids=None, \n",
        "                                          attention_mask=attention_mask, \n",
        "                                          labels=labels,\n",
        "                                          return_dict=True,\n",
        "                                 )\n",
        "                    \n",
        "                    loss = output.loss\n",
        "\n",
        "                    # 訓練時はバックプロパゲーション\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                        optimizer.step()\n",
        "\n",
        "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
        "                            print(f\"イテレーション {iteration} || Loss: {loss:.4f}\")\n",
        "\n",
        "                    iteration += 1\n",
        "\n",
        "                    # 損失の合計を更新\n",
        "                    epoch_loss += loss.item() * batch_size\n",
        "\n",
        "            # epochごとのloss\n",
        "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs} | phase {phase} |  Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    return net"
      ],
      "metadata": {
        "id": "QL4pkF070TZ8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習・検証を実行\n",
        "num_epochs = 5\n",
        "net_trained = train_model(model, dataloaders_dict, optimizer, num_epochs=num_epochs)"
      ],
      "metadata": {
        "id": "g8JlfJDj2fDw",
        "outputId": "851627f1-34c2-43ab-8c10-a0d64b348a6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用デバイス： cuda:0\n",
            "-----start-------\n",
            "イテレーション 10 || Loss: 0.3647\n",
            "イテレーション 20 || Loss: 0.2239\n",
            "イテレーション 30 || Loss: 0.1501\n",
            "イテレーション 40 || Loss: 0.1161\n",
            "イテレーション 50 || Loss: 0.0596\n",
            "イテレーション 60 || Loss: 0.0532\n",
            "イテレーション 70 || Loss: 0.0362\n",
            "イテレーション 80 || Loss: 0.0607\n",
            "イテレーション 90 || Loss: 0.0480\n",
            "イテレーション 100 || Loss: 0.0443\n",
            "Epoch 1/5 | phase train |  Loss: 0.1918\n",
            "Epoch 1/5 | phase val |  Loss: 0.0042\n",
            "イテレーション 10 || Loss: 0.0320\n",
            "イテレーション 20 || Loss: 0.0286\n",
            "イテレーション 30 || Loss: 0.0132\n",
            "イテレーション 40 || Loss: 0.0248\n",
            "イテレーション 50 || Loss: 0.0137\n",
            "イテレーション 60 || Loss: 0.0253\n",
            "イテレーション 70 || Loss: 0.0245\n",
            "イテレーション 80 || Loss: 0.0211\n",
            "イテレーション 90 || Loss: 0.0196\n",
            "イテレーション 100 || Loss: 0.0163\n",
            "Epoch 2/5 | phase train |  Loss: 0.0236\n",
            "Epoch 2/5 | phase val |  Loss: 0.0032\n",
            "イテレーション 10 || Loss: 0.0063\n",
            "イテレーション 20 || Loss: 0.0077\n",
            "イテレーション 30 || Loss: 0.0058\n",
            "イテレーション 40 || Loss: 0.0134\n",
            "イテレーション 50 || Loss: 0.0153\n",
            "イテレーション 60 || Loss: 0.0094\n",
            "イテレーション 70 || Loss: 0.0083\n",
            "イテレーション 80 || Loss: 0.0191\n",
            "イテレーション 90 || Loss: 0.0091\n",
            "イテレーション 100 || Loss: 0.0117\n",
            "Epoch 3/5 | phase train |  Loss: 0.0138\n",
            "Epoch 3/5 | phase val |  Loss: 0.0035\n",
            "イテレーション 10 || Loss: 0.0115\n",
            "イテレーション 20 || Loss: 0.0081\n",
            "イテレーション 30 || Loss: 0.0153\n",
            "イテレーション 40 || Loss: 0.0062\n",
            "イテレーション 50 || Loss: 0.0061\n",
            "イテレーション 60 || Loss: 0.0108\n",
            "イテレーション 70 || Loss: 0.0063\n",
            "イテレーション 80 || Loss: 0.0123\n",
            "イテレーション 90 || Loss: 0.0067\n",
            "イテレーション 100 || Loss: 0.0087\n",
            "Epoch 4/5 | phase train |  Loss: 0.0087\n",
            "Epoch 4/5 | phase val |  Loss: 0.0033\n",
            "イテレーション 10 || Loss: 0.0037\n",
            "イテレーション 20 || Loss: 0.0075\n",
            "イテレーション 30 || Loss: 0.0136\n",
            "イテレーション 40 || Loss: 0.0026\n",
            "イテレーション 50 || Loss: 0.0065\n",
            "イテレーション 60 || Loss: 0.0027\n",
            "イテレーション 70 || Loss: 0.0042\n",
            "イテレーション 80 || Loss: 0.0076\n",
            "イテレーション 90 || Loss: 0.0043\n",
            "イテレーション 100 || Loss: 0.0033\n",
            "Epoch 5/5 | phase train |  Loss: 0.0058\n",
            "Epoch 5/5 | phase val |  Loss: 0.0035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデル保存\n",
        "torch.save(net_trained.state_dict(), './model.pth')"
      ],
      "metadata": {
        "id": "0zhLxePHsGA3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルロード\n",
        "# model = BertModel.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\", add_pooling_layer=False, output_attentions=True, output_hidden_states=True)\n",
        "# net_trained = MyBertForTokenClassification()\n",
        "# net_trained.load_state_dict(torch.load('./model.pth'), strict=False)\n",
        "# net_trained.to(device)"
      ],
      "metadata": {
        "id": "2QMxD4gSuFq5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# テスト時に使うトークナイザーをロード\n",
        "tokenizer = NerTokenizerForTest.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "id": "rZa2pIX1n_YO",
        "outputId": "74f7b91e-6655-4a3d-e449-d3d418aa61e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
            "The class this function is called from is 'NerTokenizerForTest'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text, tokenizer, model):\n",
        "    \"\"\"BERTで固有表現抽出を行うための関数。\n",
        "    \"\"\"\n",
        "    # 符号化\n",
        "    encoding, spans = tokenizer.encode_plus_untagged(text)\n",
        "    encoding = { k: v.cuda() for k, v in encoding.items() }\n",
        "    # print(encoding)\n",
        "\n",
        "    # ラベルの予測値の計算\n",
        "    with torch.no_grad():\n",
        "        output = model(**encoding)\n",
        "        scores = output.logits\n",
        "        labels_predicted = scores[0].argmax(-1).cpu().numpy().tolist() \n",
        "\n",
        "    # ラベル列を固有表現に変換\n",
        "    entities = tokenizer.convert_bert_output_to_entities(\n",
        "        text, labels_predicted, spans\n",
        "    )\n",
        "\n",
        "    return entities\n",
        "\n",
        "# 固有表現抽出\n",
        "entities_list = [] # 正解の固有表現\n",
        "entities_predicted_list = [] # 予測された固有表現\n",
        "for sample in tqdm(dataset_test):\n",
        "    text = sample['text']\n",
        "    entities_predicted = predict(text, tokenizer, net_trained) # BERTで予測\n",
        "    entities_list.append(sample['entities'])\n",
        "    entities_predicted_list.append( entities_predicted )"
      ],
      "metadata": {
        "id": "iIRTHph5gJj4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff3a1a05-b7cc-495d-c1b7-27f9e623ec9d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1070/1070 [01:03<00:00, 16.80it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 4\n",
        "print(\"# 正解 #\")\n",
        "print(entities_list[i])\n",
        "print(\"# 推論 #\")\n",
        "print(entities_predicted_list[i])\n",
        "print(\"# もとの文章 #\")\n",
        "print(dataset_test[i][\"text\"])"
      ],
      "metadata": {
        "id": "70U_pGw1hNWy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1b98430-97da-42c1-8d5b-4af4458e5462"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 正解 #\n",
            "[{'name': 'ジョンソン', 'span': [0, 5], 'type_id': 1}, {'name': 'フランス', 'span': [6, 10], 'type_id': 5}, {'name': 'ペリエ・ヴィッテル', 'span': [11, 20], 'type_id': 2}, {'name': 'アジア地区', 'span': [23, 28], 'type_id': 5}]\n",
            "# 推論 #\n",
            "[{'name': 'ジョンソン', 'span': [0, 5], 'type_id': 1}, {'name': 'フランス', 'span': [6, 10], 'type_id': 5}, {'name': 'ペリエ・ヴィッテル', 'span': [11, 20], 'type_id': 2}, {'name': 'アジア', 'span': [23, 26], 'type_id': 4}]\n",
            "# もとの文章 #\n",
            "ジョンソンはフランスのペリエ・ヴィッテルにて、アジア地区シニアマネジャーを務めた。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(entities_list, entities_predicted_list, type_id=None):\n",
        "    \"\"\"\n",
        "    正解と予測を比較し、モデルの固有表現抽出の性能を評価する。\n",
        "    type_idがNoneのときは、全ての固有表現のタイプに対して評価する。\n",
        "    type_idが整数を指定すると、その固有表現のタイプのIDに対して評価を行う。\n",
        "    \"\"\"\n",
        "    num_entities = 0 # 固有表現(正解)の個数\n",
        "    num_predictions = 0 # BERTにより予測された固有表現の個数\n",
        "    num_correct = 0 # BERTにより予測のうち正解であった固有表現の数\n",
        "\n",
        "    # それぞれの文章で予測と正解を比較。\n",
        "    # 予測は文章中の位置とタイプIDが一致すれば正解とみなす。\n",
        "    for entities, entities_predicted in zip(entities_list, entities_predicted_list):\n",
        "\n",
        "        if type_id:\n",
        "            entities = [ e for e in entities if e['type_id'] == type_id ]\n",
        "            entities_predicted = [ \n",
        "                e for e in entities_predicted if e['type_id'] == type_id\n",
        "            ]\n",
        "            \n",
        "        get_span_type = lambda e: (e['span'][0], e['span'][1], e['type_id'])\n",
        "        set_entities = set( get_span_type(e) for e in entities )\n",
        "        set_entities_predicted = set( get_span_type(e) for e in entities_predicted )\n",
        "\n",
        "        num_entities += len(entities)\n",
        "        num_predictions += len(entities_predicted)\n",
        "        num_correct += len( set_entities & set_entities_predicted )\n",
        "\n",
        "    # 指標を計算\n",
        "    precision = num_correct/num_predictions # 適合率\n",
        "    recall = num_correct/num_entities # 再現率\n",
        "    f_value = 2*precision*recall/(precision+recall) # F値\n",
        "\n",
        "    result = {\n",
        "        'num_entities': num_entities,\n",
        "        'num_predictions': num_predictions,\n",
        "        'num_correct': num_correct,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f_value': f_value\n",
        "    }\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "kesRewvRhVVd"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 評価結果\n",
        "import pandas as pd\n",
        "eval_df = pd.DataFrame()\n",
        "for k, v in type_id_dict.items():\n",
        "  eval_res = evaluate_model(entities_list, entities_predicted_list, type_id=v)\n",
        "  eval_df[k] = eval_res.values()\n",
        "\n",
        "eval_res_all = evaluate_model(entities_list, entities_predicted_list, type_id=None)\n",
        "eval_df[\"ALL\"] = eval_res_all.values()\n",
        "\n",
        "eval_df.index = eval_res_all.keys()\n",
        "eval_df"
      ],
      "metadata": {
        "id": "1dzE6nRXA9Ws",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "713639d1-f609-42a0-fbb3-64067376257c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                         人名         法人名      政治的組織名     その他の組織名          地名  \\\n",
              "num_entities     604.000000  504.000000  249.000000  222.000000  452.000000   \n",
              "num_predictions  608.000000  500.000000  297.000000  230.000000  468.000000   \n",
              "num_correct      577.000000  450.000000  218.000000  184.000000  400.000000   \n",
              "precision          0.949013    0.900000    0.734007    0.800000    0.854701   \n",
              "recall             0.955298    0.892857    0.875502    0.828829    0.884956   \n",
              "f_value            0.952145    0.896414    0.798535    0.814159    0.869565   \n",
              "\n",
              "                        施設名         製品名       イベント名          ALL  \n",
              "num_entities     222.000000  231.000000  203.000000  2687.000000  \n",
              "num_predictions  231.000000  247.000000  201.000000  2782.000000  \n",
              "num_correct      180.000000  186.000000  176.000000  2371.000000  \n",
              "precision          0.779221    0.753036    0.875622     0.852265  \n",
              "recall             0.810811    0.805195    0.866995     0.882397  \n",
              "f_value            0.794702    0.778243    0.871287     0.867069  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-57369e50-7485-4a89-a493-0c9b6b3fd1ca\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>人名</th>\n",
              "      <th>法人名</th>\n",
              "      <th>政治的組織名</th>\n",
              "      <th>その他の組織名</th>\n",
              "      <th>地名</th>\n",
              "      <th>施設名</th>\n",
              "      <th>製品名</th>\n",
              "      <th>イベント名</th>\n",
              "      <th>ALL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>num_entities</th>\n",
              "      <td>604.000000</td>\n",
              "      <td>504.000000</td>\n",
              "      <td>249.000000</td>\n",
              "      <td>222.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>222.000000</td>\n",
              "      <td>231.000000</td>\n",
              "      <td>203.000000</td>\n",
              "      <td>2687.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>num_predictions</th>\n",
              "      <td>608.000000</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>297.000000</td>\n",
              "      <td>230.000000</td>\n",
              "      <td>468.000000</td>\n",
              "      <td>231.000000</td>\n",
              "      <td>247.000000</td>\n",
              "      <td>201.000000</td>\n",
              "      <td>2782.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>num_correct</th>\n",
              "      <td>577.000000</td>\n",
              "      <td>450.000000</td>\n",
              "      <td>218.000000</td>\n",
              "      <td>184.000000</td>\n",
              "      <td>400.000000</td>\n",
              "      <td>180.000000</td>\n",
              "      <td>186.000000</td>\n",
              "      <td>176.000000</td>\n",
              "      <td>2371.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.949013</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.734007</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.854701</td>\n",
              "      <td>0.779221</td>\n",
              "      <td>0.753036</td>\n",
              "      <td>0.875622</td>\n",
              "      <td>0.852265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.955298</td>\n",
              "      <td>0.892857</td>\n",
              "      <td>0.875502</td>\n",
              "      <td>0.828829</td>\n",
              "      <td>0.884956</td>\n",
              "      <td>0.810811</td>\n",
              "      <td>0.805195</td>\n",
              "      <td>0.866995</td>\n",
              "      <td>0.882397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_value</th>\n",
              "      <td>0.952145</td>\n",
              "      <td>0.896414</td>\n",
              "      <td>0.798535</td>\n",
              "      <td>0.814159</td>\n",
              "      <td>0.869565</td>\n",
              "      <td>0.794702</td>\n",
              "      <td>0.778243</td>\n",
              "      <td>0.871287</td>\n",
              "      <td>0.867069</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-57369e50-7485-4a89-a493-0c9b6b3fd1ca')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-57369e50-7485-4a89-a493-0c9b6b3fd1ca button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-57369e50-7485-4a89-a493-0c9b6b3fd1ca');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "veGtGf8_4EEF"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}